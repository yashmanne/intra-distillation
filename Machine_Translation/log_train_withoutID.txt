use_id is false.
Training without Intra-Distillation
2023-03-11 05:15:35 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './models/de-3-5-false//log/', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 16, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './models/de-3-5-false/', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 40, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='./data/data-bin-de/', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=16, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=512, max_target_positions=512, max_tokens=4096, max_tokens_valid=4096, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=40, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./models/de-3-5-false/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='./models/de-3-5-false//log/', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=8000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': './data/data-bin-de/', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 512, 'max_target_positions': 512, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 8000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-03-11 05:15:35 | INFO | fairseq.tasks.translation | [de] dictionary: 12001 types
2023-03-11 05:15:35 | INFO | fairseq.tasks.translation | [en] dictionary: 12001 types
2023-03-11 05:15:35 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(12001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=12001, bias=False)
  )
)
2023-03-11 05:15:35 | INFO | fairseq_cli.train | task: TranslationTask
2023-03-11 05:15:35 | INFO | fairseq_cli.train | model: TransformerModel
2023-03-11 05:15:35 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-03-11 05:15:35 | INFO | fairseq_cli.train | num. shared model params: 49,976,832 (num. trained: 49,976,832)
2023-03-11 05:15:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-03-11 05:15:35 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: ./data/data-bin-de/valid.de-en.de
2023-03-11 05:15:35 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: ./data/data-bin-de/valid.de-en.en
2023-03-11 05:15:35 | INFO | fairseq.tasks.translation | ./data/data-bin-de/ valid de-en 7283 examples
2023-03-11 05:15:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-03-11 05:15:38 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-SXM2-16GB                    
2023-03-11 05:15:38 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-03-11 05:15:38 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-03-11 05:15:38 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2023-03-11 05:15:38 | INFO | fairseq.trainer | Preparing to load checkpoint ./models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:15:38 | INFO | fairseq.trainer | No existing checkpoint found ./models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:15:38 | INFO | fairseq.trainer | loading train data for epoch 1
2023-03-11 05:15:38 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: ./data/data-bin-de/train.de-en.de
2023-03-11 05:15:38 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: ./data/data-bin-de/train.de-en.en
2023-03-11 05:15:38 | INFO | fairseq.tasks.translation | ./data/data-bin-de/ train de-en 160239 examples
2023-03-11 05:15:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:15:39 | INFO | fairseq.trainer | begin training epoch 1
2023-03-11 05:15:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:15:45 | INFO | train_inner | epoch 001:    100 / 1102 loss=13.194, nll_loss=13.072, ppl=8609.45, wps=53572.5, ups=15.14, wpb=3542.1, bsz=126.9, num_updates=100, lr=6.25e-06, gnorm=4.495, loss_scale=16, train_wall=7, gb_free=13.5, wall=7
2023-03-11 05:15:52 | INFO | train_inner | epoch 001:    200 / 1102 loss=11.528, nll_loss=11.212, ppl=2371.91, wps=54281.8, ups=14.96, wpb=3628.8, bsz=141.5, num_updates=200, lr=1.25e-05, gnorm=1.955, loss_scale=16, train_wall=7, gb_free=13.6, wall=14
2023-03-11 05:15:59 | INFO | train_inner | epoch 001:    300 / 1102 loss=10.772, nll_loss=10.362, ppl=1316.48, wps=54349.4, ups=15.02, wpb=3619, bsz=132.6, num_updates=300, lr=1.875e-05, gnorm=1.515, loss_scale=16, train_wall=6, gb_free=13.5, wall=20
2023-03-11 05:16:06 | INFO | train_inner | epoch 001:    400 / 1102 loss=9.927, nll_loss=9.39, ppl=670.69, wps=52837.8, ups=14.87, wpb=3552.2, bsz=160.2, num_updates=400, lr=2.5e-05, gnorm=1.898, loss_scale=16, train_wall=7, gb_free=13.6, wall=27
2023-03-11 05:16:12 | INFO | train_inner | epoch 001:    500 / 1102 loss=9.483, nll_loss=8.841, ppl=458.65, wps=53176.4, ups=14.98, wpb=3549.8, bsz=149.1, num_updates=500, lr=3.125e-05, gnorm=1.867, loss_scale=16, train_wall=6, gb_free=13.7, wall=34
2023-03-11 05:16:19 | INFO | train_inner | epoch 001:    600 / 1102 loss=9.172, nll_loss=8.458, ppl=351.58, wps=54576.9, ups=15.03, wpb=3632.3, bsz=155.8, num_updates=600, lr=3.75e-05, gnorm=1.842, loss_scale=16, train_wall=6, gb_free=13.5, wall=40
2023-03-11 05:16:25 | INFO | train_inner | epoch 001:    700 / 1102 loss=8.957, nll_loss=8.196, ppl=293.2, wps=54989.9, ups=15.31, wpb=3592.2, bsz=168, num_updates=700, lr=4.375e-05, gnorm=2.146, loss_scale=16, train_wall=6, gb_free=13.6, wall=47
2023-03-11 05:16:32 | INFO | train_inner | epoch 001:    800 / 1102 loss=8.911, nll_loss=8.14, ppl=282.1, wps=54151.4, ups=15.13, wpb=3579.1, bsz=133.5, num_updates=800, lr=5e-05, gnorm=1.636, loss_scale=16, train_wall=6, gb_free=13.5, wall=54
2023-03-11 05:16:39 | INFO | train_inner | epoch 001:    900 / 1102 loss=8.782, nll_loss=7.988, ppl=253.93, wps=53723.1, ups=15.03, wpb=3574, bsz=146, num_updates=900, lr=5.625e-05, gnorm=1.917, loss_scale=16, train_wall=6, gb_free=13.6, wall=60
2023-03-11 05:16:45 | INFO | train_inner | epoch 001:   1000 / 1102 loss=8.58, nll_loss=7.76, ppl=216.76, wps=54567.3, ups=15.15, wpb=3601.9, bsz=148.5, num_updates=1000, lr=6.25e-05, gnorm=1.924, loss_scale=16, train_wall=6, gb_free=13.5, wall=67
2023-03-11 05:16:52 | INFO | train_inner | epoch 001:   1100 / 1102 loss=8.547, nll_loss=7.72, ppl=210.86, wps=53231.1, ups=15.14, wpb=3517, bsz=137.8, num_updates=1100, lr=6.875e-05, gnorm=2.009, loss_scale=16, train_wall=6, gb_free=13.6, wall=73
2023-03-11 05:16:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:16:53 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.277 | nll_loss 7.372 | ppl 165.62 | wps 132577 | wpb 2790.1 | bsz 113.8 | num_updates 1102
2023-03-11 05:16:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1102 updates
2023-03-11 05:16:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:16:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 1 @ 1102 updates, score 8.277) (writing took 1.4292715360024886 seconds)
2023-03-11 05:16:55 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-03-11 05:16:55 | INFO | train | epoch 001 | loss 9.802 | nll_loss 9.192 | ppl 584.71 | wps 51900.1 | ups 14.49 | wpb 3581.5 | bsz 145.4 | num_updates 1102 | lr 6.8875e-05 | gnorm 2.108 | loss_scale 16 | train_wall 71 | gb_free 13.5 | wall 76
2023-03-11 05:16:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:16:55 | INFO | fairseq.trainer | begin training epoch 2
2023-03-11 05:16:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:17:02 | INFO | train_inner | epoch 002:     98 / 1102 loss=8.27, nll_loss=7.407, ppl=169.72, wps=37703.5, ups=10.36, wpb=3637.9, bsz=158, num_updates=1200, lr=7.5e-05, gnorm=1.88, loss_scale=16, train_wall=6, gb_free=13.6, wall=83
2023-03-11 05:17:08 | INFO | train_inner | epoch 002:    198 / 1102 loss=8.28, nll_loss=7.416, ppl=170.82, wps=54498.6, ups=15.11, wpb=3607.4, bsz=145.6, num_updates=1300, lr=8.125e-05, gnorm=1.692, loss_scale=16, train_wall=6, gb_free=13.5, wall=90
2023-03-11 05:17:15 | INFO | train_inner | epoch 002:    298 / 1102 loss=8.163, nll_loss=7.284, ppl=155.83, wps=54454, ups=15.28, wpb=3563.1, bsz=151.2, num_updates=1400, lr=8.75e-05, gnorm=1.708, loss_scale=16, train_wall=6, gb_free=13.4, wall=96
2023-03-11 05:17:21 | INFO | train_inner | epoch 002:    398 / 1102 loss=8.179, nll_loss=7.301, ppl=157.7, wps=55155, ups=15.2, wpb=3627.7, bsz=136.3, num_updates=1500, lr=9.375e-05, gnorm=1.607, loss_scale=16, train_wall=6, gb_free=13.5, wall=103
2023-03-11 05:17:28 | INFO | train_inner | epoch 002:    498 / 1102 loss=7.995, nll_loss=7.092, ppl=136.44, wps=54129.4, ups=15.26, wpb=3546.2, bsz=146.6, num_updates=1600, lr=0.0001, gnorm=1.628, loss_scale=16, train_wall=6, gb_free=13.5, wall=109
2023-03-11 05:17:35 | INFO | train_inner | epoch 002:    598 / 1102 loss=7.915, nll_loss=6.999, ppl=127.88, wps=53468.5, ups=14.79, wpb=3615.4, bsz=145.4, num_updates=1700, lr=0.00010625, gnorm=1.644, loss_scale=16, train_wall=7, gb_free=13.5, wall=116
2023-03-11 05:17:41 | INFO | train_inner | epoch 002:    698 / 1102 loss=7.817, nll_loss=6.887, ppl=118.35, wps=55033.7, ups=15.2, wpb=3620, bsz=155.5, num_updates=1800, lr=0.0001125, gnorm=1.503, loss_scale=16, train_wall=6, gb_free=13.5, wall=123
2023-03-11 05:17:48 | INFO | train_inner | epoch 002:    798 / 1102 loss=7.743, nll_loss=6.803, ppl=111.63, wps=55597.9, ups=15.18, wpb=3662.6, bsz=149.4, num_updates=1900, lr=0.00011875, gnorm=1.528, loss_scale=16, train_wall=6, gb_free=13.5, wall=129
2023-03-11 05:17:54 | INFO | train_inner | epoch 002:    898 / 1102 loss=7.816, nll_loss=6.884, ppl=118.12, wps=52698, ups=15.02, wpb=3508.3, bsz=135.4, num_updates=2000, lr=0.000125, gnorm=1.564, loss_scale=16, train_wall=6, gb_free=13.6, wall=136
2023-03-11 05:18:01 | INFO | train_inner | epoch 002:    998 / 1102 loss=7.741, nll_loss=6.799, ppl=111.33, wps=53264.5, ups=15.03, wpb=3543.9, bsz=138.9, num_updates=2100, lr=0.00013125, gnorm=1.513, loss_scale=16, train_wall=6, gb_free=13.6, wall=143
2023-03-11 05:18:08 | INFO | train_inner | epoch 002:   1098 / 1102 loss=7.738, nll_loss=6.795, ppl=111.03, wps=52631.2, ups=15.1, wpb=3485, bsz=138.6, num_updates=2200, lr=0.0001375, gnorm=1.549, loss_scale=16, train_wall=6, gb_free=13.5, wall=149
2023-03-11 05:18:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:18:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.43 | nll_loss 6.404 | ppl 84.67 | wps 139082 | wpb 2790.1 | bsz 113.8 | num_updates 2204 | best_loss 7.43
2023-03-11 05:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2204 updates
2023-03-11 05:18:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:18:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:18:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 2 @ 2204 updates, score 7.43) (writing took 7.047336780000478 seconds)
2023-03-11 05:18:16 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-03-11 05:18:16 | INFO | train | epoch 002 | loss 7.969 | nll_loss 7.061 | ppl 133.55 | wps 48423.1 | ups 13.52 | wpb 3581.5 | bsz 145.4 | num_updates 2204 | lr 0.00013775 | gnorm 1.619 | loss_scale 16 | train_wall 71 | gb_free 13.6 | wall 158
2023-03-11 05:18:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:18:16 | INFO | fairseq.trainer | begin training epoch 3
2023-03-11 05:18:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:18:23 | INFO | train_inner | epoch 003:     96 / 1102 loss=7.606, nll_loss=6.647, ppl=100.22, wps=23669.9, ups=6.58, wpb=3596.8, bsz=141.4, num_updates=2300, lr=0.00014375, gnorm=1.354, loss_scale=16, train_wall=6, gb_free=13.5, wall=165
2023-03-11 05:18:29 | INFO | train_inner | epoch 003:    196 / 1102 loss=7.541, nll_loss=6.571, ppl=95.07, wps=53928.5, ups=15.36, wpb=3511.1, bsz=137.8, num_updates=2400, lr=0.00015, gnorm=1.469, loss_scale=16, train_wall=6, gb_free=13.6, wall=171
2023-03-11 05:18:36 | INFO | train_inner | epoch 003:    296 / 1102 loss=7.472, nll_loss=6.492, ppl=90.02, wps=55405.5, ups=15.32, wpb=3615.4, bsz=145.8, num_updates=2500, lr=0.00015625, gnorm=1.312, loss_scale=16, train_wall=6, gb_free=13.5, wall=178
2023-03-11 05:18:42 | INFO | train_inner | epoch 003:    396 / 1102 loss=7.397, nll_loss=6.407, ppl=84.85, wps=54615, ups=15.34, wpb=3560.4, bsz=149.5, num_updates=2600, lr=0.0001625, gnorm=1.42, loss_scale=16, train_wall=6, gb_free=13.6, wall=184
2023-03-11 05:18:49 | INFO | train_inner | epoch 003:    496 / 1102 loss=7.39, nll_loss=6.397, ppl=84.29, wps=55139.2, ups=15.38, wpb=3585.6, bsz=134.2, num_updates=2700, lr=0.00016875, gnorm=1.42, loss_scale=16, train_wall=6, gb_free=13.8, wall=191
2023-03-11 05:18:56 | INFO | train_inner | epoch 003:    596 / 1102 loss=7.243, nll_loss=6.232, ppl=75.17, wps=54684.2, ups=15.29, wpb=3577.6, bsz=153.2, num_updates=2800, lr=0.000175, gnorm=1.419, loss_scale=16, train_wall=6, gb_free=13.6, wall=197
2023-03-11 05:19:02 | INFO | train_inner | epoch 003:    696 / 1102 loss=7.189, nll_loss=6.168, ppl=71.9, wps=54841.7, ups=15.02, wpb=3651.3, bsz=155.8, num_updates=2900, lr=0.00018125, gnorm=1.458, loss_scale=16, train_wall=6, gb_free=13.7, wall=204
2023-03-11 05:19:09 | INFO | train_inner | epoch 003:    796 / 1102 loss=7.115, nll_loss=6.085, ppl=67.87, wps=54823.9, ups=15.12, wpb=3625.8, bsz=150.9, num_updates=3000, lr=0.0001875, gnorm=1.399, loss_scale=16, train_wall=6, gb_free=13.6, wall=210
2023-03-11 05:19:15 | INFO | train_inner | epoch 003:    896 / 1102 loss=7.233, nll_loss=6.218, ppl=74.46, wps=54064, ups=15.24, wpb=3547.9, bsz=125.6, num_updates=3100, lr=0.00019375, gnorm=1.343, loss_scale=16, train_wall=6, gb_free=13.6, wall=217
2023-03-11 05:19:22 | INFO | train_inner | epoch 003:    996 / 1102 loss=7.037, nll_loss=5.996, ppl=63.8, wps=54455, ups=15.37, wpb=3543.2, bsz=149.9, num_updates=3200, lr=0.0002, gnorm=1.463, loss_scale=16, train_wall=6, gb_free=13.4, wall=223
2023-03-11 05:19:28 | INFO | train_inner | epoch 003:   1096 / 1102 loss=6.936, nll_loss=5.879, ppl=58.85, wps=54991, ups=15.36, wpb=3579.9, bsz=154.6, num_updates=3300, lr=0.00020625, gnorm=1.516, loss_scale=16, train_wall=6, gb_free=13.6, wall=230
2023-03-11 05:19:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:19:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.701 | nll_loss 5.55 | ppl 46.86 | wps 137706 | wpb 2790.1 | bsz 113.8 | num_updates 3306 | best_loss 6.701
2023-03-11 05:19:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3306 updates
2023-03-11 05:19:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:19:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 3 @ 3306 updates, score 6.701) (writing took 7.130259661000309 seconds)
2023-03-11 05:19:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-03-11 05:19:37 | INFO | train | epoch 003 | loss 7.283 | nll_loss 6.276 | ppl 77.49 | wps 48790.6 | ups 13.62 | wpb 3581.5 | bsz 145.4 | num_updates 3306 | lr 0.000206625 | gnorm 1.416 | loss_scale 16 | train_wall 70 | gb_free 13.6 | wall 239
2023-03-11 05:19:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:19:37 | INFO | fairseq.trainer | begin training epoch 4
2023-03-11 05:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:19:44 | INFO | train_inner | epoch 004:     94 / 1102 loss=6.827, nll_loss=5.758, ppl=54.11, wps=23510.1, ups=6.52, wpb=3605.8, bsz=150.7, num_updates=3400, lr=0.0002125, gnorm=1.342, loss_scale=16, train_wall=7, gb_free=13.5, wall=245
2023-03-11 05:19:50 | INFO | train_inner | epoch 004:    194 / 1102 loss=6.884, nll_loss=5.82, ppl=56.5, wps=54073, ups=15.16, wpb=3567.5, bsz=139.7, num_updates=3500, lr=0.00021875, gnorm=1.407, loss_scale=16, train_wall=6, gb_free=13.6, wall=252
2023-03-11 05:19:57 | INFO | train_inner | epoch 004:    294 / 1102 loss=6.8, nll_loss=5.725, ppl=52.88, wps=54603, ups=15.26, wpb=3577.3, bsz=129.8, num_updates=3600, lr=0.000225, gnorm=1.365, loss_scale=16, train_wall=6, gb_free=13.7, wall=258
2023-03-11 05:20:04 | INFO | train_inner | epoch 004:    394 / 1102 loss=6.734, nll_loss=5.649, ppl=50.18, wps=53634.2, ups=15.07, wpb=3558.3, bsz=138.8, num_updates=3700, lr=0.00023125, gnorm=1.421, loss_scale=16, train_wall=6, gb_free=13.5, wall=265
2023-03-11 05:20:10 | INFO | train_inner | epoch 004:    494 / 1102 loss=6.69, nll_loss=5.598, ppl=48.45, wps=53373.4, ups=14.96, wpb=3567.2, bsz=137.8, num_updates=3800, lr=0.0002375, gnorm=1.372, loss_scale=16, train_wall=7, gb_free=13.5, wall=272
2023-03-11 05:20:17 | INFO | train_inner | epoch 004:    594 / 1102 loss=6.569, nll_loss=5.461, ppl=44.04, wps=54889.7, ups=15.17, wpb=3619.3, bsz=143.1, num_updates=3900, lr=0.00024375, gnorm=1.343, loss_scale=16, train_wall=6, gb_free=13.5, wall=278
2023-03-11 05:20:23 | INFO | train_inner | epoch 004:    694 / 1102 loss=6.538, nll_loss=5.425, ppl=42.97, wps=55100.5, ups=15.21, wpb=3623.6, bsz=144.1, num_updates=4000, lr=0.00025, gnorm=1.404, loss_scale=16, train_wall=6, gb_free=13.6, wall=285
2023-03-11 05:20:30 | INFO | train_inner | epoch 004:    794 / 1102 loss=6.434, nll_loss=5.304, ppl=39.5, wps=53494.3, ups=15.24, wpb=3510.5, bsz=151, num_updates=4100, lr=0.00025625, gnorm=1.453, loss_scale=16, train_wall=6, gb_free=13.5, wall=292
2023-03-11 05:20:36 | INFO | train_inner | epoch 004:    894 / 1102 loss=6.449, nll_loss=5.322, ppl=40, wps=53654, ups=15.29, wpb=3508.5, bsz=152.6, num_updates=4200, lr=0.0002625, gnorm=1.414, loss_scale=16, train_wall=6, gb_free=13.7, wall=298
2023-03-11 05:20:43 | INFO | train_inner | epoch 004:    994 / 1102 loss=6.345, nll_loss=5.205, ppl=36.88, wps=53915.5, ups=14.95, wpb=3606.3, bsz=150, num_updates=4300, lr=0.00026875, gnorm=1.347, loss_scale=16, train_wall=7, gb_free=13.7, wall=305
2023-03-11 05:20:50 | INFO | train_inner | epoch 004:   1094 / 1102 loss=6.205, nll_loss=5.043, ppl=32.96, wps=56022.6, ups=15.45, wpb=3625.5, bsz=161.7, num_updates=4400, lr=0.000275, gnorm=1.348, loss_scale=16, train_wall=6, gb_free=13.5, wall=311
2023-03-11 05:20:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:20:52 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.986 | nll_loss 4.682 | ppl 25.67 | wps 138530 | wpb 2790.1 | bsz 113.8 | num_updates 4408 | best_loss 5.986
2023-03-11 05:20:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4408 updates
2023-03-11 05:20:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:20:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:20:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 4 @ 4408 updates, score 5.986) (writing took 7.1322021820014925 seconds)
2023-03-11 05:20:59 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-03-11 05:20:59 | INFO | train | epoch 004 | loss 6.585 | nll_loss 5.479 | ppl 44.59 | wps 48492.5 | ups 13.54 | wpb 3581.5 | bsz 145.4 | num_updates 4408 | lr 0.0002755 | gnorm 1.383 | loss_scale 16 | train_wall 71 | gb_free 13.5 | wall 320
2023-03-11 05:20:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:20:59 | INFO | fairseq.trainer | begin training epoch 5
2023-03-11 05:20:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:21:05 | INFO | train_inner | epoch 005:     92 / 1102 loss=6.203, nll_loss=5.041, ppl=32.91, wps=23206.2, ups=6.55, wpb=3543.9, bsz=149.5, num_updates=4500, lr=0.00028125, gnorm=1.369, loss_scale=16, train_wall=6, gb_free=13.7, wall=327
2023-03-11 05:21:11 | INFO | train_inner | epoch 005:    192 / 1102 loss=6.01, nll_loss=4.82, ppl=28.24, wps=55013.9, ups=15.29, wpb=3598.3, bsz=151.7, num_updates=4600, lr=0.0002875, gnorm=1.298, loss_scale=16, train_wall=6, gb_free=13.6, wall=333
2023-03-11 05:21:18 | INFO | train_inner | epoch 005:    292 / 1102 loss=6.083, nll_loss=4.902, ppl=29.9, wps=54382.6, ups=15.13, wpb=3594.4, bsz=143.8, num_updates=4700, lr=0.00029375, gnorm=1.315, loss_scale=16, train_wall=6, gb_free=13.5, wall=340
2023-03-11 05:21:25 | INFO | train_inner | epoch 005:    392 / 1102 loss=5.966, nll_loss=4.767, ppl=27.23, wps=55261.9, ups=15.2, wpb=3635.8, bsz=148.2, num_updates=4800, lr=0.0003, gnorm=1.315, loss_scale=16, train_wall=6, gb_free=13.5, wall=346
2023-03-11 05:21:31 | INFO | train_inner | epoch 005:    492 / 1102 loss=6.081, nll_loss=4.895, ppl=29.74, wps=54569.7, ups=15.16, wpb=3600, bsz=129.4, num_updates=4900, lr=0.00030625, gnorm=1.354, loss_scale=16, train_wall=6, gb_free=13.6, wall=353
2023-03-11 05:21:38 | INFO | train_inner | epoch 005:    592 / 1102 loss=5.938, nll_loss=4.732, ppl=26.58, wps=54715.2, ups=15.27, wpb=3582.2, bsz=136, num_updates=5000, lr=0.0003125, gnorm=1.281, loss_scale=16, train_wall=6, gb_free=13.5, wall=359
2023-03-11 05:21:44 | INFO | train_inner | epoch 005:    692 / 1102 loss=5.806, nll_loss=4.582, ppl=23.95, wps=54689.2, ups=15.25, wpb=3587, bsz=158.8, num_updates=5100, lr=0.00031875, gnorm=1.318, loss_scale=16, train_wall=6, gb_free=13.4, wall=366
2023-03-11 05:21:51 | INFO | train_inner | epoch 005:    792 / 1102 loss=5.828, nll_loss=4.604, ppl=24.32, wps=55414.4, ups=15.13, wpb=3662.5, bsz=142.6, num_updates=5200, lr=0.000325, gnorm=1.304, loss_scale=16, train_wall=6, gb_free=13.5, wall=373
2023-03-11 05:21:58 | INFO | train_inner | epoch 005:    892 / 1102 loss=5.735, nll_loss=4.499, ppl=22.61, wps=54289.4, ups=15.24, wpb=3561.8, bsz=160.2, num_updates=5300, lr=0.00033125, gnorm=1.376, loss_scale=16, train_wall=6, gb_free=13.6, wall=379
2023-03-11 05:22:04 | INFO | train_inner | epoch 005:    992 / 1102 loss=5.759, nll_loss=4.524, ppl=23.01, wps=52479.7, ups=14.83, wpb=3538.8, bsz=138.5, num_updates=5400, lr=0.0003375, gnorm=1.292, loss_scale=16, train_wall=7, gb_free=13.5, wall=386
2023-03-11 05:22:11 | INFO | train_inner | epoch 005:   1092 / 1102 loss=5.704, nll_loss=4.461, ppl=22.03, wps=52697.1, ups=15.09, wpb=3493, bsz=139.9, num_updates=5500, lr=0.00034375, gnorm=1.314, loss_scale=16, train_wall=6, gb_free=13.6, wall=393
2023-03-11 05:22:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:22:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.36 | nll_loss 3.962 | ppl 15.59 | wps 138097 | wpb 2790.1 | bsz 113.8 | num_updates 5510 | best_loss 5.36
2023-03-11 05:22:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5510 updates
2023-03-11 05:22:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:22:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 5 @ 5510 updates, score 5.36) (writing took 7.129211130002659 seconds)
2023-03-11 05:22:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-03-11 05:22:20 | INFO | train | epoch 005 | loss 5.913 | nll_loss 4.704 | ppl 26.06 | wps 48500 | ups 13.54 | wpb 3581.5 | bsz 145.4 | num_updates 5510 | lr 0.000344375 | gnorm 1.321 | loss_scale 16 | train_wall 71 | gb_free 13.6 | wall 402
2023-03-11 05:22:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:22:20 | INFO | fairseq.trainer | begin training epoch 6
2023-03-11 05:22:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:22:26 | INFO | train_inner | epoch 006:     90 / 1102 loss=5.562, nll_loss=4.298, ppl=19.68, wps=23623.8, ups=6.58, wpb=3592.4, bsz=141.9, num_updates=5600, lr=0.00035, gnorm=1.32, loss_scale=16, train_wall=6, gb_free=13.5, wall=408
2023-03-11 05:22:33 | INFO | train_inner | epoch 006:    190 / 1102 loss=5.461, nll_loss=4.182, ppl=18.15, wps=53393.5, ups=15.41, wpb=3464.3, bsz=150.2, num_updates=5700, lr=0.00035625, gnorm=1.312, loss_scale=16, train_wall=6, gb_free=13.5, wall=414
2023-03-11 05:22:39 | INFO | train_inner | epoch 006:    290 / 1102 loss=5.536, nll_loss=4.267, ppl=19.25, wps=53352.2, ups=15.1, wpb=3533.6, bsz=134.5, num_updates=5800, lr=0.0003625, gnorm=1.303, loss_scale=16, train_wall=6, gb_free=13.5, wall=421
2023-03-11 05:22:46 | INFO | train_inner | epoch 006:    390 / 1102 loss=5.412, nll_loss=4.124, ppl=17.44, wps=55339, ups=15.2, wpb=3639.6, bsz=148.2, num_updates=5900, lr=0.00036875, gnorm=1.266, loss_scale=16, train_wall=6, gb_free=13.6, wall=427
2023-03-11 05:22:52 | INFO | train_inner | epoch 006:    490 / 1102 loss=5.408, nll_loss=4.119, ppl=17.37, wps=54621.3, ups=15.14, wpb=3608.7, bsz=142.5, num_updates=6000, lr=0.000375, gnorm=1.266, loss_scale=16, train_wall=6, gb_free=13.5, wall=434
2023-03-11 05:22:59 | INFO | train_inner | epoch 006:    590 / 1102 loss=5.434, nll_loss=4.147, ppl=17.72, wps=53826, ups=15.2, wpb=3542.3, bsz=134.8, num_updates=6100, lr=0.00038125, gnorm=1.292, loss_scale=16, train_wall=6, gb_free=13.8, wall=441
2023-03-11 05:23:06 | INFO | train_inner | epoch 006:    690 / 1102 loss=5.245, nll_loss=3.932, ppl=15.27, wps=54319.1, ups=15.18, wpb=3577.8, bsz=163.4, num_updates=6200, lr=0.0003875, gnorm=1.227, loss_scale=16, train_wall=6, gb_free=13.5, wall=447
2023-03-11 05:23:12 | INFO | train_inner | epoch 006:    790 / 1102 loss=5.194, nll_loss=3.873, ppl=14.65, wps=55252, ups=15.19, wpb=3636.4, bsz=159.2, num_updates=6300, lr=0.00039375, gnorm=1.217, loss_scale=16, train_wall=6, gb_free=13.7, wall=454
2023-03-11 05:23:19 | INFO | train_inner | epoch 006:    890 / 1102 loss=5.368, nll_loss=4.069, ppl=16.79, wps=54223.8, ups=15.03, wpb=3607.9, bsz=140.2, num_updates=6400, lr=0.0004, gnorm=1.305, loss_scale=16, train_wall=6, gb_free=13.5, wall=460
2023-03-11 05:23:26 | INFO | train_inner | epoch 006:    990 / 1102 loss=5.31, nll_loss=4.003, ppl=16.03, wps=54206.3, ups=15.01, wpb=3610.6, bsz=134.5, num_updates=6500, lr=0.00040625, gnorm=1.27, loss_scale=16, train_wall=6, gb_free=13.6, wall=467
2023-03-11 05:23:32 | INFO | train_inner | epoch 006:   1090 / 1102 loss=5.205, nll_loss=3.883, ppl=14.75, wps=54423.9, ups=15.16, wpb=3591.1, bsz=151.6, num_updates=6600, lr=0.0004125, gnorm=1.247, loss_scale=16, train_wall=6, gb_free=13.5, wall=474
2023-03-11 05:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:23:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.998 | nll_loss 3.527 | ppl 11.53 | wps 137486 | wpb 2790.1 | bsz 113.8 | num_updates 6612 | best_loss 4.998
2023-03-11 05:23:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6612 updates
2023-03-11 05:23:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:23:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:23:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 6 @ 6612 updates, score 4.998) (writing took 7.1035939490029705 seconds)
2023-03-11 05:23:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-03-11 05:23:41 | INFO | train | epoch 006 | loss 5.371 | nll_loss 4.076 | ppl 16.86 | wps 48542.6 | ups 13.55 | wpb 3581.5 | bsz 145.4 | num_updates 6612 | lr 0.00041325 | gnorm 1.274 | loss_scale 16 | train_wall 71 | gb_free 13.7 | wall 483
2023-03-11 05:23:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:23:41 | INFO | fairseq.trainer | begin training epoch 7
2023-03-11 05:23:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:23:47 | INFO | train_inner | epoch 007:     88 / 1102 loss=5.066, nll_loss=3.725, ppl=13.22, wps=24024.1, ups=6.6, wpb=3641.9, bsz=144.4, num_updates=6700, lr=0.00041875, gnorm=1.232, loss_scale=16, train_wall=6, gb_free=13.5, wall=489
2023-03-11 05:23:54 | INFO | train_inner | epoch 007:    188 / 1102 loss=5.026, nll_loss=3.679, ppl=12.81, wps=54912.2, ups=15.33, wpb=3582, bsz=154.2, num_updates=6800, lr=0.000425, gnorm=1.25, loss_scale=16, train_wall=6, gb_free=13.6, wall=495
2023-03-11 05:24:00 | INFO | train_inner | epoch 007:    288 / 1102 loss=5.098, nll_loss=3.759, ppl=13.54, wps=54084.8, ups=15.36, wpb=3521.1, bsz=145.4, num_updates=6900, lr=0.00043125, gnorm=1.237, loss_scale=16, train_wall=6, gb_free=13.6, wall=502
2023-03-11 05:24:07 | INFO | train_inner | epoch 007:    388 / 1102 loss=5.098, nll_loss=3.758, ppl=13.53, wps=53025, ups=15, wpb=3535.3, bsz=137.8, num_updates=7000, lr=0.0004375, gnorm=1.239, loss_scale=16, train_wall=6, gb_free=13.5, wall=509
2023-03-11 05:24:14 | INFO | train_inner | epoch 007:    488 / 1102 loss=5.041, nll_loss=3.693, ppl=12.93, wps=53766.5, ups=14.86, wpb=3619, bsz=143, num_updates=7100, lr=0.00044375, gnorm=1.244, loss_scale=16, train_wall=7, gb_free=13.5, wall=515
2023-03-11 05:24:20 | INFO | train_inner | epoch 007:    588 / 1102 loss=5.042, nll_loss=3.693, ppl=12.94, wps=54651.3, ups=15.26, wpb=3581.6, bsz=142, num_updates=7200, lr=0.00045, gnorm=1.249, loss_scale=16, train_wall=6, gb_free=13.6, wall=522
2023-03-11 05:24:27 | INFO | train_inner | epoch 007:    688 / 1102 loss=5.059, nll_loss=3.714, ppl=13.12, wps=53382.3, ups=15.27, wpb=3496.4, bsz=140.8, num_updates=7300, lr=0.00045625, gnorm=1.308, loss_scale=16, train_wall=6, gb_free=13.5, wall=528
2023-03-11 05:24:33 | INFO | train_inner | epoch 007:    788 / 1102 loss=4.949, nll_loss=3.589, ppl=12.04, wps=56253.4, ups=15.39, wpb=3655.3, bsz=152.9, num_updates=7400, lr=0.0004625, gnorm=1.215, loss_scale=16, train_wall=6, gb_free=13.6, wall=535
2023-03-11 05:24:40 | INFO | train_inner | epoch 007:    888 / 1102 loss=4.933, nll_loss=3.571, ppl=11.88, wps=55404.9, ups=15.33, wpb=3614.2, bsz=156.6, num_updates=7500, lr=0.00046875, gnorm=1.196, loss_scale=16, train_wall=6, gb_free=13.6, wall=541
2023-03-11 05:24:46 | INFO | train_inner | epoch 007:    988 / 1102 loss=4.984, nll_loss=3.628, ppl=12.36, wps=54541.9, ups=15.22, wpb=3584.7, bsz=142.4, num_updates=7600, lr=0.000475, gnorm=1.222, loss_scale=16, train_wall=6, gb_free=13.6, wall=548
2023-03-11 05:24:53 | INFO | train_inner | epoch 007:   1088 / 1102 loss=4.983, nll_loss=3.628, ppl=12.36, wps=54496.1, ups=15.28, wpb=3565.5, bsz=136, num_updates=7700, lr=0.00048125, gnorm=1.216, loss_scale=16, train_wall=6, gb_free=13.5, wall=555
2023-03-11 05:24:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:24:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.785 | nll_loss 3.265 | ppl 9.61 | wps 131714 | wpb 2790.1 | bsz 113.8 | num_updates 7714 | best_loss 4.785
2023-03-11 05:24:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7714 updates
2023-03-11 05:24:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:24:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:25:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 7 @ 7714 updates, score 4.785) (writing took 7.053819302000193 seconds)
2023-03-11 05:25:02 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-03-11 05:25:02 | INFO | train | epoch 007 | loss 5.021 | nll_loss 3.672 | ppl 12.74 | wps 48733.8 | ups 13.61 | wpb 3581.5 | bsz 145.4 | num_updates 7714 | lr 0.000482125 | gnorm 1.237 | loss_scale 16 | train_wall 70 | gb_free 13.7 | wall 564
2023-03-11 05:25:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:25:02 | INFO | fairseq.trainer | begin training epoch 8
2023-03-11 05:25:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:25:08 | INFO | train_inner | epoch 008:     86 / 1102 loss=4.844, nll_loss=3.47, ppl=11.08, wps=22904, ups=6.57, wpb=3487.2, bsz=145.1, num_updates=7800, lr=0.0004875, gnorm=1.223, loss_scale=16, train_wall=6, gb_free=13.5, wall=570
2023-03-11 05:25:15 | INFO | train_inner | epoch 008:    186 / 1102 loss=4.818, nll_loss=3.439, ppl=10.84, wps=54036.6, ups=15.28, wpb=3536.7, bsz=149.2, num_updates=7900, lr=0.00049375, gnorm=1.246, loss_scale=16, train_wall=6, gb_free=13.6, wall=576
2023-03-11 05:25:21 | INFO | train_inner | epoch 008:    286 / 1102 loss=4.873, nll_loss=3.5, ppl=11.32, wps=53796.3, ups=15.36, wpb=3501.6, bsz=138.9, num_updates=8000, lr=0.0005, gnorm=1.243, loss_scale=16, train_wall=6, gb_free=13.5, wall=583
2023-03-11 05:25:28 | INFO | train_inner | epoch 008:    386 / 1102 loss=4.809, nll_loss=3.428, ppl=10.76, wps=54835.3, ups=15.14, wpb=3621.7, bsz=147.4, num_updates=8100, lr=0.000496904, gnorm=1.199, loss_scale=16, train_wall=6, gb_free=13.5, wall=589
2023-03-11 05:25:34 | INFO | train_inner | epoch 008:    486 / 1102 loss=4.686, nll_loss=3.29, ppl=9.78, wps=56320.4, ups=15.31, wpb=3679.6, bsz=164.2, num_updates=8200, lr=0.000493865, gnorm=1.142, loss_scale=16, train_wall=6, gb_free=13.4, wall=596
2023-03-11 05:25:41 | INFO | train_inner | epoch 008:    586 / 1102 loss=4.805, nll_loss=3.424, ppl=10.73, wps=54790.3, ups=15.32, wpb=3575.3, bsz=144.4, num_updates=8300, lr=0.000490881, gnorm=1.203, loss_scale=16, train_wall=6, gb_free=13.5, wall=602
2023-03-11 05:25:47 | INFO | train_inner | epoch 008:    686 / 1102 loss=4.749, nll_loss=3.36, ppl=10.27, wps=55803.4, ups=15.33, wpb=3640, bsz=149.4, num_updates=8400, lr=0.00048795, gnorm=1.18, loss_scale=16, train_wall=6, gb_free=13.5, wall=609
2023-03-11 05:25:54 | INFO | train_inner | epoch 008:    786 / 1102 loss=4.762, nll_loss=3.376, ppl=10.38, wps=54641.4, ups=15.18, wpb=3598.8, bsz=140.6, num_updates=8500, lr=0.000485071, gnorm=1.17, loss_scale=16, train_wall=6, gb_free=13.5, wall=616
2023-03-11 05:26:01 | INFO | train_inner | epoch 008:    886 / 1102 loss=4.787, nll_loss=3.403, ppl=10.58, wps=54769, ups=15.27, wpb=3587.9, bsz=135.7, num_updates=8600, lr=0.000482243, gnorm=1.155, loss_scale=16, train_wall=6, gb_free=13.6, wall=622
2023-03-11 05:26:07 | INFO | train_inner | epoch 008:    986 / 1102 loss=4.732, nll_loss=3.343, ppl=10.15, wps=52919.6, ups=15, wpb=3528.6, bsz=149.2, num_updates=8700, lr=0.000479463, gnorm=1.194, loss_scale=16, train_wall=6, gb_free=13.6, wall=629
2023-03-11 05:26:14 | INFO | train_inner | epoch 008:   1086 / 1102 loss=4.742, nll_loss=3.353, ppl=10.22, wps=55109.2, ups=15.25, wpb=3613.1, bsz=139.4, num_updates=8800, lr=0.000476731, gnorm=1.158, loss_scale=16, train_wall=6, gb_free=13.6, wall=635
2023-03-11 05:26:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:26:16 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.579 | nll_loss 3.024 | ppl 8.14 | wps 138322 | wpb 2790.1 | bsz 113.8 | num_updates 8816 | best_loss 4.579
2023-03-11 05:26:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8816 updates
2023-03-11 05:26:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:26:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 8 @ 8816 updates, score 4.579) (writing took 7.139281634998042 seconds)
2023-03-11 05:26:23 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-03-11 05:26:23 | INFO | train | epoch 008 | loss 4.779 | nll_loss 3.395 | ppl 10.52 | wps 48715.8 | ups 13.6 | wpb 3581.5 | bsz 145.4 | num_updates 8816 | lr 0.000476298 | gnorm 1.191 | loss_scale 16 | train_wall 70 | gb_free 13.6 | wall 645
2023-03-11 05:26:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:26:23 | INFO | fairseq.trainer | begin training epoch 9
2023-03-11 05:26:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:26:29 | INFO | train_inner | epoch 009:     84 / 1102 loss=4.615, nll_loss=3.209, ppl=9.25, wps=23753.4, ups=6.57, wpb=3616.2, bsz=142.1, num_updates=8900, lr=0.000474045, gnorm=1.151, loss_scale=16, train_wall=6, gb_free=13.6, wall=651
2023-03-11 05:26:36 | INFO | train_inner | epoch 009:    184 / 1102 loss=4.585, nll_loss=3.174, ppl=9.03, wps=54391.1, ups=15.19, wpb=3581.4, bsz=135.9, num_updates=9000, lr=0.000471405, gnorm=1.115, loss_scale=16, train_wall=6, gb_free=13.6, wall=657
2023-03-11 05:26:42 | INFO | train_inner | epoch 009:    284 / 1102 loss=4.605, nll_loss=3.197, ppl=9.17, wps=54813.5, ups=15.21, wpb=3604.2, bsz=145.2, num_updates=9100, lr=0.000468807, gnorm=1.194, loss_scale=16, train_wall=6, gb_free=13.5, wall=664
2023-03-11 05:26:49 | INFO | train_inner | epoch 009:    384 / 1102 loss=4.59, nll_loss=3.179, ppl=9.06, wps=54218.5, ups=15.19, wpb=3568.3, bsz=136.3, num_updates=9200, lr=0.000466252, gnorm=1.135, loss_scale=16, train_wall=6, gb_free=13.5, wall=670
2023-03-11 05:26:55 | INFO | train_inner | epoch 009:    484 / 1102 loss=4.531, nll_loss=3.113, ppl=8.65, wps=55293.6, ups=15.26, wpb=3622.8, bsz=152.6, num_updates=9300, lr=0.000463739, gnorm=1.12, loss_scale=16, train_wall=6, gb_free=13.5, wall=677
2023-03-11 05:27:02 | INFO | train_inner | epoch 009:    584 / 1102 loss=4.526, nll_loss=3.107, ppl=8.62, wps=54645.3, ups=15.37, wpb=3554.4, bsz=149.8, num_updates=9400, lr=0.000461266, gnorm=1.17, loss_scale=16, train_wall=6, gb_free=13.5, wall=683
2023-03-11 05:27:08 | INFO | train_inner | epoch 009:    684 / 1102 loss=4.584, nll_loss=3.174, ppl=9.03, wps=54267.8, ups=15.19, wpb=3571.8, bsz=144.4, num_updates=9500, lr=0.000458831, gnorm=1.185, loss_scale=16, train_wall=6, gb_free=13.5, wall=690
2023-03-11 05:27:15 | INFO | train_inner | epoch 009:    784 / 1102 loss=4.494, nll_loss=3.073, ppl=8.41, wps=54611.7, ups=15.26, wpb=3578.1, bsz=153.5, num_updates=9600, lr=0.000456435, gnorm=1.091, loss_scale=16, train_wall=6, gb_free=13.6, wall=697
2023-03-11 05:27:22 | INFO | train_inner | epoch 009:    884 / 1102 loss=4.578, nll_loss=3.167, ppl=8.98, wps=54260.3, ups=15.32, wpb=3540.9, bsz=140.1, num_updates=9700, lr=0.000454077, gnorm=1.156, loss_scale=16, train_wall=6, gb_free=13.6, wall=703
2023-03-11 05:27:28 | INFO | train_inner | epoch 009:    984 / 1102 loss=4.557, nll_loss=3.144, ppl=8.84, wps=54156, ups=15.22, wpb=3557.6, bsz=145.1, num_updates=9800, lr=0.000451754, gnorm=1.143, loss_scale=16, train_wall=6, gb_free=13.6, wall=710
2023-03-11 05:27:35 | INFO | train_inner | epoch 009:   1084 / 1102 loss=4.494, nll_loss=3.073, ppl=8.42, wps=54714.9, ups=15.16, wpb=3610.2, bsz=148.3, num_updates=9900, lr=0.000449467, gnorm=1.057, loss_scale=16, train_wall=6, gb_free=13.5, wall=716
2023-03-11 05:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:27:37 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.452 | nll_loss 2.892 | ppl 7.42 | wps 138880 | wpb 2790.1 | bsz 113.8 | num_updates 9918 | best_loss 4.452
2023-03-11 05:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9918 updates
2023-03-11 05:27:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:27:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:27:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 9 @ 9918 updates, score 4.452) (writing took 7.055757882000762 seconds)
2023-03-11 05:27:44 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-03-11 05:27:44 | INFO | train | epoch 009 | loss 4.556 | nll_loss 3.143 | ppl 8.83 | wps 48775.9 | ups 13.62 | wpb 3581.5 | bsz 145.4 | num_updates 9918 | lr 0.000449059 | gnorm 1.137 | loss_scale 16 | train_wall 70 | gb_free 13.5 | wall 726
2023-03-11 05:27:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:27:44 | INFO | fairseq.trainer | begin training epoch 10
2023-03-11 05:27:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:27:50 | INFO | train_inner | epoch 010:     82 / 1102 loss=4.39, nll_loss=2.955, ppl=7.76, wps=23777.4, ups=6.6, wpb=3600.4, bsz=146.8, num_updates=10000, lr=0.000447214, gnorm=1.09, loss_scale=16, train_wall=6, gb_free=13.7, wall=731
2023-03-11 05:27:56 | INFO | train_inner | epoch 010:    182 / 1102 loss=4.413, nll_loss=2.98, ppl=7.89, wps=53767.1, ups=15.33, wpb=3506.7, bsz=138.1, num_updates=10100, lr=0.000444994, gnorm=1.134, loss_scale=16, train_wall=6, gb_free=13.6, wall=738
2023-03-11 05:28:03 | INFO | train_inner | epoch 010:    282 / 1102 loss=4.363, nll_loss=2.923, ppl=7.58, wps=55038.9, ups=15.23, wpb=3614.9, bsz=148.6, num_updates=10200, lr=0.000442807, gnorm=1.076, loss_scale=16, train_wall=6, gb_free=13.5, wall=745
2023-03-11 05:28:09 | INFO | train_inner | epoch 010:    382 / 1102 loss=4.359, nll_loss=2.919, ppl=7.56, wps=54477.5, ups=15.32, wpb=3555.9, bsz=149.1, num_updates=10300, lr=0.000440653, gnorm=1.056, loss_scale=16, train_wall=6, gb_free=13.6, wall=751
2023-03-11 05:28:16 | INFO | train_inner | epoch 010:    482 / 1102 loss=4.458, nll_loss=3.031, ppl=8.18, wps=54497.3, ups=15.32, wpb=3557.8, bsz=135.8, num_updates=10400, lr=0.000438529, gnorm=1.149, loss_scale=16, train_wall=6, gb_free=13.6, wall=758
2023-03-11 05:28:23 | INFO | train_inner | epoch 010:    582 / 1102 loss=4.371, nll_loss=2.933, ppl=7.64, wps=54710, ups=15.21, wpb=3597.3, bsz=146.7, num_updates=10500, lr=0.000436436, gnorm=1.074, loss_scale=16, train_wall=6, gb_free=13.5, wall=764
2023-03-11 05:28:29 | INFO | train_inner | epoch 010:    682 / 1102 loss=4.443, nll_loss=3.015, ppl=8.08, wps=54250.6, ups=15.26, wpb=3554.6, bsz=140, num_updates=10600, lr=0.000434372, gnorm=1.124, loss_scale=16, train_wall=6, gb_free=13.6, wall=771
2023-03-11 05:28:36 | INFO | train_inner | epoch 010:    782 / 1102 loss=4.361, nll_loss=2.922, ppl=7.58, wps=55623.4, ups=15.08, wpb=3688.9, bsz=149.2, num_updates=10700, lr=0.000432338, gnorm=1.059, loss_scale=16, train_wall=6, gb_free=13.5, wall=777
2023-03-11 05:28:42 | INFO | train_inner | epoch 010:    882 / 1102 loss=4.438, nll_loss=3.01, ppl=8.05, wps=54276.2, ups=15.16, wpb=3580.9, bsz=133.1, num_updates=10800, lr=0.000430331, gnorm=1.106, loss_scale=16, train_wall=6, gb_free=13.6, wall=784
2023-03-11 05:28:49 | INFO | train_inner | epoch 010:    982 / 1102 loss=4.352, nll_loss=2.912, ppl=7.53, wps=53789.1, ups=14.97, wpb=3593.1, bsz=155.2, num_updates=10900, lr=0.000428353, gnorm=1.076, loss_scale=16, train_wall=7, gb_free=14.2, wall=791
2023-03-11 05:28:56 | INFO | train_inner | epoch 010:   1082 / 1102 loss=4.336, nll_loss=2.896, ppl=7.44, wps=53664.6, ups=15.02, wpb=3572.2, bsz=169, num_updates=11000, lr=0.000426401, gnorm=1.118, loss_scale=16, train_wall=6, gb_free=13.5, wall=797
2023-03-11 05:28:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:28:58 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.361 | nll_loss 2.784 | ppl 6.89 | wps 137900 | wpb 2790.1 | bsz 113.8 | num_updates 11020 | best_loss 4.361
2023-03-11 05:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 11020 updates
2023-03-11 05:28:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:29:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:29:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 10 @ 11020 updates, score 4.361) (writing took 7.118649848998757 seconds)
2023-03-11 05:29:06 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-03-11 05:29:06 | INFO | train | epoch 010 | loss 4.391 | nll_loss 2.955 | ppl 7.76 | wps 48614.5 | ups 13.57 | wpb 3581.5 | bsz 145.4 | num_updates 11020 | lr 0.000426014 | gnorm 1.097 | loss_scale 16 | train_wall 71 | gb_free 13.6 | wall 807
2023-03-11 05:29:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:29:06 | INFO | fairseq.trainer | begin training epoch 11
2023-03-11 05:29:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:29:11 | INFO | train_inner | epoch 011:     80 / 1102 loss=4.323, nll_loss=2.88, ppl=7.36, wps=22880.2, ups=6.54, wpb=3496.3, bsz=133, num_updates=11100, lr=0.000424476, gnorm=1.092, loss_scale=16, train_wall=6, gb_free=13.6, wall=813
2023-03-11 05:29:18 | INFO | train_inner | epoch 011:    180 / 1102 loss=4.255, nll_loss=2.802, ppl=6.97, wps=55674.5, ups=15.26, wpb=3648.7, bsz=140.7, num_updates=11200, lr=0.000422577, gnorm=1.043, loss_scale=16, train_wall=6, gb_free=13.6, wall=819
2023-03-11 05:29:24 | INFO | train_inner | epoch 011:    280 / 1102 loss=4.307, nll_loss=2.86, ppl=7.26, wps=55748.8, ups=15.36, wpb=3629, bsz=134.3, num_updates=11300, lr=0.000420703, gnorm=1.066, loss_scale=16, train_wall=6, gb_free=13.5, wall=826
2023-03-11 05:29:31 | INFO | train_inner | epoch 011:    380 / 1102 loss=4.206, nll_loss=2.746, ppl=6.71, wps=55219.7, ups=15.3, wpb=3608.7, bsz=163.9, num_updates=11400, lr=0.000418854, gnorm=1.05, loss_scale=16, train_wall=6, gb_free=13.5, wall=832
2023-03-11 05:29:37 | INFO | train_inner | epoch 011:    480 / 1102 loss=4.219, nll_loss=2.762, ppl=6.78, wps=55355.6, ups=15.41, wpb=3592.5, bsz=155, num_updates=11500, lr=0.000417029, gnorm=1.057, loss_scale=16, train_wall=6, gb_free=13.6, wall=839
2023-03-11 05:29:44 | INFO | train_inner | epoch 011:    580 / 1102 loss=4.251, nll_loss=2.797, ppl=6.95, wps=54670.1, ups=15.23, wpb=3590.8, bsz=151, num_updates=11600, lr=0.000415227, gnorm=1.057, loss_scale=16, train_wall=6, gb_free=13.5, wall=845
2023-03-11 05:29:50 | INFO | train_inner | epoch 011:    680 / 1102 loss=4.238, nll_loss=2.784, ppl=6.89, wps=55410.5, ups=15.3, wpb=3621.9, bsz=155.1, num_updates=11700, lr=0.000413449, gnorm=1.042, loss_scale=16, train_wall=6, gb_free=13.6, wall=852
2023-03-11 05:29:57 | INFO | train_inner | epoch 011:    780 / 1102 loss=4.265, nll_loss=2.814, ppl=7.03, wps=53947.7, ups=15.54, wpb=3471.7, bsz=151.6, num_updates=11800, lr=0.000411693, gnorm=1.132, loss_scale=16, train_wall=6, gb_free=13.5, wall=858
2023-03-11 05:30:03 | INFO | train_inner | epoch 011:    880 / 1102 loss=4.275, nll_loss=2.825, ppl=7.08, wps=54463.5, ups=15.19, wpb=3585.8, bsz=136.6, num_updates=11900, lr=0.00040996, gnorm=1.056, loss_scale=16, train_wall=6, gb_free=13.6, wall=865
2023-03-11 05:30:10 | INFO | train_inner | epoch 011:    980 / 1102 loss=4.294, nll_loss=2.847, ppl=7.19, wps=54247.1, ups=15.3, wpb=3545, bsz=134.1, num_updates=12000, lr=0.000408248, gnorm=1.083, loss_scale=16, train_wall=6, gb_free=13.5, wall=871
2023-03-11 05:30:16 | INFO | train_inner | epoch 011:   1080 / 1102 loss=4.335, nll_loss=2.894, ppl=7.43, wps=54657.7, ups=15.28, wpb=3577.7, bsz=133.2, num_updates=12100, lr=0.000406558, gnorm=1.1, loss_scale=16, train_wall=6, gb_free=13.6, wall=878
2023-03-11 05:30:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:30:19 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.327 | nll_loss 2.754 | ppl 6.75 | wps 140166 | wpb 2790.1 | bsz 113.8 | num_updates 12122 | best_loss 4.327
2023-03-11 05:30:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 12122 updates
2023-03-11 05:30:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:30:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 11 @ 12122 updates, score 4.327) (writing took 7.120059884000511 seconds)
2023-03-11 05:30:26 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-03-11 05:30:26 | INFO | train | epoch 011 | loss 4.263 | nll_loss 2.812 | ppl 7.02 | wps 48913.8 | ups 13.66 | wpb 3581.5 | bsz 145.4 | num_updates 12122 | lr 0.000406189 | gnorm 1.068 | loss_scale 16 | train_wall 70 | gb_free 13.6 | wall 888
2023-03-11 05:30:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:30:26 | INFO | fairseq.trainer | begin training epoch 12
2023-03-11 05:30:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:30:31 | INFO | train_inner | epoch 012:     78 / 1102 loss=4.146, nll_loss=2.678, ppl=6.4, wps=23646.6, ups=6.6, wpb=3580.1, bsz=140, num_updates=12200, lr=0.000404888, gnorm=1.025, loss_scale=16, train_wall=6, gb_free=13.6, wall=893
2023-03-11 05:30:38 | INFO | train_inner | epoch 012:    178 / 1102 loss=4.134, nll_loss=2.664, ppl=6.34, wps=55376, ups=15.22, wpb=3638.3, bsz=145.8, num_updates=12300, lr=0.000403239, gnorm=1.049, loss_scale=16, train_wall=6, gb_free=13.5, wall=900
2023-03-11 05:30:45 | INFO | train_inner | epoch 012:    278 / 1102 loss=4.08, nll_loss=2.603, ppl=6.07, wps=55224.2, ups=15.33, wpb=3601.8, bsz=164.1, num_updates=12400, lr=0.00040161, gnorm=1.027, loss_scale=16, train_wall=6, gb_free=13.6, wall=906
2023-03-11 05:30:51 | INFO | train_inner | epoch 012:    378 / 1102 loss=4.165, nll_loss=2.7, ppl=6.5, wps=54381.3, ups=15.1, wpb=3601.1, bsz=145.4, num_updates=12500, lr=0.0004, gnorm=1.059, loss_scale=16, train_wall=6, gb_free=13.4, wall=913
2023-03-11 05:30:58 | INFO | train_inner | epoch 012:    478 / 1102 loss=4.157, nll_loss=2.69, ppl=6.45, wps=54925.4, ups=15.22, wpb=3608.1, bsz=141.9, num_updates=12600, lr=0.00039841, gnorm=1.064, loss_scale=16, train_wall=6, gb_free=13.5, wall=919
2023-03-11 05:31:04 | INFO | train_inner | epoch 012:    578 / 1102 loss=4.159, nll_loss=2.693, ppl=6.47, wps=54394.6, ups=15.19, wpb=3581.2, bsz=143.4, num_updates=12700, lr=0.000396838, gnorm=1.06, loss_scale=16, train_wall=6, gb_free=13.6, wall=926
2023-03-11 05:31:11 | INFO | train_inner | epoch 012:    678 / 1102 loss=4.192, nll_loss=2.731, ppl=6.64, wps=55389.7, ups=15.38, wpb=3601.9, bsz=141.4, num_updates=12800, lr=0.000395285, gnorm=1.048, loss_scale=16, train_wall=6, gb_free=13.5, wall=932
2023-03-11 05:31:17 | INFO | train_inner | epoch 012:    778 / 1102 loss=4.22, nll_loss=2.763, ppl=6.79, wps=54516.5, ups=15.32, wpb=3559.2, bsz=134.6, num_updates=12900, lr=0.00039375, gnorm=1.085, loss_scale=16, train_wall=6, gb_free=13.6, wall=939
2023-03-11 05:31:24 | INFO | train_inner | epoch 012:    878 / 1102 loss=4.16, nll_loss=2.696, ppl=6.48, wps=54391.1, ups=15.36, wpb=3540.9, bsz=153.9, num_updates=13000, lr=0.000392232, gnorm=1.034, loss_scale=16, train_wall=6, gb_free=13.5, wall=945
2023-03-11 05:31:30 | INFO | train_inner | epoch 012:    978 / 1102 loss=4.179, nll_loss=2.717, ppl=6.57, wps=55451.4, ups=15.29, wpb=3627.4, bsz=146.5, num_updates=13100, lr=0.000390732, gnorm=1.035, loss_scale=16, train_wall=6, gb_free=13.4, wall=952
2023-03-11 05:31:37 | INFO | train_inner | epoch 012:   1078 / 1102 loss=4.2, nll_loss=2.742, ppl=6.69, wps=53216.9, ups=15.29, wpb=3480, bsz=139.2, num_updates=13200, lr=0.000389249, gnorm=1.094, loss_scale=16, train_wall=6, gb_free=13.6, wall=959
2023-03-11 05:31:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:31:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.208 | nll_loss 2.617 | ppl 6.14 | wps 139472 | wpb 2790.1 | bsz 113.8 | num_updates 13224 | best_loss 4.208
2023-03-11 05:31:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 13224 updates
2023-03-11 05:31:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:31:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:31:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 12 @ 13224 updates, score 4.208) (writing took 7.130428792999737 seconds)
2023-03-11 05:31:47 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-03-11 05:31:47 | INFO | train | epoch 012 | loss 4.16 | nll_loss 2.695 | ppl 6.48 | wps 48836.5 | ups 13.64 | wpb 3581.5 | bsz 145.4 | num_updates 13224 | lr 0.000388896 | gnorm 1.053 | loss_scale 16 | train_wall 70 | gb_free 13.5 | wall 969
2023-03-11 05:31:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:31:47 | INFO | fairseq.trainer | begin training epoch 13
2023-03-11 05:31:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:31:52 | INFO | train_inner | epoch 013:     76 / 1102 loss=4.022, nll_loss=2.539, ppl=5.81, wps=23237.1, ups=6.58, wpb=3530.3, bsz=154.7, num_updates=13300, lr=0.000387783, gnorm=1.009, loss_scale=16, train_wall=6, gb_free=13.5, wall=974
2023-03-11 05:31:59 | INFO | train_inner | epoch 013:    176 / 1102 loss=4.023, nll_loss=2.539, ppl=5.81, wps=54918.3, ups=15.28, wpb=3593.5, bsz=154.6, num_updates=13400, lr=0.000386334, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=13.5, wall=980
2023-03-11 05:32:05 | INFO | train_inner | epoch 013:    276 / 1102 loss=4.115, nll_loss=2.643, ppl=6.25, wps=54576.6, ups=15.2, wpb=3589.9, bsz=132.2, num_updates=13500, lr=0.0003849, gnorm=1.055, loss_scale=16, train_wall=6, gb_free=13.5, wall=987
2023-03-11 05:32:12 | INFO | train_inner | epoch 013:    376 / 1102 loss=4.084, nll_loss=2.606, ppl=6.09, wps=54573.1, ups=15.38, wpb=3547.3, bsz=143.8, num_updates=13600, lr=0.000383482, gnorm=1.084, loss_scale=16, train_wall=6, gb_free=13.5, wall=993
2023-03-11 05:32:18 | INFO | train_inner | epoch 013:    476 / 1102 loss=4.039, nll_loss=2.557, ppl=5.89, wps=54916.5, ups=15.12, wpb=3632.5, bsz=152.4, num_updates=13700, lr=0.00038208, gnorm=1.026, loss_scale=16, train_wall=6, gb_free=13.6, wall=1000
2023-03-11 05:32:25 | INFO | train_inner | epoch 013:    576 / 1102 loss=4.029, nll_loss=2.547, ppl=5.84, wps=55311.3, ups=15.37, wpb=3598.7, bsz=157.6, num_updates=13800, lr=0.000380693, gnorm=1.029, loss_scale=16, train_wall=6, gb_free=13.5, wall=1006
2023-03-11 05:32:31 | INFO | train_inner | epoch 013:    676 / 1102 loss=4.082, nll_loss=2.606, ppl=6.09, wps=55337, ups=15.32, wpb=3611.5, bsz=144.5, num_updates=13900, lr=0.000379322, gnorm=1.031, loss_scale=16, train_wall=6, gb_free=13.7, wall=1013
2023-03-11 05:32:38 | INFO | train_inner | epoch 013:    776 / 1102 loss=4.131, nll_loss=2.662, ppl=6.33, wps=54519.7, ups=15.32, wpb=3557.7, bsz=137.7, num_updates=14000, lr=0.000377964, gnorm=1.088, loss_scale=16, train_wall=6, gb_free=13.5, wall=1020
2023-03-11 05:32:45 | INFO | train_inner | epoch 013:    876 / 1102 loss=4.124, nll_loss=2.654, ppl=6.3, wps=54195.3, ups=15.21, wpb=3564, bsz=136.9, num_updates=14100, lr=0.000376622, gnorm=1.06, loss_scale=16, train_wall=6, gb_free=13.7, wall=1026
2023-03-11 05:32:51 | INFO | train_inner | epoch 013:    976 / 1102 loss=4.097, nll_loss=2.625, ppl=6.17, wps=55445.8, ups=15.38, wpb=3606, bsz=145.8, num_updates=14200, lr=0.000375293, gnorm=1.04, loss_scale=16, train_wall=6, gb_free=13.5, wall=1033
2023-03-11 05:32:58 | INFO | train_inner | epoch 013:   1076 / 1102 loss=4.077, nll_loss=2.602, ppl=6.07, wps=54549.5, ups=15.2, wpb=3588.6, bsz=148.1, num_updates=14300, lr=0.000373979, gnorm=1.033, loss_scale=16, train_wall=6, gb_free=13.7, wall=1039
2023-03-11 05:32:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:33:01 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.194 | nll_loss 2.607 | ppl 6.09 | wps 138079 | wpb 2790.1 | bsz 113.8 | num_updates 14326 | best_loss 4.194
2023-03-11 05:33:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 14326 updates
2023-03-11 05:33:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:33:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 13 @ 14326 updates, score 4.194) (writing took 7.068048178000026 seconds)
2023-03-11 05:33:08 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-03-11 05:33:08 | INFO | train | epoch 013 | loss 4.077 | nll_loss 2.601 | ppl 6.07 | wps 48898.5 | ups 13.65 | wpb 3581.5 | bsz 145.4 | num_updates 14326 | lr 0.000373639 | gnorm 1.044 | loss_scale 16 | train_wall 70 | gb_free 13.5 | wall 1049
2023-03-11 05:33:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:33:08 | INFO | fairseq.trainer | begin training epoch 14
2023-03-11 05:33:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:33:13 | INFO | train_inner | epoch 014:     74 / 1102 loss=4.026, nll_loss=2.543, ppl=5.83, wps=23280, ups=6.6, wpb=3526.1, bsz=141.6, num_updates=14400, lr=0.000372678, gnorm=1.044, loss_scale=16, train_wall=6, gb_free=13.6, wall=1054
2023-03-11 05:33:19 | INFO | train_inner | epoch 014:    174 / 1102 loss=3.997, nll_loss=2.508, ppl=5.69, wps=54207.8, ups=15.43, wpb=3512.2, bsz=138.5, num_updates=14500, lr=0.000371391, gnorm=1.056, loss_scale=16, train_wall=6, gb_free=13.6, wall=1061
2023-03-11 05:33:26 | INFO | train_inner | epoch 014:    274 / 1102 loss=3.977, nll_loss=2.487, ppl=5.61, wps=56352.3, ups=15.43, wpb=3651.7, bsz=150.2, num_updates=14600, lr=0.000370117, gnorm=1.044, loss_scale=16, train_wall=6, gb_free=13.4, wall=1067
2023-03-11 05:33:32 | INFO | train_inner | epoch 014:    374 / 1102 loss=4.02, nll_loss=2.535, ppl=5.8, wps=54669.5, ups=15.35, wpb=3562.1, bsz=139.2, num_updates=14700, lr=0.000368856, gnorm=1.042, loss_scale=16, train_wall=6, gb_free=13.5, wall=1074
2023-03-11 05:33:39 | INFO | train_inner | epoch 014:    474 / 1102 loss=4.025, nll_loss=2.541, ppl=5.82, wps=55706.2, ups=15.3, wpb=3640.8, bsz=137, num_updates=14800, lr=0.000367607, gnorm=1.024, loss_scale=16, train_wall=6, gb_free=13.5, wall=1080
2023-03-11 05:33:45 | INFO | train_inner | epoch 014:    574 / 1102 loss=4.058, nll_loss=2.58, ppl=5.98, wps=53967.6, ups=15.29, wpb=3530.2, bsz=134.9, num_updates=14900, lr=0.000366372, gnorm=1.07, loss_scale=16, train_wall=6, gb_free=13.6, wall=1087
2023-03-11 05:33:52 | INFO | train_inner | epoch 014:    674 / 1102 loss=3.962, nll_loss=2.47, ppl=5.54, wps=53550.2, ups=15.38, wpb=3481.1, bsz=157, num_updates=15000, lr=0.000365148, gnorm=1.04, loss_scale=16, train_wall=6, gb_free=13.7, wall=1093
2023-03-11 05:33:58 | INFO | train_inner | epoch 014:    774 / 1102 loss=3.995, nll_loss=2.508, ppl=5.69, wps=56215.4, ups=15.41, wpb=3648, bsz=150.4, num_updates=15100, lr=0.000363937, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=13.5, wall=1100
2023-03-11 05:34:05 | INFO | train_inner | epoch 014:    874 / 1102 loss=3.997, nll_loss=2.511, ppl=5.7, wps=55699.4, ups=15.31, wpb=3639.3, bsz=155.7, num_updates=15200, lr=0.000362738, gnorm=1.025, loss_scale=16, train_wall=6, gb_free=13.6, wall=1106
2023-03-11 05:34:11 | INFO | train_inner | epoch 014:    974 / 1102 loss=4.043, nll_loss=2.563, ppl=5.91, wps=55185.4, ups=15.33, wpb=3600.1, bsz=137.8, num_updates=15300, lr=0.000361551, gnorm=1.061, loss_scale=16, train_wall=6, gb_free=13.5, wall=1113
2023-03-11 05:34:18 | INFO | train_inner | epoch 014:   1074 / 1102 loss=3.988, nll_loss=2.501, ppl=5.66, wps=55450, ups=15.38, wpb=3606, bsz=151.6, num_updates=15400, lr=0.000360375, gnorm=1.007, loss_scale=16, train_wall=6, gb_free=13.6, wall=1119
2023-03-11 05:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:34:21 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.158 | nll_loss 2.562 | ppl 5.9 | wps 136825 | wpb 2790.1 | bsz 113.8 | num_updates 15428 | best_loss 4.158
2023-03-11 05:34:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 15428 updates
2023-03-11 05:34:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:34:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:34:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 14 @ 15428 updates, score 4.158) (writing took 7.098961588999373 seconds)
2023-03-11 05:34:28 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-03-11 05:34:28 | INFO | train | epoch 014 | loss 4.004 | nll_loss 2.518 | ppl 5.73 | wps 49050.4 | ups 13.7 | wpb 3581.5 | bsz 145.4 | num_updates 15428 | lr 0.000360048 | gnorm 1.037 | loss_scale 16 | train_wall 70 | gb_free 13.6 | wall 1130
2023-03-11 05:34:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:34:28 | INFO | fairseq.trainer | begin training epoch 15
2023-03-11 05:34:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:34:33 | INFO | train_inner | epoch 015:     72 / 1102 loss=3.944, nll_loss=2.45, ppl=5.46, wps=23181.3, ups=6.54, wpb=3547.1, bsz=147, num_updates=15500, lr=0.000359211, gnorm=1.064, loss_scale=16, train_wall=6, gb_free=13.5, wall=1135
2023-03-11 05:34:40 | INFO | train_inner | epoch 015:    172 / 1102 loss=3.937, nll_loss=2.441, ppl=5.43, wps=54059.8, ups=15.17, wpb=3562.6, bsz=142.1, num_updates=15600, lr=0.000358057, gnorm=1.146, loss_scale=16, train_wall=6, gb_free=13.6, wall=1141
2023-03-11 05:34:46 | INFO | train_inner | epoch 015:    272 / 1102 loss=3.946, nll_loss=2.451, ppl=5.47, wps=55235.1, ups=15.28, wpb=3614.5, bsz=134.2, num_updates=15700, lr=0.000356915, gnorm=1.033, loss_scale=16, train_wall=6, gb_free=13.6, wall=1148
2023-03-11 05:34:53 | INFO | train_inner | epoch 015:    372 / 1102 loss=3.913, nll_loss=2.414, ppl=5.33, wps=56045.8, ups=15.26, wpb=3672.8, bsz=149.9, num_updates=15800, lr=0.000355784, gnorm=0.981, loss_scale=16, train_wall=6, gb_free=13.6, wall=1154
2023-03-11 05:34:59 | INFO | train_inner | epoch 015:    472 / 1102 loss=3.88, nll_loss=2.378, ppl=5.2, wps=53503.7, ups=15.24, wpb=3511.4, bsz=164.3, num_updates=15900, lr=0.000354663, gnorm=1.044, loss_scale=16, train_wall=6, gb_free=13.9, wall=1161
2023-03-11 05:35:06 | INFO | train_inner | epoch 015:    572 / 1102 loss=3.985, nll_loss=2.496, ppl=5.64, wps=54070.9, ups=15.17, wpb=3565.5, bsz=135.3, num_updates=16000, lr=0.000353553, gnorm=1.023, loss_scale=16, train_wall=6, gb_free=13.6, wall=1168
2023-03-11 05:35:13 | INFO | train_inner | epoch 015:    672 / 1102 loss=3.908, nll_loss=2.41, ppl=5.32, wps=54632.6, ups=15.33, wpb=3562.9, bsz=152, num_updates=16100, lr=0.000352454, gnorm=1.027, loss_scale=16, train_wall=6, gb_free=13.6, wall=1174
2023-03-11 05:35:19 | INFO | train_inner | epoch 015:    772 / 1102 loss=3.97, nll_loss=2.479, ppl=5.57, wps=54108.6, ups=15.15, wpb=3570.6, bsz=139.6, num_updates=16200, lr=0.000351364, gnorm=1.022, loss_scale=16, train_wall=6, gb_free=13.6, wall=1181
2023-03-11 05:35:26 | INFO | train_inner | epoch 015:    872 / 1102 loss=3.947, nll_loss=2.455, ppl=5.48, wps=55284.7, ups=15.36, wpb=3599, bsz=145, num_updates=16300, lr=0.000350285, gnorm=1.022, loss_scale=16, train_wall=6, gb_free=13.7, wall=1187
2023-03-11 05:35:32 | INFO | train_inner | epoch 015:    972 / 1102 loss=3.953, nll_loss=2.461, ppl=5.51, wps=54567.9, ups=15.38, wpb=3546.9, bsz=154.6, num_updates=16400, lr=0.000349215, gnorm=1.058, loss_scale=32, train_wall=6, gb_free=13.5, wall=1194
2023-03-11 05:35:39 | INFO | train_inner | epoch 015:   1072 / 1102 loss=4.007, nll_loss=2.522, ppl=5.74, wps=55054.8, ups=15.31, wpb=3595.2, bsz=136.4, num_updates=16500, lr=0.000348155, gnorm=1.033, loss_scale=32, train_wall=6, gb_free=13.7, wall=1200
2023-03-11 05:35:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:35:42 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.103 | nll_loss 2.506 | ppl 5.68 | wps 140239 | wpb 2790.1 | bsz 113.8 | num_updates 16530 | best_loss 4.103
2023-03-11 05:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 16530 updates
2023-03-11 05:35:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:35:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:35:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 15 @ 16530 updates, score 4.103) (writing took 7.022210203002032 seconds)
2023-03-11 05:35:49 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-03-11 05:35:49 | INFO | train | epoch 015 | loss 3.943 | nll_loss 2.448 | ppl 5.46 | wps 48827.4 | ups 13.63 | wpb 3581.5 | bsz 145.4 | num_updates 16530 | lr 0.000347839 | gnorm 1.039 | loss_scale 32 | train_wall 70 | gb_free 13.7 | wall 1211
2023-03-11 05:35:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:35:49 | INFO | fairseq.trainer | begin training epoch 16
2023-03-11 05:35:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:35:54 | INFO | train_inner | epoch 016:     70 / 1102 loss=3.876, nll_loss=2.374, ppl=5.18, wps=24016.9, ups=6.65, wpb=3608.9, bsz=145.4, num_updates=16600, lr=0.000347105, gnorm=1.01, loss_scale=32, train_wall=6, gb_free=13.5, wall=1215
2023-03-11 05:36:00 | INFO | train_inner | epoch 016:    170 / 1102 loss=3.849, nll_loss=2.341, ppl=5.07, wps=55048.9, ups=15.28, wpb=3603.4, bsz=141.7, num_updates=16700, lr=0.000346064, gnorm=0.994, loss_scale=32, train_wall=6, gb_free=13.6, wall=1222
2023-03-11 05:36:07 | INFO | train_inner | epoch 016:    270 / 1102 loss=3.859, nll_loss=2.353, ppl=5.11, wps=54910.1, ups=15.28, wpb=3593.3, bsz=150.1, num_updates=16800, lr=0.000345033, gnorm=1.029, loss_scale=32, train_wall=6, gb_free=13.6, wall=1228
2023-03-11 05:36:13 | INFO | train_inner | epoch 016:    370 / 1102 loss=3.897, nll_loss=2.395, ppl=5.26, wps=54392.3, ups=15.44, wpb=3522.6, bsz=133, num_updates=16900, lr=0.00034401, gnorm=1.049, loss_scale=32, train_wall=6, gb_free=13.6, wall=1235
2023-03-11 05:36:20 | INFO | train_inner | epoch 016:    470 / 1102 loss=3.901, nll_loss=2.401, ppl=5.28, wps=54033.8, ups=15.34, wpb=3521.3, bsz=140.1, num_updates=17000, lr=0.000342997, gnorm=1.058, loss_scale=32, train_wall=6, gb_free=13.6, wall=1241
2023-03-11 05:36:26 | INFO | train_inner | epoch 016:    570 / 1102 loss=3.87, nll_loss=2.367, ppl=5.16, wps=56324.4, ups=15.44, wpb=3649, bsz=154.2, num_updates=17100, lr=0.000341993, gnorm=0.987, loss_scale=32, train_wall=6, gb_free=13.5, wall=1248
2023-03-11 05:36:33 | INFO | train_inner | epoch 016:    670 / 1102 loss=3.884, nll_loss=2.382, ppl=5.21, wps=54701, ups=15.34, wpb=3566.5, bsz=151.4, num_updates=17200, lr=0.000340997, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.5, wall=1254
2023-03-11 05:36:39 | INFO | train_inner | epoch 016:    770 / 1102 loss=3.907, nll_loss=2.408, ppl=5.31, wps=55296.9, ups=15.27, wpb=3622.2, bsz=138.5, num_updates=17300, lr=0.00034001, gnorm=1.002, loss_scale=32, train_wall=6, gb_free=13.5, wall=1261
2023-03-11 05:36:46 | INFO | train_inner | epoch 016:    870 / 1102 loss=3.909, nll_loss=2.411, ppl=5.32, wps=55001.6, ups=15.31, wpb=3592, bsz=142.4, num_updates=17400, lr=0.000339032, gnorm=1.046, loss_scale=32, train_wall=6, gb_free=13.5, wall=1267
2023-03-11 05:36:53 | INFO | train_inner | epoch 016:    970 / 1102 loss=3.897, nll_loss=2.399, ppl=5.27, wps=53507.7, ups=15.04, wpb=3556.9, bsz=157, num_updates=17500, lr=0.000338062, gnorm=1.007, loss_scale=32, train_wall=6, gb_free=13.5, wall=1274
2023-03-11 05:36:59 | INFO | train_inner | epoch 016:   1070 / 1102 loss=3.885, nll_loss=2.384, ppl=5.22, wps=55552.1, ups=15.35, wpb=3620, bsz=151.8, num_updates=17600, lr=0.0003371, gnorm=1.019, loss_scale=32, train_wall=6, gb_free=13.4, wall=1281
2023-03-11 05:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:37:03 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.084 | nll_loss 2.482 | ppl 5.59 | wps 137751 | wpb 2790.1 | bsz 113.8 | num_updates 17632 | best_loss 4.084
2023-03-11 05:37:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 17632 updates
2023-03-11 05:37:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:37:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 16 @ 17632 updates, score 4.084) (writing took 7.044899214000907 seconds)
2023-03-11 05:37:10 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-03-11 05:37:10 | INFO | train | epoch 016 | loss 3.885 | nll_loss 2.383 | ppl 5.22 | wps 48987.3 | ups 13.68 | wpb 3581.5 | bsz 145.4 | num_updates 17632 | lr 0.000336794 | gnorm 1.025 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 1291
2023-03-11 05:37:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:37:10 | INFO | fairseq.trainer | begin training epoch 17
2023-03-11 05:37:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:37:14 | INFO | train_inner | epoch 017:     68 / 1102 loss=3.83, nll_loss=2.321, ppl=5, wps=23958.6, ups=6.63, wpb=3613, bsz=141.3, num_updates=17700, lr=0.000336146, gnorm=1.018, loss_scale=32, train_wall=6, gb_free=13.6, wall=1296
2023-03-11 05:37:21 | INFO | train_inner | epoch 017:    168 / 1102 loss=3.794, nll_loss=2.279, ppl=4.85, wps=54169.9, ups=15.24, wpb=3553.3, bsz=141.6, num_updates=17800, lr=0.000335201, gnorm=0.99, loss_scale=32, train_wall=6, gb_free=13.5, wall=1302
2023-03-11 05:37:27 | INFO | train_inner | epoch 017:    268 / 1102 loss=3.808, nll_loss=2.295, ppl=4.91, wps=52892.7, ups=15.32, wpb=3451.5, bsz=145, num_updates=17900, lr=0.000334263, gnorm=1.05, loss_scale=32, train_wall=6, gb_free=13.5, wall=1309
2023-03-11 05:37:34 | INFO | train_inner | epoch 017:    368 / 1102 loss=3.803, nll_loss=2.289, ppl=4.89, wps=55629.2, ups=15.44, wpb=3603.5, bsz=151, num_updates=18000, lr=0.000333333, gnorm=1.004, loss_scale=32, train_wall=6, gb_free=13.6, wall=1315
2023-03-11 05:37:40 | INFO | train_inner | epoch 017:    468 / 1102 loss=3.852, nll_loss=2.345, ppl=5.08, wps=54246.2, ups=15.39, wpb=3525.4, bsz=145.8, num_updates=18100, lr=0.000332411, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.5, wall=1322
2023-03-11 05:37:47 | INFO | train_inner | epoch 017:    568 / 1102 loss=3.84, nll_loss=2.332, ppl=5.04, wps=54808.8, ups=15.27, wpb=3589.3, bsz=150.7, num_updates=18200, lr=0.000331497, gnorm=1.024, loss_scale=32, train_wall=6, gb_free=13.5, wall=1328
2023-03-11 05:37:53 | INFO | train_inner | epoch 017:    668 / 1102 loss=3.827, nll_loss=2.318, ppl=4.99, wps=55842.1, ups=15.36, wpb=3634.9, bsz=153.4, num_updates=18300, lr=0.00033059, gnorm=0.996, loss_scale=32, train_wall=6, gb_free=13.5, wall=1335
2023-03-11 05:38:00 | INFO | train_inner | epoch 017:    768 / 1102 loss=3.825, nll_loss=2.315, ppl=4.98, wps=56255, ups=15.26, wpb=3687.4, bsz=142.6, num_updates=18400, lr=0.00032969, gnorm=0.984, loss_scale=32, train_wall=6, gb_free=13.5, wall=1341
2023-03-11 05:38:06 | INFO | train_inner | epoch 017:    868 / 1102 loss=3.824, nll_loss=2.315, ppl=4.98, wps=54519.2, ups=15.21, wpb=3585.3, bsz=157, num_updates=18500, lr=0.000328798, gnorm=1.023, loss_scale=32, train_wall=6, gb_free=13.6, wall=1348
2023-03-11 05:38:13 | INFO | train_inner | epoch 017:    968 / 1102 loss=3.891, nll_loss=2.389, ppl=5.24, wps=54342.2, ups=15.17, wpb=3582.5, bsz=132.8, num_updates=18600, lr=0.000327913, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.5, wall=1355
2023-03-11 05:38:20 | INFO | train_inner | epoch 017:   1068 / 1102 loss=3.897, nll_loss=2.398, ppl=5.27, wps=53162.4, ups=15.04, wpb=3533.6, bsz=143.3, num_updates=18700, lr=0.000327035, gnorm=1.071, loss_scale=32, train_wall=6, gb_free=13.6, wall=1361
2023-03-11 05:38:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:38:23 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.061 | nll_loss 2.454 | ppl 5.48 | wps 141521 | wpb 2790.1 | bsz 113.8 | num_updates 18734 | best_loss 4.061
2023-03-11 05:38:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 18734 updates
2023-03-11 05:38:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:38:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 17 @ 18734 updates, score 4.061) (writing took 7.032938092001132 seconds)
2023-03-11 05:38:30 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-03-11 05:38:30 | INFO | train | epoch 017 | loss 3.834 | nll_loss 2.325 | ppl 5.01 | wps 48908.8 | ups 13.66 | wpb 3581.5 | bsz 145.4 | num_updates 18734 | lr 0.000326738 | gnorm 1.02 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 1372
2023-03-11 05:38:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:38:30 | INFO | fairseq.trainer | begin training epoch 18
2023-03-11 05:38:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:38:35 | INFO | train_inner | epoch 018:     66 / 1102 loss=3.821, nll_loss=2.308, ppl=4.95, wps=23435.4, ups=6.63, wpb=3535.3, bsz=121.3, num_updates=18800, lr=0.000326164, gnorm=1.028, loss_scale=32, train_wall=6, gb_free=13.5, wall=1376
2023-03-11 05:38:41 | INFO | train_inner | epoch 018:    166 / 1102 loss=3.774, nll_loss=2.255, ppl=4.77, wps=54946.9, ups=15.39, wpb=3570.2, bsz=136.2, num_updates=18900, lr=0.0003253, gnorm=0.984, loss_scale=32, train_wall=6, gb_free=13.6, wall=1383
2023-03-11 05:38:48 | INFO | train_inner | epoch 018:    266 / 1102 loss=3.784, nll_loss=2.267, ppl=4.81, wps=55492, ups=15.29, wpb=3629.6, bsz=138.2, num_updates=19000, lr=0.000324443, gnorm=1, loss_scale=32, train_wall=6, gb_free=13.6, wall=1389
2023-03-11 05:38:54 | INFO | train_inner | epoch 018:    366 / 1102 loss=3.781, nll_loss=2.264, ppl=4.8, wps=54936.2, ups=15.24, wpb=3604.8, bsz=144.6, num_updates=19100, lr=0.000323592, gnorm=1.026, loss_scale=32, train_wall=6, gb_free=13.5, wall=1396
2023-03-11 05:39:01 | INFO | train_inner | epoch 018:    466 / 1102 loss=3.767, nll_loss=2.248, ppl=4.75, wps=54536.9, ups=15.21, wpb=3586.1, bsz=145.9, num_updates=19200, lr=0.000322749, gnorm=0.995, loss_scale=32, train_wall=6, gb_free=13.5, wall=1403
2023-03-11 05:39:08 | INFO | train_inner | epoch 018:    566 / 1102 loss=3.795, nll_loss=2.281, ppl=4.86, wps=54918.4, ups=15.23, wpb=3605.9, bsz=152, num_updates=19300, lr=0.000321911, gnorm=1.033, loss_scale=32, train_wall=6, gb_free=13.6, wall=1409
2023-03-11 05:39:14 | INFO | train_inner | epoch 018:    666 / 1102 loss=3.762, nll_loss=2.244, ppl=4.74, wps=54632.7, ups=15.36, wpb=3557.5, bsz=155.9, num_updates=19400, lr=0.000321081, gnorm=1.011, loss_scale=32, train_wall=6, gb_free=13.6, wall=1416
2023-03-11 05:39:21 | INFO | train_inner | epoch 018:    766 / 1102 loss=3.747, nll_loss=2.229, ppl=4.69, wps=55351.9, ups=15.14, wpb=3656.2, bsz=169.6, num_updates=19500, lr=0.000320256, gnorm=0.985, loss_scale=32, train_wall=6, gb_free=13.5, wall=1422
2023-03-11 05:39:27 | INFO | train_inner | epoch 018:    866 / 1102 loss=3.824, nll_loss=2.314, ppl=4.97, wps=53596.6, ups=15.15, wpb=3537.5, bsz=134.5, num_updates=19600, lr=0.000319438, gnorm=1.032, loss_scale=32, train_wall=6, gb_free=13.5, wall=1429
2023-03-11 05:39:34 | INFO | train_inner | epoch 018:    966 / 1102 loss=3.808, nll_loss=2.297, ppl=4.91, wps=55154.8, ups=15.35, wpb=3593.6, bsz=153.8, num_updates=19700, lr=0.000318626, gnorm=1.056, loss_scale=32, train_wall=6, gb_free=13.7, wall=1435
2023-03-11 05:39:40 | INFO | train_inner | epoch 018:   1066 / 1102 loss=3.833, nll_loss=2.324, ppl=5.01, wps=52907.9, ups=15.19, wpb=3484, bsz=143.2, num_updates=19800, lr=0.000317821, gnorm=1.052, loss_scale=32, train_wall=6, gb_free=13.5, wall=1442
2023-03-11 05:39:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:39:44 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.048 | nll_loss 2.433 | ppl 5.4 | wps 136646 | wpb 2790.1 | bsz 113.8 | num_updates 19836 | best_loss 4.048
2023-03-11 05:39:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 19836 updates
2023-03-11 05:39:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:39:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:39:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 18 @ 19836 updates, score 4.048) (writing took 7.110974426999746 seconds)
2023-03-11 05:39:51 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-03-11 05:39:51 | INFO | train | epoch 018 | loss 3.789 | nll_loss 2.274 | ppl 4.84 | wps 48810.9 | ups 13.63 | wpb 3581.5 | bsz 145.4 | num_updates 19836 | lr 0.000317532 | gnorm 1.018 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 1453
2023-03-11 05:39:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:39:51 | INFO | fairseq.trainer | begin training epoch 19
2023-03-11 05:39:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:39:55 | INFO | train_inner | epoch 019:     64 / 1102 loss=3.737, nll_loss=2.215, ppl=4.64, wps=23861.4, ups=6.61, wpb=3612.4, bsz=139.7, num_updates=19900, lr=0.000317021, gnorm=0.988, loss_scale=32, train_wall=6, gb_free=13.5, wall=1457
2023-03-11 05:40:02 | INFO | train_inner | epoch 019:    164 / 1102 loss=3.706, nll_loss=2.18, ppl=4.53, wps=55619.9, ups=15.17, wpb=3665.5, bsz=148.2, num_updates=20000, lr=0.000316228, gnorm=0.973, loss_scale=32, train_wall=6, gb_free=13.5, wall=1464
2023-03-11 05:40:09 | INFO | train_inner | epoch 019:    264 / 1102 loss=3.765, nll_loss=2.245, ppl=4.74, wps=53668.2, ups=15.04, wpb=3568.1, bsz=134.2, num_updates=20100, lr=0.00031544, gnorm=1.042, loss_scale=32, train_wall=6, gb_free=13.6, wall=1470
2023-03-11 05:40:15 | INFO | train_inner | epoch 019:    364 / 1102 loss=3.73, nll_loss=2.207, ppl=4.62, wps=53818.2, ups=15.16, wpb=3551, bsz=146.3, num_updates=20200, lr=0.000314658, gnorm=1.008, loss_scale=32, train_wall=6, gb_free=13.7, wall=1477
2023-03-11 05:40:22 | INFO | train_inner | epoch 019:    464 / 1102 loss=3.738, nll_loss=2.215, ppl=4.64, wps=53917.6, ups=15.14, wpb=3560.5, bsz=143.8, num_updates=20300, lr=0.000313882, gnorm=1.009, loss_scale=32, train_wall=6, gb_free=13.6, wall=1484
2023-03-11 05:40:28 | INFO | train_inner | epoch 019:    564 / 1102 loss=3.753, nll_loss=2.232, ppl=4.7, wps=54400.4, ups=15.33, wpb=3548.3, bsz=143.1, num_updates=20400, lr=0.000313112, gnorm=1.041, loss_scale=32, train_wall=6, gb_free=13.5, wall=1490
2023-03-11 05:40:35 | INFO | train_inner | epoch 019:    664 / 1102 loss=3.765, nll_loss=2.246, ppl=4.74, wps=54556.2, ups=15.42, wpb=3538.4, bsz=149.4, num_updates=20500, lr=0.000312348, gnorm=1.06, loss_scale=32, train_wall=6, gb_free=13.5, wall=1497
2023-03-11 05:40:41 | INFO | train_inner | epoch 019:    764 / 1102 loss=3.771, nll_loss=2.254, ppl=4.77, wps=54892.1, ups=15.44, wpb=3555.4, bsz=143.4, num_updates=20600, lr=0.000311588, gnorm=1.048, loss_scale=32, train_wall=6, gb_free=13.5, wall=1503
2023-03-11 05:40:48 | INFO | train_inner | epoch 019:    864 / 1102 loss=3.806, nll_loss=2.293, ppl=4.9, wps=54596.2, ups=15.37, wpb=3553.3, bsz=130.2, num_updates=20700, lr=0.000310835, gnorm=1.019, loss_scale=32, train_wall=6, gb_free=13.6, wall=1510
2023-03-11 05:40:55 | INFO | train_inner | epoch 019:    964 / 1102 loss=3.702, nll_loss=2.177, ppl=4.52, wps=54585.4, ups=15.14, wpb=3605.8, bsz=171.5, num_updates=20800, lr=0.000310087, gnorm=0.977, loss_scale=32, train_wall=6, gb_free=13.5, wall=1516
2023-03-11 05:41:01 | INFO | train_inner | epoch 019:   1064 / 1102 loss=3.754, nll_loss=2.235, ppl=4.71, wps=55725.5, ups=15.21, wpb=3664.8, bsz=152.4, num_updates=20900, lr=0.000309344, gnorm=0.993, loss_scale=32, train_wall=6, gb_free=13.5, wall=1523
2023-03-11 05:41:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:41:05 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.034 | nll_loss 2.429 | ppl 5.39 | wps 137397 | wpb 2790.1 | bsz 113.8 | num_updates 20938 | best_loss 4.034
2023-03-11 05:41:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 20938 updates
2023-03-11 05:41:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:41:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:41:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 19 @ 20938 updates, score 4.034) (writing took 7.153507055998489 seconds)
2023-03-11 05:41:12 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-03-11 05:41:12 | INFO | train | epoch 019 | loss 3.748 | nll_loss 2.227 | ppl 4.68 | wps 48735 | ups 13.61 | wpb 3581.5 | bsz 145.4 | num_updates 20938 | lr 0.000309063 | gnorm 1.016 | loss_scale 32 | train_wall 70 | gb_free 13.7 | wall 1534
2023-03-11 05:41:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:41:12 | INFO | fairseq.trainer | begin training epoch 20
2023-03-11 05:41:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:41:16 | INFO | train_inner | epoch 020:     62 / 1102 loss=3.752, nll_loss=2.232, ppl=4.7, wps=22948.5, ups=6.55, wpb=3501.5, bsz=126.2, num_updates=21000, lr=0.000308607, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.6, wall=1538
2023-03-11 05:41:23 | INFO | train_inner | epoch 020:    162 / 1102 loss=3.68, nll_loss=2.149, ppl=4.43, wps=54004, ups=15.04, wpb=3591.1, bsz=144.4, num_updates=21100, lr=0.000307875, gnorm=1, loss_scale=32, train_wall=6, gb_free=13.5, wall=1545
2023-03-11 05:41:30 | INFO | train_inner | epoch 020:    262 / 1102 loss=3.694, nll_loss=2.165, ppl=4.49, wps=54367.2, ups=15.07, wpb=3607.7, bsz=138.5, num_updates=21200, lr=0.000307148, gnorm=0.996, loss_scale=32, train_wall=6, gb_free=13.6, wall=1551
2023-03-11 05:41:36 | INFO | train_inner | epoch 020:    362 / 1102 loss=3.699, nll_loss=2.171, ppl=4.5, wps=53972.9, ups=15.24, wpb=3541.3, bsz=139.5, num_updates=21300, lr=0.000306426, gnorm=1.006, loss_scale=32, train_wall=6, gb_free=13.5, wall=1558
2023-03-11 05:41:43 | INFO | train_inner | epoch 020:    462 / 1102 loss=3.699, nll_loss=2.171, ppl=4.5, wps=54803.1, ups=15.33, wpb=3575.8, bsz=144.4, num_updates=21400, lr=0.000305709, gnorm=1.018, loss_scale=32, train_wall=6, gb_free=13.6, wall=1564
2023-03-11 05:41:49 | INFO | train_inner | epoch 020:    562 / 1102 loss=3.651, nll_loss=2.118, ppl=4.34, wps=55659.4, ups=15.36, wpb=3623.4, bsz=168.1, num_updates=21500, lr=0.000304997, gnorm=0.979, loss_scale=32, train_wall=6, gb_free=13.6, wall=1571
2023-03-11 05:41:56 | INFO | train_inner | epoch 020:    662 / 1102 loss=3.712, nll_loss=2.186, ppl=4.55, wps=54301.9, ups=15.25, wpb=3559.7, bsz=141.3, num_updates=21600, lr=0.00030429, gnorm=1.012, loss_scale=32, train_wall=6, gb_free=13.5, wall=1577
2023-03-11 05:42:02 | INFO | train_inner | epoch 020:    762 / 1102 loss=3.752, nll_loss=2.232, ppl=4.7, wps=53686.9, ups=15.02, wpb=3575.5, bsz=137, num_updates=21700, lr=0.000303588, gnorm=1.049, loss_scale=32, train_wall=6, gb_free=13.6, wall=1584
2023-03-11 05:42:09 | INFO | train_inner | epoch 020:    862 / 1102 loss=3.734, nll_loss=2.212, ppl=4.63, wps=54041.4, ups=15.21, wpb=3553.9, bsz=147.8, num_updates=21800, lr=0.000302891, gnorm=1.025, loss_scale=32, train_wall=6, gb_free=13.7, wall=1591
2023-03-11 05:42:16 | INFO | train_inner | epoch 020:    962 / 1102 loss=3.754, nll_loss=2.235, ppl=4.71, wps=54647.1, ups=15.18, wpb=3600.6, bsz=141.4, num_updates=21900, lr=0.000302199, gnorm=1.015, loss_scale=32, train_wall=6, gb_free=13.6, wall=1597
2023-03-11 05:42:22 | INFO | train_inner | epoch 020:   1062 / 1102 loss=3.685, nll_loss=2.158, ppl=4.46, wps=55108.8, ups=15.15, wpb=3638.1, bsz=170.3, num_updates=22000, lr=0.000301511, gnorm=0.997, loss_scale=32, train_wall=6, gb_free=13.5, wall=1604
2023-03-11 05:42:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:42:26 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.035 | nll_loss 2.425 | ppl 5.37 | wps 134484 | wpb 2790.1 | bsz 113.8 | num_updates 22040 | best_loss 4.034
2023-03-11 05:42:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 22040 updates
2023-03-11 05:42:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:42:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:42:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 20 @ 22040 updates, score 4.035) (writing took 3.820799470999191 seconds)
2023-03-11 05:42:30 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-03-11 05:42:30 | INFO | train | epoch 020 | loss 3.71 | nll_loss 2.184 | ppl 4.54 | wps 50607.5 | ups 14.13 | wpb 3581.5 | bsz 145.4 | num_updates 22040 | lr 0.000301238 | gnorm 1.014 | loss_scale 32 | train_wall 71 | gb_free 13.8 | wall 1612
2023-03-11 05:42:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:42:30 | INFO | fairseq.trainer | begin training epoch 21
2023-03-11 05:42:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:42:34 | INFO | train_inner | epoch 021:     60 / 1102 loss=3.706, nll_loss=2.181, ppl=4.53, wps=30169.8, ups=8.34, wpb=3617.5, bsz=143.9, num_updates=22100, lr=0.000300828, gnorm=1.013, loss_scale=32, train_wall=6, gb_free=13.5, wall=1616
2023-03-11 05:42:41 | INFO | train_inner | epoch 021:    160 / 1102 loss=3.628, nll_loss=2.09, ppl=4.26, wps=53952.2, ups=15.28, wpb=3530.9, bsz=146.9, num_updates=22200, lr=0.00030015, gnorm=1.031, loss_scale=32, train_wall=6, gb_free=13.5, wall=1622
2023-03-11 05:42:47 | INFO | train_inner | epoch 021:    260 / 1102 loss=3.62, nll_loss=2.082, ppl=4.23, wps=55642.2, ups=15.3, wpb=3637.6, bsz=151.9, num_updates=22300, lr=0.000299476, gnorm=1.014, loss_scale=32, train_wall=6, gb_free=13.5, wall=1629
2023-03-11 05:42:54 | INFO | train_inner | epoch 021:    360 / 1102 loss=3.701, nll_loss=2.173, ppl=4.51, wps=54068.2, ups=15.34, wpb=3525.7, bsz=132.6, num_updates=22400, lr=0.000298807, gnorm=1.037, loss_scale=32, train_wall=6, gb_free=13.6, wall=1635
2023-03-11 05:43:00 | INFO | train_inner | epoch 021:    460 / 1102 loss=3.648, nll_loss=2.114, ppl=4.33, wps=54777.5, ups=15.19, wpb=3606.5, bsz=150.7, num_updates=22500, lr=0.000298142, gnorm=1.002, loss_scale=32, train_wall=6, gb_free=13.5, wall=1642
2023-03-11 05:43:07 | INFO | train_inner | epoch 021:    560 / 1102 loss=3.691, nll_loss=2.162, ppl=4.48, wps=53847.2, ups=15.22, wpb=3538.3, bsz=139.4, num_updates=22600, lr=0.000297482, gnorm=1.027, loss_scale=32, train_wall=6, gb_free=13.7, wall=1649
2023-03-11 05:43:14 | INFO | train_inner | epoch 021:    660 / 1102 loss=3.687, nll_loss=2.158, ppl=4.46, wps=54701.8, ups=15.27, wpb=3583.3, bsz=144.2, num_updates=22700, lr=0.000296826, gnorm=1.012, loss_scale=32, train_wall=6, gb_free=13.6, wall=1655
2023-03-11 05:43:20 | INFO | train_inner | epoch 021:    760 / 1102 loss=3.694, nll_loss=2.165, ppl=4.49, wps=55064, ups=15.32, wpb=3594.3, bsz=146.9, num_updates=22800, lr=0.000296174, gnorm=1.028, loss_scale=32, train_wall=6, gb_free=13.5, wall=1662
2023-03-11 05:43:27 | INFO | train_inner | epoch 021:    860 / 1102 loss=3.651, nll_loss=2.118, ppl=4.34, wps=55796, ups=15.28, wpb=3652.1, bsz=157.4, num_updates=22900, lr=0.000295527, gnorm=0.969, loss_scale=32, train_wall=6, gb_free=13.6, wall=1668
2023-03-11 05:43:33 | INFO | train_inner | epoch 021:    960 / 1102 loss=3.681, nll_loss=2.152, ppl=4.44, wps=54269.3, ups=15.28, wpb=3551.2, bsz=151.1, num_updates=23000, lr=0.000294884, gnorm=1.022, loss_scale=32, train_wall=6, gb_free=13.4, wall=1675
2023-03-11 05:43:40 | INFO | train_inner | epoch 021:   1060 / 1102 loss=3.689, nll_loss=2.161, ppl=4.47, wps=54744.6, ups=15.25, wpb=3589.5, bsz=149.3, num_updates=23100, lr=0.000294245, gnorm=0.995, loss_scale=32, train_wall=6, gb_free=13.6, wall=1681
2023-03-11 05:43:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:43:44 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.017 | nll_loss 2.406 | ppl 5.3 | wps 139619 | wpb 2790.1 | bsz 113.8 | num_updates 23142 | best_loss 4.017
2023-03-11 05:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 23142 updates
2023-03-11 05:43:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:43:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:43:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 21 @ 23142 updates, score 4.017) (writing took 7.108151267999347 seconds)
2023-03-11 05:43:51 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-03-11 05:43:51 | INFO | train | epoch 021 | loss 3.673 | nll_loss 2.142 | ppl 4.41 | wps 48810.6 | ups 13.63 | wpb 3581.5 | bsz 145.4 | num_updates 23142 | lr 0.000293978 | gnorm 1.015 | loss_scale 32 | train_wall 70 | gb_free 13.5 | wall 1693
2023-03-11 05:43:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:43:51 | INFO | fairseq.trainer | begin training epoch 22
2023-03-11 05:43:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:43:55 | INFO | train_inner | epoch 022:     58 / 1102 loss=3.702, nll_loss=2.174, ppl=4.51, wps=23878.5, ups=6.59, wpb=3625.9, bsz=123.8, num_updates=23200, lr=0.00029361, gnorm=1.022, loss_scale=32, train_wall=6, gb_free=13.5, wall=1697
2023-03-11 05:44:02 | INFO | train_inner | epoch 022:    158 / 1102 loss=3.592, nll_loss=2.048, ppl=4.14, wps=54528.5, ups=15.13, wpb=3604.9, bsz=147, num_updates=23300, lr=0.000292979, gnorm=0.999, loss_scale=32, train_wall=6, gb_free=13.5, wall=1703
2023-03-11 05:44:08 | INFO | train_inner | epoch 022:    258 / 1102 loss=3.619, nll_loss=2.079, ppl=4.22, wps=53652.8, ups=15.05, wpb=3566.1, bsz=141, num_updates=23400, lr=0.000292353, gnorm=1.008, loss_scale=32, train_wall=6, gb_free=13.6, wall=1710
2023-03-11 05:44:15 | INFO | train_inner | epoch 022:    358 / 1102 loss=3.607, nll_loss=2.066, ppl=4.19, wps=54423, ups=15.27, wpb=3565.1, bsz=160, num_updates=23500, lr=0.00029173, gnorm=1.024, loss_scale=32, train_wall=6, gb_free=13.5, wall=1716
2023-03-11 05:44:21 | INFO | train_inner | epoch 022:    458 / 1102 loss=3.591, nll_loss=2.049, ppl=4.14, wps=54490.3, ups=15.08, wpb=3612.3, bsz=159.9, num_updates=23600, lr=0.000291111, gnorm=0.97, loss_scale=32, train_wall=6, gb_free=13.4, wall=1723
2023-03-11 05:44:28 | INFO | train_inner | epoch 022:    558 / 1102 loss=3.653, nll_loss=2.118, ppl=4.34, wps=54109.6, ups=15.09, wpb=3585.5, bsz=137.5, num_updates=23700, lr=0.000290496, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=13.7, wall=1730
2023-03-11 05:44:35 | INFO | train_inner | epoch 022:    658 / 1102 loss=3.676, nll_loss=2.146, ppl=4.42, wps=53631.6, ups=15.26, wpb=3513.6, bsz=145.3, num_updates=23800, lr=0.000289886, gnorm=1.055, loss_scale=32, train_wall=6, gb_free=13.6, wall=1736
2023-03-11 05:44:41 | INFO | train_inner | epoch 022:    758 / 1102 loss=3.646, nll_loss=2.111, ppl=4.32, wps=54892.8, ups=15.2, wpb=3610.3, bsz=149, num_updates=23900, lr=0.000289278, gnorm=1.022, loss_scale=32, train_wall=6, gb_free=13.5, wall=1743
2023-03-11 05:44:48 | INFO | train_inner | epoch 022:    858 / 1102 loss=3.664, nll_loss=2.133, ppl=4.38, wps=54859.6, ups=15.21, wpb=3605.9, bsz=140.6, num_updates=24000, lr=0.000288675, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=13.6, wall=1749
2023-03-11 05:44:54 | INFO | train_inner | epoch 022:    958 / 1102 loss=3.727, nll_loss=2.202, ppl=4.6, wps=54441.8, ups=15.29, wpb=3561.1, bsz=123.8, num_updates=24100, lr=0.000288076, gnorm=1.04, loss_scale=32, train_wall=6, gb_free=13.5, wall=1756
2023-03-11 05:45:01 | INFO | train_inner | epoch 022:   1058 / 1102 loss=3.684, nll_loss=2.156, ppl=4.46, wps=54006.2, ups=15.35, wpb=3517.5, bsz=147.8, num_updates=24200, lr=0.00028748, gnorm=1.038, loss_scale=32, train_wall=6, gb_free=13.6, wall=1762
2023-03-11 05:45:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:45:05 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.011 | nll_loss 2.396 | ppl 5.27 | wps 132295 | wpb 2790.1 | bsz 113.8 | num_updates 24244 | best_loss 4.011
2023-03-11 05:45:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 24244 updates
2023-03-11 05:45:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:45:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:45:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 22 @ 24244 updates, score 4.011) (writing took 7.090402600999369 seconds)
2023-03-11 05:45:12 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-03-11 05:45:12 | INFO | train | epoch 022 | loss 3.643 | nll_loss 2.107 | ppl 4.31 | wps 48580.5 | ups 13.56 | wpb 3581.5 | bsz 145.4 | num_updates 24244 | lr 0.000287219 | gnorm 1.017 | loss_scale 32 | train_wall 71 | gb_free 13.4 | wall 1774
2023-03-11 05:45:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:45:12 | INFO | fairseq.trainer | begin training epoch 23
2023-03-11 05:45:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:45:16 | INFO | train_inner | epoch 023:     56 / 1102 loss=3.606, nll_loss=2.067, ppl=4.19, wps=23790.4, ups=6.55, wpb=3634.4, bsz=150.2, num_updates=24300, lr=0.000286888, gnorm=0.99, loss_scale=32, train_wall=6, gb_free=13.8, wall=1778
2023-03-11 05:45:23 | INFO | train_inner | epoch 023:    156 / 1102 loss=3.552, nll_loss=2.002, ppl=4.01, wps=54495, ups=15.18, wpb=3590.4, bsz=146.7, num_updates=24400, lr=0.000286299, gnorm=0.973, loss_scale=32, train_wall=6, gb_free=13.5, wall=1784
2023-03-11 05:45:29 | INFO | train_inner | epoch 023:    256 / 1102 loss=3.578, nll_loss=2.032, ppl=4.09, wps=54435.9, ups=15.22, wpb=3576, bsz=146.1, num_updates=24500, lr=0.000285714, gnorm=1.01, loss_scale=32, train_wall=6, gb_free=13.5, wall=1791
2023-03-11 05:45:36 | INFO | train_inner | epoch 023:    356 / 1102 loss=3.624, nll_loss=2.085, ppl=4.24, wps=53878.9, ups=15.39, wpb=3500.4, bsz=140.2, num_updates=24600, lr=0.000285133, gnorm=1.035, loss_scale=32, train_wall=6, gb_free=13.6, wall=1797
2023-03-11 05:45:42 | INFO | train_inner | epoch 023:    456 / 1102 loss=3.609, nll_loss=2.067, ppl=4.19, wps=54387.7, ups=15.26, wpb=3564.9, bsz=146.7, num_updates=24700, lr=0.000284555, gnorm=1.024, loss_scale=32, train_wall=6, gb_free=13.6, wall=1804
2023-03-11 05:45:49 | INFO | train_inner | epoch 023:    556 / 1102 loss=3.632, nll_loss=2.093, ppl=4.27, wps=55009.9, ups=15.21, wpb=3615.9, bsz=131, num_updates=24800, lr=0.000283981, gnorm=1.034, loss_scale=32, train_wall=6, gb_free=13.6, wall=1810
2023-03-11 05:45:55 | INFO | train_inner | epoch 023:    656 / 1102 loss=3.616, nll_loss=2.076, ppl=4.22, wps=55444.2, ups=15.26, wpb=3633.8, bsz=140.7, num_updates=24900, lr=0.00028341, gnorm=1.002, loss_scale=32, train_wall=6, gb_free=13.5, wall=1817
2023-03-11 05:46:02 | INFO | train_inner | epoch 023:    756 / 1102 loss=3.637, nll_loss=2.101, ppl=4.29, wps=54488.9, ups=15.21, wpb=3581.9, bsz=143, num_updates=25000, lr=0.000282843, gnorm=1.026, loss_scale=32, train_wall=6, gb_free=13.5, wall=1824
2023-03-11 05:46:09 | INFO | train_inner | epoch 023:    856 / 1102 loss=3.63, nll_loss=2.093, ppl=4.27, wps=53319.2, ups=14.96, wpb=3563.2, bsz=146.5, num_updates=25100, lr=0.000282279, gnorm=1.024, loss_scale=32, train_wall=7, gb_free=13.5, wall=1830
2023-03-11 05:46:15 | INFO | train_inner | epoch 023:    956 / 1102 loss=3.617, nll_loss=2.08, ppl=4.23, wps=54011.1, ups=15.15, wpb=3565.7, bsz=154.9, num_updates=25200, lr=0.000281718, gnorm=1.03, loss_scale=32, train_wall=6, gb_free=13.5, wall=1837
2023-03-11 05:46:22 | INFO | train_inner | epoch 023:   1056 / 1102 loss=3.619, nll_loss=2.083, ppl=4.24, wps=55343.9, ups=15.3, wpb=3617.1, bsz=159.2, num_updates=25300, lr=0.000281161, gnorm=1.008, loss_scale=32, train_wall=6, gb_free=13.5, wall=1843
2023-03-11 05:46:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:46:26 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.998 | nll_loss 2.392 | ppl 5.25 | wps 138746 | wpb 2790.1 | bsz 113.8 | num_updates 25346 | best_loss 3.998
2023-03-11 05:46:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 25346 updates
2023-03-11 05:46:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:46:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:46:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 23 @ 25346 updates, score 3.998) (writing took 7.110428297000908 seconds)
2023-03-11 05:46:33 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-03-11 05:46:33 | INFO | train | epoch 023 | loss 3.611 | nll_loss 2.071 | ppl 4.2 | wps 48670.4 | ups 13.59 | wpb 3581.5 | bsz 145.4 | num_updates 25346 | lr 0.000280906 | gnorm 1.015 | loss_scale 32 | train_wall 71 | gb_free 13.5 | wall 1855
2023-03-11 05:46:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:46:33 | INFO | fairseq.trainer | begin training epoch 24
2023-03-11 05:46:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:46:37 | INFO | train_inner | epoch 024:     54 / 1102 loss=3.574, nll_loss=2.029, ppl=4.08, wps=23494.4, ups=6.57, wpb=3576.6, bsz=149.4, num_updates=25400, lr=0.000280607, gnorm=1.009, loss_scale=32, train_wall=6, gb_free=13.5, wall=1859
2023-03-11 05:46:44 | INFO | train_inner | epoch 024:    154 / 1102 loss=3.557, nll_loss=2.008, ppl=4.02, wps=55779.5, ups=15.25, wpb=3656.6, bsz=141, num_updates=25500, lr=0.000280056, gnorm=1.002, loss_scale=32, train_wall=6, gb_free=13.6, wall=1865
2023-03-11 05:46:50 | INFO | train_inner | epoch 024:    254 / 1102 loss=3.559, nll_loss=2.011, ppl=4.03, wps=55429.8, ups=15.28, wpb=3628.7, bsz=146.9, num_updates=25600, lr=0.000279508, gnorm=0.996, loss_scale=32, train_wall=6, gb_free=13.5, wall=1872
2023-03-11 05:46:57 | INFO | train_inner | epoch 024:    354 / 1102 loss=3.608, nll_loss=2.066, ppl=4.19, wps=53204, ups=15.23, wpb=3494.2, bsz=129.9, num_updates=25700, lr=0.000278964, gnorm=1.038, loss_scale=32, train_wall=6, gb_free=13.6, wall=1878
2023-03-11 05:47:03 | INFO | train_inner | epoch 024:    454 / 1102 loss=3.585, nll_loss=2.04, ppl=4.11, wps=53364.5, ups=15.04, wpb=3548.8, bsz=137, num_updates=25800, lr=0.000278423, gnorm=1.026, loss_scale=32, train_wall=6, gb_free=13.6, wall=1885
2023-03-11 05:47:10 | INFO | train_inner | epoch 024:    554 / 1102 loss=3.547, nll_loss=1.998, ppl=4, wps=55182.1, ups=15.32, wpb=3602.9, bsz=157.7, num_updates=25900, lr=0.000277885, gnorm=1.006, loss_scale=32, train_wall=6, gb_free=13.6, wall=1891
2023-03-11 05:47:16 | INFO | train_inner | epoch 024:    654 / 1102 loss=3.607, nll_loss=2.066, ppl=4.19, wps=55604.2, ups=15.36, wpb=3619.2, bsz=136.4, num_updates=26000, lr=0.00027735, gnorm=1.002, loss_scale=32, train_wall=6, gb_free=13.4, wall=1898
2023-03-11 05:47:23 | INFO | train_inner | epoch 024:    754 / 1102 loss=3.578, nll_loss=2.034, ppl=4.09, wps=53729.5, ups=15.39, wpb=3491.2, bsz=148.1, num_updates=26100, lr=0.000276818, gnorm=1.035, loss_scale=32, train_wall=6, gb_free=13.6, wall=1904
2023-03-11 05:47:30 | INFO | train_inner | epoch 024:    854 / 1102 loss=3.592, nll_loss=2.05, ppl=4.14, wps=53880.6, ups=15.09, wpb=3571, bsz=151.4, num_updates=26200, lr=0.000276289, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=13.7, wall=1911
2023-03-11 05:47:36 | INFO | train_inner | epoch 024:    954 / 1102 loss=3.616, nll_loss=2.077, ppl=4.22, wps=54845.7, ups=15.36, wpb=3570.1, bsz=141.3, num_updates=26300, lr=0.000275764, gnorm=1.02, loss_scale=32, train_wall=6, gb_free=13.6, wall=1918
2023-03-11 05:47:43 | INFO | train_inner | epoch 024:   1054 / 1102 loss=3.598, nll_loss=2.058, ppl=4.17, wps=55845.7, ups=15.37, wpb=3634.5, bsz=162.5, num_updates=26400, lr=0.000275241, gnorm=0.998, loss_scale=32, train_wall=6, gb_free=13.5, wall=1924
2023-03-11 05:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:47:47 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.003 | nll_loss 2.388 | ppl 5.23 | wps 136957 | wpb 2790.1 | bsz 113.8 | num_updates 26448 | best_loss 3.998
2023-03-11 05:47:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 26448 updates
2023-03-11 05:47:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:47:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 24 @ 26448 updates, score 4.003) (writing took 3.9155529039999237 seconds)
2023-03-11 05:47:51 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-03-11 05:47:51 | INFO | train | epoch 024 | loss 3.583 | nll_loss 2.039 | ppl 4.11 | wps 50802.3 | ups 14.18 | wpb 3581.5 | bsz 145.4 | num_updates 26448 | lr 0.000274991 | gnorm 1.014 | loss_scale 32 | train_wall 70 | gb_free 13.5 | wall 1933
2023-03-11 05:47:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:47:51 | INFO | fairseq.trainer | begin training epoch 25
2023-03-11 05:47:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:47:55 | INFO | train_inner | epoch 025:     52 / 1102 loss=3.58, nll_loss=2.034, ppl=4.1, wps=28903.8, ups=8.31, wpb=3479.5, bsz=131, num_updates=26500, lr=0.000274721, gnorm=1.036, loss_scale=32, train_wall=6, gb_free=13.6, wall=1936
2023-03-11 05:48:01 | INFO | train_inner | epoch 025:    152 / 1102 loss=3.511, nll_loss=1.955, ppl=3.88, wps=54566.4, ups=15.18, wpb=3595.1, bsz=142.9, num_updates=26600, lr=0.000274204, gnorm=1.001, loss_scale=32, train_wall=6, gb_free=13.6, wall=1943
2023-03-11 05:48:08 | INFO | train_inner | epoch 025:    252 / 1102 loss=3.496, nll_loss=1.939, ppl=3.83, wps=54447.4, ups=14.86, wpb=3663.6, bsz=159.1, num_updates=26700, lr=0.00027369, gnorm=0.987, loss_scale=32, train_wall=7, gb_free=13.5, wall=1950
2023-03-11 05:48:14 | INFO | train_inner | epoch 025:    352 / 1102 loss=3.56, nll_loss=2.012, ppl=4.03, wps=55048.1, ups=15.31, wpb=3594.9, bsz=146.3, num_updates=26800, lr=0.000273179, gnorm=1.025, loss_scale=32, train_wall=6, gb_free=13.5, wall=1956
2023-03-11 05:48:21 | INFO | train_inner | epoch 025:    452 / 1102 loss=3.559, nll_loss=2.011, ppl=4.03, wps=53660.6, ups=15.09, wpb=3556.3, bsz=137.5, num_updates=26900, lr=0.000272671, gnorm=1.007, loss_scale=32, train_wall=6, gb_free=13.6, wall=1963
2023-03-11 05:48:28 | INFO | train_inner | epoch 025:    552 / 1102 loss=3.58, nll_loss=2.035, ppl=4.1, wps=53771.4, ups=15.25, wpb=3526, bsz=134.2, num_updates=27000, lr=0.000272166, gnorm=1.057, loss_scale=32, train_wall=6, gb_free=13.4, wall=1969
2023-03-11 05:48:34 | INFO | train_inner | epoch 025:    652 / 1102 loss=3.568, nll_loss=2.021, ppl=4.06, wps=54381, ups=15.23, wpb=3570.6, bsz=143.1, num_updates=27100, lr=0.000271663, gnorm=1.024, loss_scale=32, train_wall=6, gb_free=13.6, wall=1976
2023-03-11 05:48:41 | INFO | train_inner | epoch 025:    752 / 1102 loss=3.611, nll_loss=2.069, ppl=4.2, wps=53790.1, ups=15.18, wpb=3543.7, bsz=129.7, num_updates=27200, lr=0.000271163, gnorm=1.031, loss_scale=32, train_wall=6, gb_free=13.5, wall=1982
2023-03-11 05:48:47 | INFO | train_inner | epoch 025:    852 / 1102 loss=3.546, nll_loss=1.997, ppl=3.99, wps=54129.2, ups=15.12, wpb=3579.2, bsz=146.4, num_updates=27300, lr=0.000270666, gnorm=0.995, loss_scale=32, train_wall=6, gb_free=13.5, wall=1989
2023-03-11 05:48:54 | INFO | train_inner | epoch 025:    952 / 1102 loss=3.54, nll_loss=1.992, ppl=3.98, wps=55745.6, ups=15.24, wpb=3658.5, bsz=166.6, num_updates=27400, lr=0.000270172, gnorm=0.992, loss_scale=32, train_wall=6, gb_free=13.6, wall=1996
2023-03-11 05:49:01 | INFO | train_inner | epoch 025:   1052 / 1102 loss=3.565, nll_loss=2.021, ppl=4.06, wps=54740.1, ups=15.1, wpb=3625.6, bsz=163.4, num_updates=27500, lr=0.00026968, gnorm=1.007, loss_scale=32, train_wall=6, gb_free=13.6, wall=2002
2023-03-11 05:49:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:49:05 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.994 | nll_loss 2.378 | ppl 5.2 | wps 133512 | wpb 2790.1 | bsz 113.8 | num_updates 27550 | best_loss 3.994
2023-03-11 05:49:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 27550 updates
2023-03-11 05:49:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:49:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:49:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 25 @ 27550 updates, score 3.994) (writing took 7.1652500569980475 seconds)
2023-03-11 05:49:13 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-03-11 05:49:13 | INFO | train | epoch 025 | loss 3.555 | nll_loss 2.007 | ppl 4.02 | wps 48403.3 | ups 13.51 | wpb 3581.5 | bsz 145.4 | num_updates 27550 | lr 0.000269435 | gnorm 1.016 | loss_scale 32 | train_wall 71 | gb_free 13.5 | wall 2014
2023-03-11 05:49:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:49:13 | INFO | fairseq.trainer | begin training epoch 26
2023-03-11 05:49:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:49:16 | INFO | train_inner | epoch 026:     50 / 1102 loss=3.554, nll_loss=2.007, ppl=4.02, wps=23173.3, ups=6.49, wpb=3573.1, bsz=145.8, num_updates=27600, lr=0.000269191, gnorm=1.019, loss_scale=32, train_wall=7, gb_free=13.5, wall=2018
2023-03-11 05:49:23 | INFO | train_inner | epoch 026:    150 / 1102 loss=3.45, nll_loss=1.887, ppl=3.7, wps=53319.6, ups=15.09, wpb=3532.7, bsz=155.4, num_updates=27700, lr=0.000268705, gnorm=0.991, loss_scale=32, train_wall=6, gb_free=13.7, wall=2024
2023-03-11 05:49:29 | INFO | train_inner | epoch 026:    250 / 1102 loss=3.519, nll_loss=1.965, ppl=3.9, wps=54784.7, ups=15.3, wpb=3581.8, bsz=144.6, num_updates=27800, lr=0.000268221, gnorm=1.04, loss_scale=32, train_wall=6, gb_free=13.7, wall=2031
2023-03-11 05:49:36 | INFO | train_inner | epoch 026:    350 / 1102 loss=3.527, nll_loss=1.974, ppl=3.93, wps=54599.7, ups=15.18, wpb=3597.3, bsz=136.2, num_updates=27900, lr=0.00026774, gnorm=1.002, loss_scale=32, train_wall=6, gb_free=13.4, wall=2037
2023-03-11 05:49:42 | INFO | train_inner | epoch 026:    450 / 1102 loss=3.564, nll_loss=2.014, ppl=4.04, wps=55586.1, ups=15.35, wpb=3621.5, bsz=132.6, num_updates=28000, lr=0.000267261, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.4, wall=2044
2023-03-11 05:49:49 | INFO | train_inner | epoch 026:    550 / 1102 loss=3.537, nll_loss=1.986, ppl=3.96, wps=54311.3, ups=15.39, wpb=3528.1, bsz=146.5, num_updates=28100, lr=0.000266785, gnorm=1.026, loss_scale=32, train_wall=6, gb_free=13.5, wall=2050
2023-03-11 05:49:55 | INFO | train_inner | epoch 026:    650 / 1102 loss=3.523, nll_loss=1.97, ppl=3.92, wps=54946.5, ups=15.34, wpb=3580.9, bsz=139.8, num_updates=28200, lr=0.000266312, gnorm=1.014, loss_scale=32, train_wall=6, gb_free=13.5, wall=2057
2023-03-11 05:50:02 | INFO | train_inner | epoch 026:    750 / 1102 loss=3.572, nll_loss=2.026, ppl=4.07, wps=54757.1, ups=15.04, wpb=3640.8, bsz=139.1, num_updates=28300, lr=0.000265841, gnorm=1.027, loss_scale=32, train_wall=6, gb_free=13.6, wall=2064
2023-03-11 05:50:09 | INFO | train_inner | epoch 026:    850 / 1102 loss=3.555, nll_loss=2.008, ppl=4.02, wps=53621.4, ups=15.19, wpb=3529, bsz=145.7, num_updates=28400, lr=0.000265372, gnorm=1.033, loss_scale=32, train_wall=6, gb_free=13.6, wall=2070
2023-03-11 05:50:15 | INFO | train_inner | epoch 026:    950 / 1102 loss=3.514, nll_loss=1.962, ppl=3.9, wps=56026.3, ups=15.4, wpb=3638.5, bsz=166.1, num_updates=28500, lr=0.000264906, gnorm=0.988, loss_scale=32, train_wall=6, gb_free=13.5, wall=2077
2023-03-11 05:50:22 | INFO | train_inner | epoch 026:   1050 / 1102 loss=3.531, nll_loss=1.982, ppl=3.95, wps=55341.5, ups=15.2, wpb=3641.4, bsz=159.4, num_updates=28600, lr=0.000264443, gnorm=1, loss_scale=32, train_wall=6, gb_free=13.4, wall=2083
2023-03-11 05:50:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:50:26 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.989 | nll_loss 2.374 | ppl 5.18 | wps 138845 | wpb 2790.1 | bsz 113.8 | num_updates 28652 | best_loss 3.989
2023-03-11 05:50:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 28652 updates
2023-03-11 05:50:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:50:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:50:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 26 @ 28652 updates, score 3.989) (writing took 7.134594823000953 seconds)
2023-03-11 05:50:34 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-03-11 05:50:34 | INFO | train | epoch 026 | loss 3.532 | nll_loss 1.98 | ppl 3.94 | wps 48743.6 | ups 13.61 | wpb 3581.5 | bsz 145.4 | num_updates 28652 | lr 0.000264203 | gnorm 1.02 | loss_scale 32 | train_wall 70 | gb_free 13.4 | wall 2095
2023-03-11 05:50:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:50:34 | INFO | fairseq.trainer | begin training epoch 27
2023-03-11 05:50:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:50:37 | INFO | train_inner | epoch 027:     48 / 1102 loss=3.511, nll_loss=1.954, ppl=3.88, wps=22932.8, ups=6.57, wpb=3489.2, bsz=129.5, num_updates=28700, lr=0.000263982, gnorm=1.049, loss_scale=32, train_wall=6, gb_free=13.6, wall=2098
2023-03-11 05:50:43 | INFO | train_inner | epoch 027:    148 / 1102 loss=3.437, nll_loss=1.872, ppl=3.66, wps=54611.1, ups=15.3, wpb=3569.9, bsz=156.4, num_updates=28800, lr=0.000263523, gnorm=1.007, loss_scale=32, train_wall=6, gb_free=13.5, wall=2105
2023-03-11 05:50:50 | INFO | train_inner | epoch 027:    248 / 1102 loss=3.475, nll_loss=1.914, ppl=3.77, wps=55461.8, ups=15.37, wpb=3609.3, bsz=143.4, num_updates=28900, lr=0.000263067, gnorm=1.027, loss_scale=32, train_wall=6, gb_free=13.5, wall=2111
2023-03-11 05:50:56 | INFO | train_inner | epoch 027:    348 / 1102 loss=3.504, nll_loss=1.948, ppl=3.86, wps=55942.1, ups=15.4, wpb=3633.3, bsz=142.6, num_updates=29000, lr=0.000262613, gnorm=1.014, loss_scale=32, train_wall=6, gb_free=13.5, wall=2118
2023-03-11 05:51:03 | INFO | train_inner | epoch 027:    448 / 1102 loss=3.497, nll_loss=1.94, ppl=3.84, wps=53382.6, ups=15.28, wpb=3494.1, bsz=149.1, num_updates=29100, lr=0.000262161, gnorm=1.045, loss_scale=32, train_wall=6, gb_free=13.6, wall=2125
2023-03-11 05:51:09 | INFO | train_inner | epoch 027:    548 / 1102 loss=3.466, nll_loss=1.906, ppl=3.75, wps=55434.6, ups=15.36, wpb=3610.1, bsz=161.5, num_updates=29200, lr=0.000261712, gnorm=0.993, loss_scale=32, train_wall=6, gb_free=13.5, wall=2131
2023-03-11 05:51:16 | INFO | train_inner | epoch 027:    648 / 1102 loss=3.517, nll_loss=1.963, ppl=3.9, wps=55248.6, ups=15.23, wpb=3627.5, bsz=138.8, num_updates=29300, lr=0.000261265, gnorm=1.005, loss_scale=32, train_wall=6, gb_free=13.6, wall=2138
2023-03-11 05:51:23 | INFO | train_inner | epoch 027:    748 / 1102 loss=3.548, nll_loss=1.999, ppl=4, wps=53256.4, ups=15.28, wpb=3485.1, bsz=138.7, num_updates=29400, lr=0.00026082, gnorm=1.065, loss_scale=32, train_wall=6, gb_free=13.6, wall=2144
2023-03-11 05:51:29 | INFO | train_inner | epoch 027:    848 / 1102 loss=3.523, nll_loss=1.971, ppl=3.92, wps=55692.5, ups=15.35, wpb=3628.8, bsz=147.4, num_updates=29500, lr=0.000260378, gnorm=0.991, loss_scale=32, train_wall=6, gb_free=13.5, wall=2151
2023-03-11 05:51:36 | INFO | train_inner | epoch 027:    948 / 1102 loss=3.578, nll_loss=2.032, ppl=4.09, wps=54777.5, ups=15.23, wpb=3596.4, bsz=131.8, num_updates=29600, lr=0.000259938, gnorm=1.056, loss_scale=32, train_wall=6, gb_free=13.4, wall=2157
2023-03-11 05:51:42 | INFO | train_inner | epoch 027:   1048 / 1102 loss=3.559, nll_loss=2.012, ppl=4.03, wps=54630.5, ups=15.29, wpb=3572.8, bsz=143.9, num_updates=29700, lr=0.0002595, gnorm=1.056, loss_scale=32, train_wall=6, gb_free=13.4, wall=2164
2023-03-11 05:51:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:51:47 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.986 | nll_loss 2.371 | ppl 5.17 | wps 141428 | wpb 2790.1 | bsz 113.8 | num_updates 29754 | best_loss 3.986
2023-03-11 05:51:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 29754 updates
2023-03-11 05:51:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:51:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:51:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 27 @ 29754 updates, score 3.986) (writing took 7.069230216002325 seconds)
2023-03-11 05:51:54 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-03-11 05:51:54 | INFO | train | epoch 027 | loss 3.507 | nll_loss 1.951 | ppl 3.87 | wps 48991.4 | ups 13.68 | wpb 3581.5 | bsz 145.4 | num_updates 29754 | lr 0.000259264 | gnorm 1.023 | loss_scale 32 | train_wall 70 | gb_free 13.5 | wall 2176
2023-03-11 05:51:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:51:54 | INFO | fairseq.trainer | begin training epoch 28
2023-03-11 05:51:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:51:57 | INFO | train_inner | epoch 028:     46 / 1102 loss=3.481, nll_loss=1.922, ppl=3.79, wps=23733.7, ups=6.64, wpb=3574.4, bsz=143.6, num_updates=29800, lr=0.000259064, gnorm=1.002, loss_scale=32, train_wall=6, gb_free=13.5, wall=2179
2023-03-11 05:52:04 | INFO | train_inner | epoch 028:    146 / 1102 loss=3.452, nll_loss=1.888, ppl=3.7, wps=55717.7, ups=15.37, wpb=3625.5, bsz=146.6, num_updates=29900, lr=0.00025863, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=13.5, wall=2185
2023-03-11 05:52:10 | INFO | train_inner | epoch 028:    246 / 1102 loss=3.439, nll_loss=1.873, ppl=3.66, wps=53847.4, ups=15.43, wpb=3489.7, bsz=148.8, num_updates=30000, lr=0.000258199, gnorm=1.03, loss_scale=32, train_wall=6, gb_free=13.6, wall=2192
2023-03-11 05:52:17 | INFO | train_inner | epoch 028:    346 / 1102 loss=3.451, nll_loss=1.887, ppl=3.7, wps=55205.9, ups=15.44, wpb=3576, bsz=147.7, num_updates=30100, lr=0.00025777, gnorm=1.03, loss_scale=32, train_wall=6, gb_free=13.5, wall=2198
2023-03-11 05:52:23 | INFO | train_inner | epoch 028:    446 / 1102 loss=3.526, nll_loss=1.971, ppl=3.92, wps=54253.6, ups=15.42, wpb=3518.7, bsz=124.2, num_updates=30200, lr=0.000257343, gnorm=1.086, loss_scale=32, train_wall=6, gb_free=13.6, wall=2205
2023-03-11 05:52:30 | INFO | train_inner | epoch 028:    546 / 1102 loss=3.483, nll_loss=1.924, ppl=3.79, wps=56180.9, ups=15.42, wpb=3643.2, bsz=149.1, num_updates=30300, lr=0.000256917, gnorm=0.996, loss_scale=32, train_wall=6, gb_free=13.5, wall=2211
2023-03-11 05:52:36 | INFO | train_inner | epoch 028:    646 / 1102 loss=3.496, nll_loss=1.94, ppl=3.84, wps=55908.2, ups=15.33, wpb=3647.1, bsz=150.2, num_updates=30400, lr=0.000256495, gnorm=1.016, loss_scale=32, train_wall=6, gb_free=13.5, wall=2218
2023-03-11 05:52:43 | INFO | train_inner | epoch 028:    746 / 1102 loss=3.498, nll_loss=1.941, ppl=3.84, wps=55370.3, ups=15.47, wpb=3580, bsz=139.6, num_updates=30500, lr=0.000256074, gnorm=1.014, loss_scale=32, train_wall=6, gb_free=13.7, wall=2224
2023-03-11 05:52:49 | INFO | train_inner | epoch 028:    846 / 1102 loss=3.519, nll_loss=1.967, ppl=3.91, wps=55411, ups=15.45, wpb=3586.4, bsz=139.7, num_updates=30600, lr=0.000255655, gnorm=1.013, loss_scale=32, train_wall=6, gb_free=13.7, wall=2231
2023-03-11 05:52:56 | INFO | train_inner | epoch 028:    946 / 1102 loss=3.481, nll_loss=1.924, ppl=3.79, wps=54961.8, ups=15.32, wpb=3587.6, bsz=154.1, num_updates=30700, lr=0.000255238, gnorm=0.996, loss_scale=32, train_wall=6, gb_free=13.5, wall=2237
2023-03-11 05:53:02 | INFO | train_inner | epoch 028:   1046 / 1102 loss=3.505, nll_loss=1.95, ppl=3.86, wps=54275.8, ups=15.32, wpb=3543.4, bsz=150.8, num_updates=30800, lr=0.000254824, gnorm=1.023, loss_scale=32, train_wall=6, gb_free=13.6, wall=2244
2023-03-11 05:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:53:07 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.98 | nll_loss 2.364 | ppl 5.15 | wps 138311 | wpb 2790.1 | bsz 113.8 | num_updates 30856 | best_loss 3.98
2023-03-11 05:53:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 30856 updates
2023-03-11 05:53:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:53:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 28 @ 30856 updates, score 3.98) (writing took 7.1534411430002365 seconds)
2023-03-11 05:53:14 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-03-11 05:53:14 | INFO | train | epoch 028 | loss 3.483 | nll_loss 1.924 | ppl 3.8 | wps 49131 | ups 13.72 | wpb 3581.5 | bsz 145.4 | num_updates 30856 | lr 0.000254592 | gnorm 1.02 | loss_scale 32 | train_wall 70 | gb_free 13.5 | wall 2256
2023-03-11 05:53:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:53:15 | INFO | fairseq.trainer | begin training epoch 29
2023-03-11 05:53:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:53:17 | INFO | train_inner | epoch 029:     44 / 1102 loss=3.466, nll_loss=1.905, ppl=3.75, wps=23457.7, ups=6.57, wpb=3570.9, bsz=149, num_updates=30900, lr=0.000254411, gnorm=1.025, loss_scale=32, train_wall=6, gb_free=13.7, wall=2259
2023-03-11 05:53:24 | INFO | train_inner | epoch 029:    144 / 1102 loss=3.42, nll_loss=1.851, ppl=3.61, wps=54612.8, ups=15.26, wpb=3578.3, bsz=140.1, num_updates=31000, lr=0.000254, gnorm=1.003, loss_scale=32, train_wall=6, gb_free=13.4, wall=2266
2023-03-11 05:53:31 | INFO | train_inner | epoch 029:    244 / 1102 loss=3.453, nll_loss=1.888, ppl=3.7, wps=54518.3, ups=15.24, wpb=3578.4, bsz=136.5, num_updates=31100, lr=0.000253592, gnorm=1.015, loss_scale=32, train_wall=6, gb_free=13.6, wall=2272
2023-03-11 05:53:37 | INFO | train_inner | epoch 029:    344 / 1102 loss=3.434, nll_loss=1.868, ppl=3.65, wps=55772.1, ups=15.39, wpb=3624.7, bsz=151.7, num_updates=31200, lr=0.000253185, gnorm=1.001, loss_scale=32, train_wall=6, gb_free=13.7, wall=2279
2023-03-11 05:53:44 | INFO | train_inner | epoch 029:    444 / 1102 loss=3.464, nll_loss=1.902, ppl=3.74, wps=54569.6, ups=15.31, wpb=3563.9, bsz=143.2, num_updates=31300, lr=0.00025278, gnorm=1.034, loss_scale=32, train_wall=6, gb_free=13.6, wall=2285
2023-03-11 05:53:50 | INFO | train_inner | epoch 029:    544 / 1102 loss=3.43, nll_loss=1.864, ppl=3.64, wps=54771.7, ups=15.38, wpb=3561.2, bsz=156.9, num_updates=31400, lr=0.000252377, gnorm=1.023, loss_scale=32, train_wall=6, gb_free=13.6, wall=2292
2023-03-11 05:53:57 | INFO | train_inner | epoch 029:    644 / 1102 loss=3.435, nll_loss=1.87, ppl=3.66, wps=55126.1, ups=15.03, wpb=3666.9, bsz=158.4, num_updates=31500, lr=0.000251976, gnorm=0.984, loss_scale=32, train_wall=6, gb_free=13.7, wall=2298
2023-03-11 05:54:03 | INFO | train_inner | epoch 029:    744 / 1102 loss=3.511, nll_loss=1.955, ppl=3.88, wps=53293.7, ups=15.14, wpb=3520.2, bsz=125.2, num_updates=31600, lr=0.000251577, gnorm=1.072, loss_scale=32, train_wall=6, gb_free=13.4, wall=2305
2023-03-11 05:54:10 | INFO | train_inner | epoch 029:    844 / 1102 loss=3.493, nll_loss=1.936, ppl=3.83, wps=54335.6, ups=15.24, wpb=3566.3, bsz=146.9, num_updates=31700, lr=0.00025118, gnorm=1.049, loss_scale=32, train_wall=6, gb_free=13.4, wall=2312
2023-03-11 05:54:17 | INFO | train_inner | epoch 029:    944 / 1102 loss=3.476, nll_loss=1.917, ppl=3.78, wps=53894.4, ups=14.99, wpb=3594.6, bsz=157.8, num_updates=31800, lr=0.000250785, gnorm=1.048, loss_scale=32, train_wall=6, gb_free=13.5, wall=2318
2023-03-11 05:54:23 | INFO | train_inner | epoch 029:   1044 / 1102 loss=3.483, nll_loss=1.925, ppl=3.8, wps=54222.3, ups=15.2, wpb=3568.2, bsz=145.6, num_updates=31900, lr=0.000250392, gnorm=1.042, loss_scale=32, train_wall=6, gb_free=13.6, wall=2325
2023-03-11 05:54:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:54:28 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.987 | nll_loss 2.378 | ppl 5.2 | wps 138559 | wpb 2790.1 | bsz 113.8 | num_updates 31958 | best_loss 3.98
2023-03-11 05:54:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 31958 updates
2023-03-11 05:54:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:54:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 05:54:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 29 @ 31958 updates, score 3.987) (writing took 3.8807098729994323 seconds)
2023-03-11 05:54:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-03-11 05:54:32 | INFO | train | epoch 029 | loss 3.463 | nll_loss 1.901 | ppl 3.73 | wps 50722.5 | ups 14.16 | wpb 3581.5 | bsz 145.4 | num_updates 31958 | lr 0.000250164 | gnorm 1.029 | loss_scale 32 | train_wall 71 | gb_free 13.5 | wall 2334
2023-03-11 05:54:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:54:32 | INFO | fairseq.trainer | begin training epoch 30
2023-03-11 05:54:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:54:35 | INFO | train_inner | epoch 030:     42 / 1102 loss=3.483, nll_loss=1.924, ppl=3.79, wps=30808.5, ups=8.37, wpb=3678.6, bsz=137.2, num_updates=32000, lr=0.00025, gnorm=1.009, loss_scale=32, train_wall=6, gb_free=13.5, wall=2337
2023-03-11 05:54:42 | INFO | train_inner | epoch 030:    142 / 1102 loss=3.399, nll_loss=1.827, ppl=3.55, wps=54765.8, ups=15.34, wpb=3570.1, bsz=137.8, num_updates=32100, lr=0.00024961, gnorm=1.017, loss_scale=32, train_wall=6, gb_free=13.4, wall=2343
2023-03-11 05:54:48 | INFO | train_inner | epoch 030:    242 / 1102 loss=3.396, nll_loss=1.823, ppl=3.54, wps=53921.6, ups=15.19, wpb=3548.8, bsz=146.2, num_updates=32200, lr=0.000249222, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=13.5, wall=2350
2023-03-11 05:54:55 | INFO | train_inner | epoch 030:    342 / 1102 loss=3.372, nll_loss=1.799, ppl=3.48, wps=55642.1, ups=15.16, wpb=3669.7, bsz=168.5, num_updates=32300, lr=0.000248836, gnorm=0.985, loss_scale=32, train_wall=6, gb_free=13.4, wall=2356
2023-03-11 05:55:01 | INFO | train_inner | epoch 030:    442 / 1102 loss=3.457, nll_loss=1.893, ppl=3.71, wps=53682, ups=15.27, wpb=3515, bsz=139.4, num_updates=32400, lr=0.000248452, gnorm=1.059, loss_scale=32, train_wall=6, gb_free=13.5, wall=2363
2023-03-11 05:55:08 | INFO | train_inner | epoch 030:    542 / 1102 loss=3.432, nll_loss=1.865, ppl=3.64, wps=54293.7, ups=15.36, wpb=3534.5, bsz=147.2, num_updates=32500, lr=0.000248069, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.6, wall=2369
2023-03-11 05:55:14 | INFO | train_inner | epoch 030:    642 / 1102 loss=3.474, nll_loss=1.913, ppl=3.77, wps=53787.7, ups=15.31, wpb=3513.9, bsz=140.4, num_updates=32600, lr=0.000247689, gnorm=1.051, loss_scale=32, train_wall=6, gb_free=13.4, wall=2376
2023-03-11 05:55:21 | INFO | train_inner | epoch 030:    742 / 1102 loss=3.503, nll_loss=1.946, ppl=3.85, wps=55216.8, ups=15.22, wpb=3628.1, bsz=127.7, num_updates=32700, lr=0.00024731, gnorm=1.035, loss_scale=32, train_wall=6, gb_free=13.5, wall=2383
2023-03-11 05:55:28 | INFO | train_inner | epoch 030:    842 / 1102 loss=3.422, nll_loss=1.856, ppl=3.62, wps=54561.4, ups=15.1, wpb=3613.4, bsz=159.5, num_updates=32800, lr=0.000246932, gnorm=0.997, loss_scale=64, train_wall=6, gb_free=13.5, wall=2389
2023-03-11 05:55:34 | INFO | train_inner | epoch 030:    942 / 1102 loss=3.497, nll_loss=1.94, ppl=3.84, wps=54218.5, ups=15.37, wpb=3527.3, bsz=133.5, num_updates=32900, lr=0.000246557, gnorm=1.051, loss_scale=64, train_wall=6, gb_free=13.6, wall=2396
2023-03-11 05:55:41 | INFO | train_inner | epoch 030:   1042 / 1102 loss=3.452, nll_loss=1.89, ppl=3.71, wps=56256.8, ups=15.18, wpb=3705.7, bsz=157, num_updates=33000, lr=0.000246183, gnorm=0.992, loss_scale=64, train_wall=6, gb_free=13.6, wall=2402
2023-03-11 05:55:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:55:46 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.978 | nll_loss 2.366 | ppl 5.15 | wps 135933 | wpb 2790.1 | bsz 113.8 | num_updates 33060 | best_loss 3.978
2023-03-11 05:55:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 33060 updates
2023-03-11 05:55:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:55:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:55:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 30 @ 33060 updates, score 3.978) (writing took 7.120053275000828 seconds)
2023-03-11 05:55:53 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-03-11 05:55:53 | INFO | train | epoch 030 | loss 3.441 | nll_loss 1.876 | ppl 3.67 | wps 48827.2 | ups 13.63 | wpb 3581.5 | bsz 145.4 | num_updates 33060 | lr 0.000245959 | gnorm 1.025 | loss_scale 64 | train_wall 70 | gb_free 13.5 | wall 2415
2023-03-11 05:55:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:55:53 | INFO | fairseq.trainer | begin training epoch 31
2023-03-11 05:55:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:55:56 | INFO | train_inner | epoch 031:     40 / 1102 loss=3.412, nll_loss=1.844, ppl=3.59, wps=23271.2, ups=6.62, wpb=3517.4, bsz=147, num_updates=33100, lr=0.000245811, gnorm=1.032, loss_scale=64, train_wall=6, gb_free=13.5, wall=2417
2023-03-11 05:56:02 | INFO | train_inner | epoch 031:    140 / 1102 loss=3.369, nll_loss=1.793, ppl=3.47, wps=55942.6, ups=15.39, wpb=3634.5, bsz=156.2, num_updates=33200, lr=0.00024544, gnorm=1.011, loss_scale=64, train_wall=6, gb_free=13.5, wall=2424
2023-03-11 05:56:09 | INFO | train_inner | epoch 031:    240 / 1102 loss=3.375, nll_loss=1.801, ppl=3.49, wps=55207.4, ups=15.26, wpb=3617.8, bsz=152.8, num_updates=33300, lr=0.000245072, gnorm=1.003, loss_scale=64, train_wall=6, gb_free=13.6, wall=2430
2023-03-11 05:56:15 | INFO | train_inner | epoch 031:    340 / 1102 loss=3.43, nll_loss=1.863, ppl=3.64, wps=53520.4, ups=15.21, wpb=3518.7, bsz=134.3, num_updates=33400, lr=0.000244704, gnorm=1.053, loss_scale=64, train_wall=6, gb_free=13.6, wall=2437
2023-03-11 05:56:22 | INFO | train_inner | epoch 031:    440 / 1102 loss=3.434, nll_loss=1.866, ppl=3.64, wps=54431.2, ups=15.33, wpb=3551.6, bsz=127.2, num_updates=33500, lr=0.000244339, gnorm=1.033, loss_scale=64, train_wall=6, gb_free=13.5, wall=2444
2023-03-11 05:56:29 | INFO | train_inner | epoch 031:    540 / 1102 loss=3.437, nll_loss=1.87, ppl=3.65, wps=55282.5, ups=15.41, wpb=3587.1, bsz=138.6, num_updates=33600, lr=0.000243975, gnorm=1.048, loss_scale=64, train_wall=6, gb_free=13.5, wall=2450
2023-03-11 05:56:35 | INFO | train_inner | epoch 031:    640 / 1102 loss=3.412, nll_loss=1.843, ppl=3.59, wps=53334.3, ups=15.47, wpb=3447.2, bsz=154.2, num_updates=33700, lr=0.000243613, gnorm=1.067, loss_scale=64, train_wall=6, gb_free=13.6, wall=2457
2023-03-11 05:56:42 | INFO | train_inner | epoch 031:    740 / 1102 loss=3.447, nll_loss=1.881, ppl=3.68, wps=54269.2, ups=15.29, wpb=3548.8, bsz=137.3, num_updates=33800, lr=0.000243252, gnorm=1.046, loss_scale=64, train_wall=6, gb_free=13.5, wall=2463
2023-03-11 05:56:48 | INFO | train_inner | epoch 031:    840 / 1102 loss=3.441, nll_loss=1.877, ppl=3.67, wps=54896, ups=15.18, wpb=3615.4, bsz=147.5, num_updates=33900, lr=0.000242893, gnorm=1.015, loss_scale=64, train_wall=6, gb_free=13.6, wall=2470
2023-03-11 05:56:55 | INFO | train_inner | epoch 031:    940 / 1102 loss=3.443, nll_loss=1.879, ppl=3.68, wps=55023.8, ups=15.22, wpb=3615.9, bsz=143.5, num_updates=34000, lr=0.000242536, gnorm=1.026, loss_scale=64, train_wall=6, gb_free=13.6, wall=2476
2023-03-11 05:57:01 | INFO | train_inner | epoch 031:   1040 / 1102 loss=3.461, nll_loss=1.9, ppl=3.73, wps=55638.2, ups=15.24, wpb=3651.5, bsz=153.4, num_updates=34100, lr=0.00024218, gnorm=1.025, loss_scale=64, train_wall=6, gb_free=13.6, wall=2483
2023-03-11 05:57:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:57:07 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.975 | nll_loss 2.365 | ppl 5.15 | wps 136611 | wpb 2790.1 | bsz 113.8 | num_updates 34162 | best_loss 3.975
2023-03-11 05:57:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 34162 updates
2023-03-11 05:57:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:57:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:57:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 31 @ 34162 updates, score 3.975) (writing took 7.15722113800075 seconds)
2023-03-11 05:57:14 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-03-11 05:57:14 | INFO | train | epoch 031 | loss 3.422 | nll_loss 1.854 | ppl 3.62 | wps 48892.9 | ups 13.65 | wpb 3581.5 | bsz 145.4 | num_updates 34162 | lr 0.00024196 | gnorm 1.03 | loss_scale 64 | train_wall 70 | gb_free 13.5 | wall 2495
2023-03-11 05:57:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:57:14 | INFO | fairseq.trainer | begin training epoch 32
2023-03-11 05:57:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:57:17 | INFO | train_inner | epoch 032:     38 / 1102 loss=3.405, nll_loss=1.834, ppl=3.57, wps=23577.9, ups=6.55, wpb=3600.6, bsz=150.6, num_updates=34200, lr=0.000241825, gnorm=1.016, loss_scale=64, train_wall=6, gb_free=13.6, wall=2498
2023-03-11 05:57:23 | INFO | train_inner | epoch 032:    138 / 1102 loss=3.341, nll_loss=1.761, ppl=3.39, wps=55442.7, ups=15.33, wpb=3617.5, bsz=148.6, num_updates=34300, lr=0.000241473, gnorm=0.991, loss_scale=64, train_wall=6, gb_free=13.6, wall=2505
2023-03-11 05:57:30 | INFO | train_inner | epoch 032:    238 / 1102 loss=3.392, nll_loss=1.818, ppl=3.53, wps=55777.2, ups=15.46, wpb=3607.7, bsz=137, num_updates=34400, lr=0.000241121, gnorm=1.04, loss_scale=64, train_wall=6, gb_free=13.6, wall=2511
2023-03-11 05:57:36 | INFO | train_inner | epoch 032:    338 / 1102 loss=3.379, nll_loss=1.804, ppl=3.49, wps=55657.7, ups=15.4, wpb=3615, bsz=145.7, num_updates=34500, lr=0.000240772, gnorm=1.012, loss_scale=64, train_wall=6, gb_free=13.5, wall=2518
2023-03-11 05:57:42 | INFO | train_inner | epoch 032:    438 / 1102 loss=3.377, nll_loss=1.803, ppl=3.49, wps=53915.2, ups=15.49, wpb=3480.2, bsz=152.7, num_updates=34600, lr=0.000240424, gnorm=1.056, loss_scale=64, train_wall=6, gb_free=13.5, wall=2524
2023-03-11 05:57:49 | INFO | train_inner | epoch 032:    538 / 1102 loss=3.428, nll_loss=1.858, ppl=3.63, wps=54421.9, ups=15.36, wpb=3542.1, bsz=129.8, num_updates=34700, lr=0.000240077, gnorm=1.055, loss_scale=64, train_wall=6, gb_free=13.6, wall=2531
2023-03-11 05:57:55 | INFO | train_inner | epoch 032:    638 / 1102 loss=3.424, nll_loss=1.855, ppl=3.62, wps=55487, ups=15.47, wpb=3585.7, bsz=132.2, num_updates=34800, lr=0.000239732, gnorm=1.041, loss_scale=64, train_wall=6, gb_free=13.6, wall=2537
2023-03-11 05:58:02 | INFO | train_inner | epoch 032:    738 / 1102 loss=3.392, nll_loss=1.82, ppl=3.53, wps=55318.5, ups=15.34, wpb=3605.8, bsz=152.8, num_updates=34900, lr=0.000239388, gnorm=1.013, loss_scale=64, train_wall=6, gb_free=13.6, wall=2544
2023-03-11 05:58:08 | INFO | train_inner | epoch 032:    838 / 1102 loss=3.454, nll_loss=1.889, ppl=3.7, wps=54714.7, ups=15.39, wpb=3554.7, bsz=133.2, num_updates=35000, lr=0.000239046, gnorm=1.07, loss_scale=64, train_wall=6, gb_free=13.5, wall=2550
2023-03-11 05:58:15 | INFO | train_inner | epoch 032:    938 / 1102 loss=3.415, nll_loss=1.848, ppl=3.6, wps=55193.8, ups=15.61, wpb=3536.4, bsz=153.6, num_updates=35100, lr=0.000238705, gnorm=1.034, loss_scale=64, train_wall=6, gb_free=13.6, wall=2556
2023-03-11 05:58:21 | INFO | train_inner | epoch 032:   1038 / 1102 loss=3.441, nll_loss=1.877, ppl=3.67, wps=56128.4, ups=15.5, wpb=3622.2, bsz=155.4, num_updates=35200, lr=0.000238366, gnorm=1.018, loss_scale=64, train_wall=6, gb_free=13.4, wall=2563
2023-03-11 05:58:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:58:27 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.975 | nll_loss 2.359 | ppl 5.13 | wps 135500 | wpb 2790.1 | bsz 113.8 | num_updates 35264 | best_loss 3.975
2023-03-11 05:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 35264 updates
2023-03-11 05:58:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:58:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 32 @ 35264 updates, score 3.975) (writing took 7.100795124999422 seconds)
2023-03-11 05:58:34 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-03-11 05:58:34 | INFO | train | epoch 032 | loss 3.404 | nll_loss 1.833 | ppl 3.56 | wps 49278.1 | ups 13.76 | wpb 3581.5 | bsz 145.4 | num_updates 35264 | lr 0.000238149 | gnorm 1.031 | loss_scale 64 | train_wall 70 | gb_free 13.4 | wall 2576
2023-03-11 05:58:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:58:34 | INFO | fairseq.trainer | begin training epoch 33
2023-03-11 05:58:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:58:36 | INFO | train_inner | epoch 033:     36 / 1102 loss=3.39, nll_loss=1.82, ppl=3.53, wps=24005.3, ups=6.66, wpb=3606.1, bsz=159, num_updates=35300, lr=0.000238028, gnorm=1.005, loss_scale=64, train_wall=6, gb_free=13.6, wall=2578
2023-03-11 05:58:43 | INFO | train_inner | epoch 033:    136 / 1102 loss=3.354, nll_loss=1.774, ppl=3.42, wps=55611.2, ups=15.54, wpb=3578.3, bsz=139.7, num_updates=35400, lr=0.000237691, gnorm=1.032, loss_scale=64, train_wall=6, gb_free=13.5, wall=2584
2023-03-11 05:58:49 | INFO | train_inner | epoch 033:    236 / 1102 loss=3.353, nll_loss=1.776, ppl=3.42, wps=55956.5, ups=15.45, wpb=3622.4, bsz=153, num_updates=35500, lr=0.000237356, gnorm=1.018, loss_scale=64, train_wall=6, gb_free=13.6, wall=2591
2023-03-11 05:58:56 | INFO | train_inner | epoch 033:    336 / 1102 loss=3.363, nll_loss=1.785, ppl=3.45, wps=55123.5, ups=15.44, wpb=3570.2, bsz=136.7, num_updates=35600, lr=0.000237023, gnorm=1.032, loss_scale=64, train_wall=6, gb_free=13.7, wall=2597
2023-03-11 05:59:02 | INFO | train_inner | epoch 033:    436 / 1102 loss=3.387, nll_loss=1.813, ppl=3.51, wps=53955.3, ups=15.03, wpb=3589.8, bsz=146.9, num_updates=35700, lr=0.000236691, gnorm=1.041, loss_scale=64, train_wall=6, gb_free=13.5, wall=2604
2023-03-11 05:59:09 | INFO | train_inner | epoch 033:    536 / 1102 loss=3.415, nll_loss=1.844, ppl=3.59, wps=56390.3, ups=15.4, wpb=3661.9, bsz=134.1, num_updates=35800, lr=0.00023636, gnorm=1.023, loss_scale=64, train_wall=6, gb_free=13.5, wall=2610
2023-03-11 05:59:15 | INFO | train_inner | epoch 033:    636 / 1102 loss=3.394, nll_loss=1.823, ppl=3.54, wps=54518, ups=15.42, wpb=3536.2, bsz=143.8, num_updates=35900, lr=0.00023603, gnorm=1.07, loss_scale=64, train_wall=6, gb_free=13.6, wall=2617
2023-03-11 05:59:22 | INFO | train_inner | epoch 033:    736 / 1102 loss=3.378, nll_loss=1.805, ppl=3.49, wps=55413.5, ups=15.43, wpb=3590.4, bsz=156.3, num_updates=36000, lr=0.000235702, gnorm=1.032, loss_scale=64, train_wall=6, gb_free=13.6, wall=2623
2023-03-11 05:59:28 | INFO | train_inner | epoch 033:    836 / 1102 loss=3.386, nll_loss=1.813, ppl=3.51, wps=54832.5, ups=15.27, wpb=3591.8, bsz=147.9, num_updates=36100, lr=0.000235376, gnorm=1.028, loss_scale=64, train_wall=6, gb_free=13.5, wall=2630
2023-03-11 05:59:35 | INFO | train_inner | epoch 033:    936 / 1102 loss=3.411, nll_loss=1.842, ppl=3.59, wps=53664.7, ups=15.28, wpb=3511.7, bsz=146.8, num_updates=36200, lr=0.00023505, gnorm=1.067, loss_scale=64, train_wall=6, gb_free=13.4, wall=2637
2023-03-11 05:59:42 | INFO | train_inner | epoch 033:   1036 / 1102 loss=3.406, nll_loss=1.836, ppl=3.57, wps=54537.2, ups=15.24, wpb=3578.1, bsz=144.9, num_updates=36300, lr=0.000234726, gnorm=1.03, loss_scale=64, train_wall=6, gb_free=13.5, wall=2643
2023-03-11 05:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 05:59:47 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.962 | nll_loss 2.352 | ppl 5.11 | wps 136343 | wpb 2790.1 | bsz 113.8 | num_updates 36366 | best_loss 3.962
2023-03-11 05:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 36366 updates
2023-03-11 05:59:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:59:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_best.pt
2023-03-11 05:59:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_best.pt (epoch 33 @ 36366 updates, score 3.962) (writing took 7.082368185998348 seconds)
2023-03-11 05:59:54 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-03-11 05:59:54 | INFO | train | epoch 033 | loss 3.385 | nll_loss 1.812 | ppl 3.51 | wps 49115.2 | ups 13.71 | wpb 3581.5 | bsz 145.4 | num_updates 36366 | lr 0.000234513 | gnorm 1.035 | loss_scale 64 | train_wall 70 | gb_free 13.5 | wall 2656
2023-03-11 05:59:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 05:59:54 | INFO | fairseq.trainer | begin training epoch 34
2023-03-11 05:59:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 05:59:57 | INFO | train_inner | epoch 034:     34 / 1102 loss=3.383, nll_loss=1.81, ppl=3.51, wps=23397.4, ups=6.6, wpb=3543.1, bsz=156.2, num_updates=36400, lr=0.000234404, gnorm=1.018, loss_scale=64, train_wall=6, gb_free=13.5, wall=2658
2023-03-11 06:00:03 | INFO | train_inner | epoch 034:    134 / 1102 loss=3.325, nll_loss=1.744, ppl=3.35, wps=54989.9, ups=15.16, wpb=3627.4, bsz=155.1, num_updates=36500, lr=0.000234082, gnorm=1.001, loss_scale=64, train_wall=6, gb_free=13.5, wall=2665
2023-03-11 06:00:10 | INFO | train_inner | epoch 034:    234 / 1102 loss=3.319, nll_loss=1.736, ppl=3.33, wps=53519.2, ups=15.09, wpb=3546.7, bsz=151, num_updates=36600, lr=0.000233762, gnorm=1.031, loss_scale=64, train_wall=6, gb_free=13.6, wall=2671
2023-03-11 06:00:16 | INFO | train_inner | epoch 034:    334 / 1102 loss=3.342, nll_loss=1.762, ppl=3.39, wps=54032, ups=15.23, wpb=3548.7, bsz=142, num_updates=36700, lr=0.000233444, gnorm=1.048, loss_scale=64, train_wall=6, gb_free=13.5, wall=2678
2023-03-11 06:00:23 | INFO | train_inner | epoch 034:    434 / 1102 loss=3.398, nll_loss=1.824, ppl=3.54, wps=54640.3, ups=15.2, wpb=3593.7, bsz=125.4, num_updates=36800, lr=0.000233126, gnorm=1.053, loss_scale=64, train_wall=6, gb_free=13.7, wall=2685
2023-03-11 06:00:30 | INFO | train_inner | epoch 034:    534 / 1102 loss=3.369, nll_loss=1.792, ppl=3.46, wps=54834.3, ups=15.3, wpb=3584.6, bsz=138.2, num_updates=36900, lr=0.00023281, gnorm=1.033, loss_scale=64, train_wall=6, gb_free=13.5, wall=2691
2023-03-11 06:00:36 | INFO | train_inner | epoch 034:    634 / 1102 loss=3.361, nll_loss=1.786, ppl=3.45, wps=55968.9, ups=15.3, wpb=3657.8, bsz=162.4, num_updates=37000, lr=0.000232495, gnorm=0.999, loss_scale=64, train_wall=6, gb_free=13.5, wall=2698
2023-03-11 06:00:43 | INFO | train_inner | epoch 034:    734 / 1102 loss=3.362, nll_loss=1.786, ppl=3.45, wps=55560, ups=15.33, wpb=3624.9, bsz=153.1, num_updates=37100, lr=0.000232182, gnorm=1.014, loss_scale=64, train_wall=6, gb_free=13.6, wall=2704
2023-03-11 06:00:49 | INFO | train_inner | epoch 034:    834 / 1102 loss=3.359, nll_loss=1.783, ppl=3.44, wps=55164.7, ups=15.49, wpb=3561.7, bsz=161.4, num_updates=37200, lr=0.000231869, gnorm=1.029, loss_scale=64, train_wall=6, gb_free=13.4, wall=2711
2023-03-11 06:00:56 | INFO | train_inner | epoch 034:    934 / 1102 loss=3.44, nll_loss=1.873, ppl=3.66, wps=54608.6, ups=15.3, wpb=3568.4, bsz=131, num_updates=37300, lr=0.000231558, gnorm=1.073, loss_scale=64, train_wall=6, gb_free=13.4, wall=2717
2023-03-11 06:01:02 | INFO | train_inner | epoch 034:   1034 / 1102 loss=3.392, nll_loss=1.82, ppl=3.53, wps=53832.4, ups=15.02, wpb=3584.3, bsz=146.3, num_updates=37400, lr=0.000231249, gnorm=1.033, loss_scale=64, train_wall=6, gb_free=13.6, wall=2724
2023-03-11 06:01:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-03-11 06:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:01:08 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.979 | nll_loss 2.362 | ppl 5.14 | wps 140736 | wpb 2790.1 | bsz 113.8 | num_updates 37467 | best_loss 3.962
2023-03-11 06:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 37467 updates
2023-03-11 06:01:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:01:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:01:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 34 @ 37467 updates, score 3.979) (writing took 3.7947743049990095 seconds)
2023-03-11 06:01:12 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-03-11 06:01:12 | INFO | train | epoch 034 | loss 3.369 | nll_loss 1.793 | ppl 3.47 | wps 50831 | ups 14.19 | wpb 3581.2 | bsz 145.4 | num_updates 37467 | lr 0.000231042 | gnorm 1.035 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 2733
2023-03-11 06:01:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:01:12 | INFO | fairseq.trainer | begin training epoch 35
2023-03-11 06:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:01:14 | INFO | train_inner | epoch 035:     33 / 1102 loss=3.381, nll_loss=1.806, ppl=3.5, wps=29945.3, ups=8.43, wpb=3553.4, bsz=134.3, num_updates=37500, lr=0.00023094, gnorm=1.055, loss_scale=32, train_wall=6, gb_free=13.5, wall=2736
2023-03-11 06:01:21 | INFO | train_inner | epoch 035:    133 / 1102 loss=3.309, nll_loss=1.723, ppl=3.3, wps=55875.6, ups=15.45, wpb=3616.2, bsz=147.2, num_updates=37600, lr=0.000230633, gnorm=1.01, loss_scale=32, train_wall=6, gb_free=13.6, wall=2742
2023-03-11 06:01:27 | INFO | train_inner | epoch 035:    233 / 1102 loss=3.326, nll_loss=1.742, ppl=3.35, wps=54036.3, ups=15.35, wpb=3520.8, bsz=133.2, num_updates=37700, lr=0.000230327, gnorm=1.051, loss_scale=32, train_wall=6, gb_free=13.5, wall=2749
2023-03-11 06:01:34 | INFO | train_inner | epoch 035:    333 / 1102 loss=3.322, nll_loss=1.739, ppl=3.34, wps=55133.6, ups=15.23, wpb=3621, bsz=151.1, num_updates=37800, lr=0.000230022, gnorm=1.021, loss_scale=32, train_wall=6, gb_free=13.6, wall=2755
2023-03-11 06:01:40 | INFO | train_inner | epoch 035:    433 / 1102 loss=3.348, nll_loss=1.769, ppl=3.41, wps=55738.5, ups=15.36, wpb=3629.9, bsz=143, num_updates=37900, lr=0.000229718, gnorm=1.03, loss_scale=32, train_wall=6, gb_free=13.6, wall=2762
2023-03-11 06:01:47 | INFO | train_inner | epoch 035:    533 / 1102 loss=3.346, nll_loss=1.766, ppl=3.4, wps=54374.5, ups=15.35, wpb=3542.6, bsz=144.3, num_updates=38000, lr=0.000229416, gnorm=1.057, loss_scale=32, train_wall=6, gb_free=13.6, wall=2768
2023-03-11 06:01:53 | INFO | train_inner | epoch 035:    633 / 1102 loss=3.359, nll_loss=1.782, ppl=3.44, wps=54045.3, ups=15.12, wpb=3575.2, bsz=146.4, num_updates=38100, lr=0.000229114, gnorm=1.045, loss_scale=32, train_wall=6, gb_free=13.6, wall=2775
2023-03-11 06:02:00 | INFO | train_inner | epoch 035:    733 / 1102 loss=3.359, nll_loss=1.782, ppl=3.44, wps=55116.9, ups=15.31, wpb=3599.6, bsz=148.5, num_updates=38200, lr=0.000228814, gnorm=1.026, loss_scale=32, train_wall=6, gb_free=13.5, wall=2781
2023-03-11 06:02:07 | INFO | train_inner | epoch 035:    833 / 1102 loss=3.352, nll_loss=1.775, ppl=3.42, wps=54530.6, ups=15.12, wpb=3607.1, bsz=160.2, num_updates=38300, lr=0.000228515, gnorm=1.028, loss_scale=32, train_wall=6, gb_free=13.5, wall=2788
2023-03-11 06:02:13 | INFO | train_inner | epoch 035:    933 / 1102 loss=3.388, nll_loss=1.815, ppl=3.52, wps=53927.4, ups=15.4, wpb=3501, bsz=134.6, num_updates=38400, lr=0.000228218, gnorm=1.082, loss_scale=32, train_wall=6, gb_free=13.6, wall=2795
2023-03-11 06:02:20 | INFO | train_inner | epoch 035:   1033 / 1102 loss=3.422, nll_loss=1.853, ppl=3.61, wps=55073.1, ups=15.43, wpb=3568.2, bsz=140.6, num_updates=38500, lr=0.000227921, gnorm=1.076, loss_scale=32, train_wall=6, gb_free=13.5, wall=2801
2023-03-11 06:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:02:25 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.969 | nll_loss 2.359 | ppl 5.13 | wps 143272 | wpb 2790.1 | bsz 113.8 | num_updates 38569 | best_loss 3.962
2023-03-11 06:02:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 38569 updates
2023-03-11 06:02:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:02:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 35 @ 38569 updates, score 3.969) (writing took 3.7896748969978944 seconds)
2023-03-11 06:02:29 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2023-03-11 06:02:29 | INFO | train | epoch 035 | loss 3.352 | nll_loss 1.774 | ppl 3.42 | wps 51113.2 | ups 14.27 | wpb 3581.5 | bsz 145.4 | num_updates 38569 | lr 0.000227717 | gnorm 1.042 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 2811
2023-03-11 06:02:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:02:29 | INFO | fairseq.trainer | begin training epoch 36
2023-03-11 06:02:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:02:31 | INFO | train_inner | epoch 036:     31 / 1102 loss=3.343, nll_loss=1.764, ppl=3.4, wps=30786.1, ups=8.53, wpb=3608, bsz=148.5, num_updates=38600, lr=0.000227626, gnorm=1.036, loss_scale=32, train_wall=6, gb_free=13.5, wall=2813
2023-03-11 06:02:38 | INFO | train_inner | epoch 036:    131 / 1102 loss=3.285, nll_loss=1.695, ppl=3.24, wps=53229.5, ups=15.22, wpb=3496.8, bsz=140, num_updates=38700, lr=0.000227331, gnorm=1.058, loss_scale=32, train_wall=6, gb_free=13.5, wall=2819
2023-03-11 06:02:44 | INFO | train_inner | epoch 036:    231 / 1102 loss=3.272, nll_loss=1.683, ppl=3.21, wps=54493.1, ups=15.46, wpb=3524.6, bsz=160.1, num_updates=38800, lr=0.000227038, gnorm=1.023, loss_scale=32, train_wall=6, gb_free=13.6, wall=2826
2023-03-11 06:02:51 | INFO | train_inner | epoch 036:    331 / 1102 loss=3.325, nll_loss=1.742, ppl=3.35, wps=55627.6, ups=15.38, wpb=3616.8, bsz=144.9, num_updates=38900, lr=0.000226746, gnorm=1.039, loss_scale=32, train_wall=6, gb_free=13.6, wall=2832
2023-03-11 06:02:57 | INFO | train_inner | epoch 036:    431 / 1102 loss=3.346, nll_loss=1.766, ppl=3.4, wps=55780.8, ups=15.4, wpb=3623.2, bsz=140.6, num_updates=39000, lr=0.000226455, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.6, wall=2839
2023-03-11 06:03:04 | INFO | train_inner | epoch 036:    531 / 1102 loss=3.343, nll_loss=1.763, ppl=3.39, wps=55813.3, ups=15.25, wpb=3661.1, bsz=144, num_updates=39100, lr=0.000226166, gnorm=1.045, loss_scale=32, train_wall=6, gb_free=13.6, wall=2845
2023-03-11 06:03:10 | INFO | train_inner | epoch 036:    631 / 1102 loss=3.297, nll_loss=1.711, ppl=3.27, wps=55060.9, ups=15.33, wpb=3590.5, bsz=159.4, num_updates=39200, lr=0.000225877, gnorm=1.022, loss_scale=32, train_wall=6, gb_free=13.7, wall=2852
2023-03-11 06:03:17 | INFO | train_inner | epoch 036:    731 / 1102 loss=3.367, nll_loss=1.789, ppl=3.46, wps=54704.2, ups=15.47, wpb=3535.6, bsz=135.9, num_updates=39300, lr=0.000225589, gnorm=1.063, loss_scale=32, train_wall=6, gb_free=13.5, wall=2858
2023-03-11 06:03:23 | INFO | train_inner | epoch 036:    831 / 1102 loss=3.348, nll_loss=1.769, ppl=3.41, wps=55592, ups=15.39, wpb=3611.1, bsz=150.4, num_updates=39400, lr=0.000225303, gnorm=1.036, loss_scale=32, train_wall=6, gb_free=13.5, wall=2865
2023-03-11 06:03:30 | INFO | train_inner | epoch 036:    931 / 1102 loss=3.385, nll_loss=1.81, ppl=3.51, wps=54634, ups=15.53, wpb=3517.2, bsz=131.4, num_updates=39500, lr=0.000225018, gnorm=1.069, loss_scale=32, train_wall=6, gb_free=13.7, wall=2871
2023-03-11 06:03:36 | INFO | train_inner | epoch 036:   1031 / 1102 loss=3.37, nll_loss=1.795, ppl=3.47, wps=55908.8, ups=15.44, wpb=3620.6, bsz=151.6, num_updates=39600, lr=0.000224733, gnorm=1.049, loss_scale=32, train_wall=6, gb_free=13.6, wall=2878
2023-03-11 06:03:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:03:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.993 | nll_loss 2.388 | ppl 5.23 | wps 131132 | wpb 2790.1 | bsz 113.8 | num_updates 39671 | best_loss 3.962
2023-03-11 06:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 39671 updates
2023-03-11 06:03:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:03:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:03:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 36 @ 39671 updates, score 3.993) (writing took 3.8738228649999655 seconds)
2023-03-11 06:03:46 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2023-03-11 06:03:46 | INFO | train | epoch 036 | loss 3.337 | nll_loss 1.756 | ppl 3.38 | wps 51157.3 | ups 14.28 | wpb 3581.5 | bsz 145.4 | num_updates 39671 | lr 0.000224532 | gnorm 1.045 | loss_scale 32 | train_wall 70 | gb_free 13.5 | wall 2888
2023-03-11 06:03:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:03:46 | INFO | fairseq.trainer | begin training epoch 37
2023-03-11 06:03:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:03:48 | INFO | train_inner | epoch 037:     29 / 1102 loss=3.383, nll_loss=1.808, ppl=3.5, wps=29076.7, ups=8.25, wpb=3525.7, bsz=132.2, num_updates=39700, lr=0.00022445, gnorm=1.086, loss_scale=32, train_wall=6, gb_free=13.5, wall=2890
2023-03-11 06:03:55 | INFO | train_inner | epoch 037:    129 / 1102 loss=3.238, nll_loss=1.643, ppl=3.12, wps=52736.4, ups=14.48, wpb=3643.2, bsz=161.7, num_updates=39800, lr=0.000224168, gnorm=0.987, loss_scale=32, train_wall=7, gb_free=13.6, wall=2897
2023-03-11 06:04:02 | INFO | train_inner | epoch 037:    229 / 1102 loss=3.263, nll_loss=1.672, ppl=3.19, wps=52965.6, ups=14.78, wpb=3583.6, bsz=158.8, num_updates=39900, lr=0.000223887, gnorm=1.036, loss_scale=32, train_wall=7, gb_free=13.5, wall=2904
2023-03-11 06:04:09 | INFO | train_inner | epoch 037:    329 / 1102 loss=3.298, nll_loss=1.711, ppl=3.27, wps=54374.6, ups=14.96, wpb=3635.2, bsz=147.4, num_updates=40000, lr=0.000223607, gnorm=1.013, loss_scale=32, train_wall=7, gb_free=13.6, wall=2910
2023-03-11 06:04:15 | INFO | train_inner | epoch 037:    429 / 1102 loss=3.326, nll_loss=1.742, ppl=3.35, wps=55467.7, ups=15.47, wpb=3585.7, bsz=134.8, num_updates=40100, lr=0.000223328, gnorm=1.04, loss_scale=32, train_wall=6, gb_free=13.5, wall=2917
2023-03-11 06:04:22 | INFO | train_inner | epoch 037:    529 / 1102 loss=3.328, nll_loss=1.745, ppl=3.35, wps=56081, ups=15.55, wpb=3607, bsz=147.9, num_updates=40200, lr=0.00022305, gnorm=1.047, loss_scale=32, train_wall=6, gb_free=13.4, wall=2923
2023-03-11 06:04:28 | INFO | train_inner | epoch 037:    629 / 1102 loss=3.344, nll_loss=1.762, ppl=3.39, wps=53531, ups=15.35, wpb=3487.9, bsz=133.4, num_updates=40300, lr=0.000222773, gnorm=1.083, loss_scale=32, train_wall=6, gb_free=13.5, wall=2930
2023-03-11 06:04:35 | INFO | train_inner | epoch 037:    729 / 1102 loss=3.346, nll_loss=1.766, ppl=3.4, wps=55096, ups=15.5, wpb=3554.5, bsz=142.9, num_updates=40400, lr=0.000222497, gnorm=1.082, loss_scale=32, train_wall=6, gb_free=13.5, wall=2936
2023-03-11 06:04:41 | INFO | train_inner | epoch 037:    829 / 1102 loss=3.331, nll_loss=1.75, ppl=3.36, wps=54117.4, ups=15.13, wpb=3576, bsz=150.6, num_updates=40500, lr=0.000222222, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.5, wall=2943
2023-03-11 06:04:48 | INFO | train_inner | epoch 037:    929 / 1102 loss=3.364, nll_loss=1.786, ppl=3.45, wps=54748.8, ups=15.24, wpb=3591.4, bsz=132, num_updates=40600, lr=0.000221948, gnorm=1.065, loss_scale=32, train_wall=6, gb_free=13.5, wall=2949
2023-03-11 06:04:54 | INFO | train_inner | epoch 037:   1029 / 1102 loss=3.383, nll_loss=1.809, ppl=3.5, wps=54859.8, ups=15.41, wpb=3559.7, bsz=136.3, num_updates=40700, lr=0.000221676, gnorm=1.063, loss_scale=32, train_wall=6, gb_free=13.6, wall=2956
2023-03-11 06:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:05:00 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.983 | nll_loss 2.37 | ppl 5.17 | wps 137971 | wpb 2790.1 | bsz 113.8 | num_updates 40773 | best_loss 3.962
2023-03-11 06:05:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 40773 updates
2023-03-11 06:05:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:05:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:05:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 37 @ 40773 updates, score 3.983) (writing took 3.9226122369982477 seconds)
2023-03-11 06:05:04 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2023-03-11 06:05:04 | INFO | train | epoch 037 | loss 3.321 | nll_loss 1.738 | ppl 3.34 | wps 50556.7 | ups 14.12 | wpb 3581.5 | bsz 145.4 | num_updates 40773 | lr 0.000221477 | gnorm 1.046 | loss_scale 32 | train_wall 71 | gb_free 13.5 | wall 2966
2023-03-11 06:05:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:05:04 | INFO | fairseq.trainer | begin training epoch 38
2023-03-11 06:05:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:05:06 | INFO | train_inner | epoch 038:     27 / 1102 loss=3.321, nll_loss=1.74, ppl=3.34, wps=30762.4, ups=8.39, wpb=3668.4, bsz=152.3, num_updates=40800, lr=0.000221404, gnorm=1.015, loss_scale=32, train_wall=6, gb_free=13.5, wall=2968
2023-03-11 06:05:13 | INFO | train_inner | epoch 038:    127 / 1102 loss=3.234, nll_loss=1.639, ppl=3.12, wps=54706.2, ups=15.47, wpb=3536, bsz=163.8, num_updates=40900, lr=0.000221133, gnorm=1.017, loss_scale=32, train_wall=6, gb_free=13.5, wall=2974
2023-03-11 06:05:19 | INFO | train_inner | epoch 038:    227 / 1102 loss=3.271, nll_loss=1.68, ppl=3.2, wps=54551.3, ups=15.34, wpb=3557.1, bsz=150.4, num_updates=41000, lr=0.000220863, gnorm=1.034, loss_scale=32, train_wall=6, gb_free=13.5, wall=2981
2023-03-11 06:05:26 | INFO | train_inner | epoch 038:    327 / 1102 loss=3.288, nll_loss=1.697, ppl=3.24, wps=54305.8, ups=15.44, wpb=3517.1, bsz=129.6, num_updates=41100, lr=0.000220594, gnorm=1.044, loss_scale=32, train_wall=6, gb_free=13.4, wall=2987
2023-03-11 06:05:32 | INFO | train_inner | epoch 038:    427 / 1102 loss=3.286, nll_loss=1.698, ppl=3.25, wps=54310.6, ups=15.13, wpb=3590.1, bsz=148.3, num_updates=41200, lr=0.000220326, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.7, wall=2994
2023-03-11 06:05:39 | INFO | train_inner | epoch 038:    527 / 1102 loss=3.301, nll_loss=1.715, ppl=3.28, wps=56434, ups=15.52, wpb=3636.6, bsz=148.2, num_updates=41300, lr=0.000220059, gnorm=1.034, loss_scale=32, train_wall=6, gb_free=13.6, wall=3000
2023-03-11 06:05:45 | INFO | train_inner | epoch 038:    627 / 1102 loss=3.328, nll_loss=1.746, ppl=3.35, wps=55545.5, ups=15.49, wpb=3584.9, bsz=143.9, num_updates=41400, lr=0.000219793, gnorm=1.066, loss_scale=32, train_wall=6, gb_free=13.6, wall=3007
2023-03-11 06:05:52 | INFO | train_inner | epoch 038:    727 / 1102 loss=3.303, nll_loss=1.717, ppl=3.29, wps=55431.8, ups=15.43, wpb=3593.4, bsz=152.6, num_updates=41500, lr=0.000219529, gnorm=1.039, loss_scale=32, train_wall=6, gb_free=13.6, wall=3013
2023-03-11 06:05:58 | INFO | train_inner | epoch 038:    827 / 1102 loss=3.341, nll_loss=1.761, ppl=3.39, wps=55575.4, ups=15.48, wpb=3590, bsz=143.6, num_updates=41600, lr=0.000219265, gnorm=1.071, loss_scale=32, train_wall=6, gb_free=13.5, wall=3020
2023-03-11 06:06:05 | INFO | train_inner | epoch 038:    927 / 1102 loss=3.347, nll_loss=1.768, ppl=3.4, wps=54382.5, ups=15.29, wpb=3555.7, bsz=137.6, num_updates=41700, lr=0.000219001, gnorm=1.071, loss_scale=32, train_wall=6, gb_free=13.5, wall=3026
2023-03-11 06:06:11 | INFO | train_inner | epoch 038:   1027 / 1102 loss=3.345, nll_loss=1.765, ppl=3.4, wps=54967.4, ups=15.13, wpb=3632.6, bsz=142.7, num_updates=41800, lr=0.000218739, gnorm=1.058, loss_scale=32, train_wall=6, gb_free=13.5, wall=3033
2023-03-11 06:06:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:06:18 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.975 | nll_loss 2.361 | ppl 5.14 | wps 130783 | wpb 2790.1 | bsz 113.8 | num_updates 41875 | best_loss 3.962
2023-03-11 06:06:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 41875 updates
2023-03-11 06:06:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:06:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 38 @ 41875 updates, score 3.975) (writing took 3.823295965001307 seconds)
2023-03-11 06:06:22 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2023-03-11 06:06:22 | INFO | train | epoch 038 | loss 3.306 | nll_loss 1.721 | ppl 3.3 | wps 51117.5 | ups 14.27 | wpb 3581.5 | bsz 145.4 | num_updates 41875 | lr 0.000218543 | gnorm 1.046 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 3043
2023-03-11 06:06:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:06:22 | INFO | fairseq.trainer | begin training epoch 39
2023-03-11 06:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:06:23 | INFO | train_inner | epoch 039:     25 / 1102 loss=3.306, nll_loss=1.72, ppl=3.29, wps=29615.3, ups=8.33, wpb=3555.8, bsz=143.8, num_updates=41900, lr=0.000218478, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.5, wall=3045
2023-03-11 06:06:30 | INFO | train_inner | epoch 039:    125 / 1102 loss=3.232, nll_loss=1.636, ppl=3.11, wps=54643.2, ups=15.28, wpb=3575.6, bsz=148.8, num_updates=42000, lr=0.000218218, gnorm=1.03, loss_scale=32, train_wall=6, gb_free=13.6, wall=3051
2023-03-11 06:06:36 | INFO | train_inner | epoch 039:    225 / 1102 loss=3.274, nll_loss=1.682, ppl=3.21, wps=54982, ups=15.41, wpb=3568.7, bsz=140.1, num_updates=42100, lr=0.000217959, gnorm=1.061, loss_scale=32, train_wall=6, gb_free=13.5, wall=3058
2023-03-11 06:06:43 | INFO | train_inner | epoch 039:    325 / 1102 loss=3.246, nll_loss=1.652, ppl=3.14, wps=56421.7, ups=15.59, wpb=3618.5, bsz=155.3, num_updates=42200, lr=0.0002177, gnorm=1.013, loss_scale=32, train_wall=6, gb_free=13.5, wall=3064
2023-03-11 06:06:49 | INFO | train_inner | epoch 039:    425 / 1102 loss=3.281, nll_loss=1.693, ppl=3.23, wps=56508, ups=15.48, wpb=3650.1, bsz=154.9, num_updates=42300, lr=0.000217443, gnorm=1.024, loss_scale=32, train_wall=6, gb_free=13.6, wall=3071
2023-03-11 06:06:56 | INFO | train_inner | epoch 039:    525 / 1102 loss=3.29, nll_loss=1.702, ppl=3.25, wps=54835.3, ups=15.32, wpb=3579.6, bsz=147.1, num_updates=42400, lr=0.000217186, gnorm=1.061, loss_scale=32, train_wall=6, gb_free=13.6, wall=3077
2023-03-11 06:07:02 | INFO | train_inner | epoch 039:    625 / 1102 loss=3.278, nll_loss=1.689, ppl=3.23, wps=55220.4, ups=15.29, wpb=3611, bsz=161.1, num_updates=42500, lr=0.00021693, gnorm=1.026, loss_scale=32, train_wall=6, gb_free=13.6, wall=3084
2023-03-11 06:07:09 | INFO | train_inner | epoch 039:    725 / 1102 loss=3.328, nll_loss=1.744, ppl=3.35, wps=54622.1, ups=15.36, wpb=3556.6, bsz=132.3, num_updates=42600, lr=0.000216676, gnorm=1.054, loss_scale=32, train_wall=6, gb_free=13.6, wall=3090
2023-03-11 06:07:15 | INFO | train_inner | epoch 039:    825 / 1102 loss=3.31, nll_loss=1.725, ppl=3.31, wps=55023.6, ups=15.09, wpb=3647.5, bsz=146.2, num_updates=42700, lr=0.000216422, gnorm=1.041, loss_scale=32, train_wall=6, gb_free=13.6, wall=3097
2023-03-11 06:07:22 | INFO | train_inner | epoch 039:    925 / 1102 loss=3.329, nll_loss=1.747, ppl=3.36, wps=53785, ups=15.25, wpb=3526, bsz=137.9, num_updates=42800, lr=0.000216169, gnorm=1.079, loss_scale=32, train_wall=6, gb_free=13.5, wall=3104
2023-03-11 06:07:29 | INFO | train_inner | epoch 039:   1025 / 1102 loss=3.343, nll_loss=1.762, ppl=3.39, wps=53675.5, ups=15.22, wpb=3525.8, bsz=127, num_updates=42900, lr=0.000215917, gnorm=1.091, loss_scale=32, train_wall=6, gb_free=13.6, wall=3110
2023-03-11 06:07:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:07:35 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.982 | nll_loss 2.365 | ppl 5.15 | wps 142258 | wpb 2790.1 | bsz 113.8 | num_updates 42977 | best_loss 3.962
2023-03-11 06:07:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 42977 updates
2023-03-11 06:07:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:07:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:07:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 39 @ 42977 updates, score 3.982) (writing took 3.879783997999766 seconds)
2023-03-11 06:07:39 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2023-03-11 06:07:39 | INFO | train | epoch 039 | loss 3.291 | nll_loss 1.703 | ppl 3.26 | wps 51078.1 | ups 14.26 | wpb 3581.5 | bsz 145.4 | num_updates 42977 | lr 0.000215723 | gnorm 1.048 | loss_scale 32 | train_wall 70 | gb_free 13.5 | wall 3120
2023-03-11 06:07:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:07:39 | INFO | fairseq.trainer | begin training epoch 40
2023-03-11 06:07:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:07:40 | INFO | train_inner | epoch 040:     23 / 1102 loss=3.294, nll_loss=1.707, ppl=3.26, wps=29707.7, ups=8.41, wpb=3530.4, bsz=145.8, num_updates=43000, lr=0.000215666, gnorm=1.056, loss_scale=32, train_wall=6, gb_free=13.6, wall=3122
2023-03-11 06:07:47 | INFO | train_inner | epoch 040:    123 / 1102 loss=3.236, nll_loss=1.637, ppl=3.11, wps=54347.4, ups=15.34, wpb=3542.3, bsz=130.7, num_updates=43100, lr=0.000215415, gnorm=1.051, loss_scale=32, train_wall=6, gb_free=13.6, wall=3129
2023-03-11 06:07:54 | INFO | train_inner | epoch 040:    223 / 1102 loss=3.268, nll_loss=1.676, ppl=3.19, wps=54525.4, ups=15.16, wpb=3597.1, bsz=132.6, num_updates=43200, lr=0.000215166, gnorm=1.04, loss_scale=32, train_wall=6, gb_free=13.5, wall=3135
2023-03-11 06:08:00 | INFO | train_inner | epoch 040:    323 / 1102 loss=3.256, nll_loss=1.662, ppl=3.17, wps=55315.2, ups=15.26, wpb=3623.8, bsz=144.9, num_updates=43300, lr=0.000214917, gnorm=1.05, loss_scale=32, train_wall=6, gb_free=13.6, wall=3142
2023-03-11 06:08:07 | INFO | train_inner | epoch 040:    423 / 1102 loss=3.288, nll_loss=1.699, ppl=3.25, wps=54389, ups=15.24, wpb=3567.7, bsz=141.2, num_updates=43400, lr=0.000214669, gnorm=1.079, loss_scale=32, train_wall=6, gb_free=13.5, wall=3148
2023-03-11 06:08:13 | INFO | train_inner | epoch 040:    523 / 1102 loss=3.281, nll_loss=1.69, ppl=3.23, wps=54735.3, ups=15.37, wpb=3560.3, bsz=145.2, num_updates=43500, lr=0.000214423, gnorm=1.055, loss_scale=32, train_wall=6, gb_free=13.4, wall=3155
2023-03-11 06:08:20 | INFO | train_inner | epoch 040:    623 / 1102 loss=3.29, nll_loss=1.702, ppl=3.25, wps=54706.5, ups=15.31, wpb=3572.1, bsz=147.4, num_updates=43600, lr=0.000214176, gnorm=1.062, loss_scale=32, train_wall=6, gb_free=13.6, wall=3161
2023-03-11 06:08:26 | INFO | train_inner | epoch 040:    723 / 1102 loss=3.287, nll_loss=1.699, ppl=3.25, wps=54639.6, ups=15.27, wpb=3577.7, bsz=146.6, num_updates=43700, lr=0.000213931, gnorm=1.057, loss_scale=32, train_wall=6, gb_free=13.5, wall=3168
2023-03-11 06:08:33 | INFO | train_inner | epoch 040:    823 / 1102 loss=3.316, nll_loss=1.731, ppl=3.32, wps=53960.9, ups=15.31, wpb=3523.7, bsz=141.4, num_updates=43800, lr=0.000213687, gnorm=1.081, loss_scale=32, train_wall=6, gb_free=13.5, wall=3174
2023-03-11 06:08:39 | INFO | train_inner | epoch 040:    923 / 1102 loss=3.265, nll_loss=1.676, ppl=3.19, wps=56892, ups=15.39, wpb=3696.6, bsz=163.9, num_updates=43900, lr=0.000213443, gnorm=1, loss_scale=32, train_wall=6, gb_free=13.5, wall=3181
2023-03-11 06:08:46 | INFO | train_inner | epoch 040:   1023 / 1102 loss=3.292, nll_loss=1.707, ppl=3.27, wps=56163.1, ups=15.55, wpb=3612.3, bsz=163.4, num_updates=44000, lr=0.000213201, gnorm=1.065, loss_scale=32, train_wall=6, gb_free=13.6, wall=3187
2023-03-11 06:08:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:08:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.975 | nll_loss 2.363 | ppl 5.14 | wps 139426 | wpb 2790.1 | bsz 113.8 | num_updates 44079 | best_loss 3.962
2023-03-11 06:08:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 44079 updates
2023-03-11 06:08:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:08:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:08:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 40 @ 44079 updates, score 3.975) (writing took 3.911837703999481 seconds)
2023-03-11 06:08:56 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2023-03-11 06:08:56 | INFO | train | epoch 040 | loss 3.279 | nll_loss 1.689 | ppl 3.22 | wps 51047 | ups 14.25 | wpb 3581.5 | bsz 145.4 | num_updates 44079 | lr 0.00021301 | gnorm 1.055 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 3198
2023-03-11 06:08:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:08:56 | INFO | fairseq.trainer | begin training epoch 41
2023-03-11 06:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:08:58 | INFO | train_inner | epoch 041:     21 / 1102 loss=3.288, nll_loss=1.701, ppl=3.25, wps=29688.3, ups=8.41, wpb=3530.4, bsz=144.2, num_updates=44100, lr=0.000212959, gnorm=1.066, loss_scale=32, train_wall=6, gb_free=13.6, wall=3199
2023-03-11 06:09:04 | INFO | train_inner | epoch 041:    121 / 1102 loss=3.222, nll_loss=1.623, ppl=3.08, wps=55034.4, ups=15.25, wpb=3608.3, bsz=146.8, num_updates=44200, lr=0.000212718, gnorm=1.029, loss_scale=32, train_wall=6, gb_free=13.6, wall=3206
2023-03-11 06:09:11 | INFO | train_inner | epoch 041:    221 / 1102 loss=3.231, nll_loss=1.634, ppl=3.1, wps=55619.8, ups=15.35, wpb=3624.5, bsz=156.9, num_updates=44300, lr=0.000212478, gnorm=1.047, loss_scale=32, train_wall=6, gb_free=13.6, wall=3212
2023-03-11 06:09:17 | INFO | train_inner | epoch 041:    321 / 1102 loss=3.243, nll_loss=1.648, ppl=3.13, wps=54308.1, ups=15.08, wpb=3602, bsz=144.3, num_updates=44400, lr=0.000212238, gnorm=1.035, loss_scale=32, train_wall=6, gb_free=13.7, wall=3219
2023-03-11 06:09:24 | INFO | train_inner | epoch 041:    421 / 1102 loss=3.241, nll_loss=1.647, ppl=3.13, wps=54540.8, ups=15.14, wpb=3602.3, bsz=154.2, num_updates=44500, lr=0.000212, gnorm=1.024, loss_scale=32, train_wall=6, gb_free=13.6, wall=3226
2023-03-11 06:09:31 | INFO | train_inner | epoch 041:    521 / 1102 loss=3.272, nll_loss=1.68, ppl=3.2, wps=52724.2, ups=15.19, wpb=3471.6, bsz=138.2, num_updates=44600, lr=0.000211762, gnorm=1.081, loss_scale=32, train_wall=6, gb_free=13.6, wall=3232
2023-03-11 06:09:37 | INFO | train_inner | epoch 041:    621 / 1102 loss=3.264, nll_loss=1.673, ppl=3.19, wps=55346.7, ups=15.17, wpb=3648, bsz=153.4, num_updates=44700, lr=0.000211525, gnorm=1.036, loss_scale=32, train_wall=6, gb_free=13.6, wall=3239
2023-03-11 06:09:44 | INFO | train_inner | epoch 041:    721 / 1102 loss=3.292, nll_loss=1.704, ppl=3.26, wps=54165.7, ups=15.15, wpb=3575.1, bsz=144.2, num_updates=44800, lr=0.000211289, gnorm=1.061, loss_scale=32, train_wall=6, gb_free=13.5, wall=3245
2023-03-11 06:09:50 | INFO | train_inner | epoch 041:    821 / 1102 loss=3.278, nll_loss=1.688, ppl=3.22, wps=53419.3, ups=15.17, wpb=3520.3, bsz=144.4, num_updates=44900, lr=0.000211053, gnorm=1.08, loss_scale=32, train_wall=6, gb_free=13.7, wall=3252
2023-03-11 06:09:57 | INFO | train_inner | epoch 041:    921 / 1102 loss=3.32, nll_loss=1.735, ppl=3.33, wps=54562.5, ups=15.36, wpb=3551.3, bsz=126.7, num_updates=45000, lr=0.000210819, gnorm=1.083, loss_scale=32, train_wall=6, gb_free=13.5, wall=3258
2023-03-11 06:10:03 | INFO | train_inner | epoch 041:   1021 / 1102 loss=3.305, nll_loss=1.717, ppl=3.29, wps=54763.2, ups=15.27, wpb=3587.4, bsz=135.4, num_updates=45100, lr=0.000210585, gnorm=1.069, loss_scale=32, train_wall=6, gb_free=13.5, wall=3265
2023-03-11 06:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:10:10 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.983 | nll_loss 2.376 | ppl 5.19 | wps 139655 | wpb 2790.1 | bsz 113.8 | num_updates 45181 | best_loss 3.962
2023-03-11 06:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 45181 updates
2023-03-11 06:10:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:10:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 41 @ 45181 updates, score 3.983) (writing took 3.8163786929981143 seconds)
2023-03-11 06:10:14 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2023-03-11 06:10:14 | INFO | train | epoch 041 | loss 3.266 | nll_loss 1.674 | ppl 3.19 | wps 50761.5 | ups 14.17 | wpb 3581.5 | bsz 145.4 | num_updates 45181 | lr 0.000210396 | gnorm 1.054 | loss_scale 32 | train_wall 71 | gb_free 13.6 | wall 3275
2023-03-11 06:10:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:10:14 | INFO | fairseq.trainer | begin training epoch 42
2023-03-11 06:10:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:10:15 | INFO | train_inner | epoch 042:     19 / 1102 loss=3.254, nll_loss=1.662, ppl=3.16, wps=30540.5, ups=8.43, wpb=3624.6, bsz=154, num_updates=45200, lr=0.000210352, gnorm=1.036, loss_scale=32, train_wall=6, gb_free=13.5, wall=3277
2023-03-11 06:10:22 | INFO | train_inner | epoch 042:    119 / 1102 loss=3.208, nll_loss=1.607, ppl=3.05, wps=56141, ups=15.51, wpb=3619.8, bsz=141.3, num_updates=45300, lr=0.000210119, gnorm=1.016, loss_scale=32, train_wall=6, gb_free=13.5, wall=3283
2023-03-11 06:10:28 | INFO | train_inner | epoch 042:    219 / 1102 loss=3.221, nll_loss=1.623, ppl=3.08, wps=55304, ups=15.43, wpb=3583.6, bsz=144, num_updates=45400, lr=0.000209888, gnorm=1.058, loss_scale=32, train_wall=6, gb_free=13.5, wall=3290
2023-03-11 06:10:35 | INFO | train_inner | epoch 042:    319 / 1102 loss=3.229, nll_loss=1.632, ppl=3.1, wps=56262.3, ups=15.54, wpb=3620.4, bsz=150, num_updates=45500, lr=0.000209657, gnorm=1.029, loss_scale=32, train_wall=6, gb_free=13.6, wall=3296
2023-03-11 06:10:41 | INFO | train_inner | epoch 042:    419 / 1102 loss=3.25, nll_loss=1.653, ppl=3.15, wps=52967.1, ups=15.33, wpb=3455.4, bsz=125.4, num_updates=45600, lr=0.000209427, gnorm=1.095, loss_scale=32, train_wall=6, gb_free=13.5, wall=3303
2023-03-11 06:10:48 | INFO | train_inner | epoch 042:    519 / 1102 loss=3.257, nll_loss=1.664, ppl=3.17, wps=56060.3, ups=15.56, wpb=3603.8, bsz=144.4, num_updates=45700, lr=0.000209198, gnorm=1.043, loss_scale=32, train_wall=6, gb_free=13.6, wall=3309
2023-03-11 06:10:54 | INFO | train_inner | epoch 042:    619 / 1102 loss=3.278, nll_loss=1.686, ppl=3.22, wps=56054.6, ups=15.38, wpb=3644.9, bsz=137.2, num_updates=45800, lr=0.000208969, gnorm=1.066, loss_scale=32, train_wall=6, gb_free=13.5, wall=3316
2023-03-11 06:11:01 | INFO | train_inner | epoch 042:    719 / 1102 loss=3.275, nll_loss=1.686, ppl=3.22, wps=55575.6, ups=15.4, wpb=3608.9, bsz=151.3, num_updates=45900, lr=0.000208741, gnorm=1.053, loss_scale=32, train_wall=6, gb_free=13.5, wall=3322
2023-03-11 06:11:07 | INFO | train_inner | epoch 042:    819 / 1102 loss=3.248, nll_loss=1.655, ppl=3.15, wps=54592.4, ups=15.39, wpb=3546.6, bsz=156.8, num_updates=46000, lr=0.000208514, gnorm=1.056, loss_scale=32, train_wall=6, gb_free=13.6, wall=3329
2023-03-11 06:11:14 | INFO | train_inner | epoch 042:    919 / 1102 loss=3.263, nll_loss=1.674, ppl=3.19, wps=55988.9, ups=15.41, wpb=3632.3, bsz=160.1, num_updates=46100, lr=0.000208288, gnorm=1.037, loss_scale=32, train_wall=6, gb_free=13.7, wall=3335
2023-03-11 06:11:20 | INFO | train_inner | epoch 042:   1019 / 1102 loss=3.278, nll_loss=1.689, ppl=3.22, wps=53740.8, ups=15.5, wpb=3466.9, bsz=144.1, num_updates=46200, lr=0.000208063, gnorm=1.113, loss_scale=32, train_wall=6, gb_free=13.7, wall=3342
2023-03-11 06:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:11:27 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.976 | nll_loss 2.365 | ppl 5.15 | wps 141648 | wpb 2790.1 | bsz 113.8 | num_updates 46283 | best_loss 3.962
2023-03-11 06:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 46283 updates
2023-03-11 06:11:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:11:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:11:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 42 @ 46283 updates, score 3.976) (writing took 3.886062044999562 seconds)
2023-03-11 06:11:31 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2023-03-11 06:11:31 | INFO | train | epoch 042 | loss 3.252 | nll_loss 1.658 | ppl 3.16 | wps 51429.5 | ups 14.36 | wpb 3581.5 | bsz 145.4 | num_updates 46283 | lr 0.000207876 | gnorm 1.057 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 3352
2023-03-11 06:11:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:11:31 | INFO | fairseq.trainer | begin training epoch 43
2023-03-11 06:11:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:11:32 | INFO | train_inner | epoch 043:     17 / 1102 loss=3.261, nll_loss=1.67, ppl=3.18, wps=30702.6, ups=8.45, wpb=3631.9, bsz=148.9, num_updates=46300, lr=0.000207838, gnorm=1.056, loss_scale=32, train_wall=6, gb_free=13.4, wall=3353
2023-03-11 06:11:38 | INFO | train_inner | epoch 043:    117 / 1102 loss=3.138, nll_loss=1.53, ppl=2.89, wps=54802.7, ups=15.45, wpb=3547.7, bsz=176.7, num_updates=46400, lr=0.000207614, gnorm=1.023, loss_scale=32, train_wall=6, gb_free=13.6, wall=3360
2023-03-11 06:11:45 | INFO | train_inner | epoch 043:    217 / 1102 loss=3.2, nll_loss=1.599, ppl=3.03, wps=54861, ups=15.41, wpb=3559.7, bsz=146.4, num_updates=46500, lr=0.00020739, gnorm=1.046, loss_scale=32, train_wall=6, gb_free=13.9, wall=3366
2023-03-11 06:11:51 | INFO | train_inner | epoch 043:    317 / 1102 loss=3.206, nll_loss=1.605, ppl=3.04, wps=54892.7, ups=15.46, wpb=3550.4, bsz=141.1, num_updates=46600, lr=0.000207168, gnorm=1.044, loss_scale=32, train_wall=6, gb_free=13.5, wall=3373
2023-03-11 06:11:58 | INFO | train_inner | epoch 043:    417 / 1102 loss=3.267, nll_loss=1.672, ppl=3.19, wps=55023.3, ups=15.38, wpb=3576.6, bsz=122.5, num_updates=46700, lr=0.000206946, gnorm=1.095, loss_scale=32, train_wall=6, gb_free=13.5, wall=3379
2023-03-11 06:12:04 | INFO | train_inner | epoch 043:    517 / 1102 loss=3.235, nll_loss=1.638, ppl=3.11, wps=54305.4, ups=15.14, wpb=3587.5, bsz=145.9, num_updates=46800, lr=0.000206725, gnorm=1.064, loss_scale=32, train_wall=6, gb_free=13.5, wall=3386
2023-03-11 06:12:11 | INFO | train_inner | epoch 043:    617 / 1102 loss=3.242, nll_loss=1.647, ppl=3.13, wps=55873, ups=15.54, wpb=3595.9, bsz=153.3, num_updates=46900, lr=0.000206504, gnorm=1.05, loss_scale=32, train_wall=6, gb_free=13.6, wall=3392
2023-03-11 06:12:17 | INFO | train_inner | epoch 043:    717 / 1102 loss=3.244, nll_loss=1.648, ppl=3.13, wps=54371.6, ups=15.39, wpb=3533.7, bsz=144.9, num_updates=47000, lr=0.000206284, gnorm=1.079, loss_scale=32, train_wall=6, gb_free=13.6, wall=3399
2023-03-11 06:12:24 | INFO | train_inner | epoch 043:    817 / 1102 loss=3.294, nll_loss=1.705, ppl=3.26, wps=54720.4, ups=15.34, wpb=3566.1, bsz=131.4, num_updates=47100, lr=0.000206065, gnorm=1.086, loss_scale=32, train_wall=6, gb_free=13.6, wall=3405
2023-03-11 06:12:30 | INFO | train_inner | epoch 043:    917 / 1102 loss=3.272, nll_loss=1.683, ppl=3.21, wps=56110.9, ups=15.45, wpb=3631.4, bsz=148.6, num_updates=47200, lr=0.000205847, gnorm=1.05, loss_scale=32, train_wall=6, gb_free=13.4, wall=3412
2023-03-11 06:12:37 | INFO | train_inner | epoch 043:   1017 / 1102 loss=3.292, nll_loss=1.702, ppl=3.25, wps=54320.1, ups=15.23, wpb=3567.8, bsz=131.7, num_updates=47300, lr=0.000205629, gnorm=1.099, loss_scale=32, train_wall=6, gb_free=13.5, wall=3418
2023-03-11 06:12:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:12:44 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.989 | nll_loss 2.377 | ppl 5.2 | wps 142202 | wpb 2790.1 | bsz 113.8 | num_updates 47385 | best_loss 3.962
2023-03-11 06:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 47385 updates
2023-03-11 06:12:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:12:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:12:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 43 @ 47385 updates, score 3.989) (writing took 3.8110264460010512 seconds)
2023-03-11 06:12:48 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2023-03-11 06:12:48 | INFO | train | epoch 043 | loss 3.24 | nll_loss 1.644 | ppl 3.13 | wps 51290.4 | ups 14.32 | wpb 3581.5 | bsz 145.4 | num_updates 47385 | lr 0.000205445 | gnorm 1.061 | loss_scale 32 | train_wall 70 | gb_free 13.4 | wall 3429
2023-03-11 06:12:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:12:48 | INFO | fairseq.trainer | begin training epoch 44
2023-03-11 06:12:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:12:49 | INFO | train_inner | epoch 044:     15 / 1102 loss=3.243, nll_loss=1.65, ppl=3.14, wps=30924.2, ups=8.49, wpb=3643.6, bsz=158.2, num_updates=47400, lr=0.000205412, gnorm=1.04, loss_scale=32, train_wall=6, gb_free=13.5, wall=3430
2023-03-11 06:12:55 | INFO | train_inner | epoch 044:    115 / 1102 loss=3.2, nll_loss=1.596, ppl=3.02, wps=55234.1, ups=15.34, wpb=3600.3, bsz=131.4, num_updates=47500, lr=0.000205196, gnorm=1.071, loss_scale=32, train_wall=6, gb_free=13.5, wall=3437
2023-03-11 06:13:02 | INFO | train_inner | epoch 044:    215 / 1102 loss=3.192, nll_loss=1.588, ppl=3.01, wps=55186.5, ups=15.24, wpb=3621.7, bsz=141.2, num_updates=47600, lr=0.00020498, gnorm=1.045, loss_scale=32, train_wall=6, gb_free=13.7, wall=3443
2023-03-11 06:13:08 | INFO | train_inner | epoch 044:    315 / 1102 loss=3.199, nll_loss=1.599, ppl=3.03, wps=55003.2, ups=15.18, wpb=3623.6, bsz=162.2, num_updates=47700, lr=0.000204765, gnorm=1.04, loss_scale=32, train_wall=6, gb_free=13.4, wall=3450
2023-03-11 06:13:15 | INFO | train_inner | epoch 044:    415 / 1102 loss=3.204, nll_loss=1.603, ppl=3.04, wps=54155.8, ups=15.5, wpb=3494.5, bsz=155.3, num_updates=47800, lr=0.000204551, gnorm=1.06, loss_scale=32, train_wall=6, gb_free=13.5, wall=3456
2023-03-11 06:13:21 | INFO | train_inner | epoch 044:    515 / 1102 loss=3.233, nll_loss=1.637, ppl=3.11, wps=54632.8, ups=15.36, wpb=3557.8, bsz=149.4, num_updates=47900, lr=0.000204337, gnorm=1.064, loss_scale=32, train_wall=6, gb_free=13.7, wall=3463
2023-03-11 06:13:28 | INFO | train_inner | epoch 044:    615 / 1102 loss=3.205, nll_loss=1.605, ppl=3.04, wps=54158.4, ups=15.16, wpb=3572, bsz=157, num_updates=48000, lr=0.000204124, gnorm=1.048, loss_scale=32, train_wall=6, gb_free=13.4, wall=3469
2023-03-11 06:13:35 | INFO | train_inner | epoch 044:    715 / 1102 loss=3.247, nll_loss=1.651, ppl=3.14, wps=53846.5, ups=15.18, wpb=3546.8, bsz=133.8, num_updates=48100, lr=0.000203912, gnorm=1.079, loss_scale=32, train_wall=6, gb_free=13.5, wall=3476
2023-03-11 06:13:41 | INFO | train_inner | epoch 044:    815 / 1102 loss=3.246, nll_loss=1.651, ppl=3.14, wps=54437.2, ups=15.3, wpb=3557.4, bsz=141.5, num_updates=48200, lr=0.0002037, gnorm=1.075, loss_scale=32, train_wall=6, gb_free=13.4, wall=3483
2023-03-11 06:13:48 | INFO | train_inner | epoch 044:    915 / 1102 loss=3.264, nll_loss=1.672, ppl=3.19, wps=55255.3, ups=15.39, wpb=3590.9, bsz=135, num_updates=48300, lr=0.000203489, gnorm=1.095, loss_scale=32, train_wall=6, gb_free=13.6, wall=3489
2023-03-11 06:13:54 | INFO | train_inner | epoch 044:   1015 / 1102 loss=3.276, nll_loss=1.686, ppl=3.22, wps=55406.7, ups=15.35, wpb=3609.2, bsz=142.2, num_updates=48400, lr=0.000203279, gnorm=1.08, loss_scale=32, train_wall=6, gb_free=13.6, wall=3496
2023-03-11 06:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:14:01 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.988 | nll_loss 2.386 | ppl 5.23 | wps 142081 | wpb 2790.1 | bsz 113.8 | num_updates 48487 | best_loss 3.962
2023-03-11 06:14:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 48487 updates
2023-03-11 06:14:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 44 @ 48487 updates, score 3.988) (writing took 3.865259829999559 seconds)
2023-03-11 06:14:05 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2023-03-11 06:14:05 | INFO | train | epoch 044 | loss 3.228 | nll_loss 1.631 | ppl 3.1 | wps 51009.3 | ups 14.24 | wpb 3581.5 | bsz 145.4 | num_updates 48487 | lr 0.000203096 | gnorm 1.065 | loss_scale 32 | train_wall 70 | gb_free 13.6 | wall 3507
2023-03-11 06:14:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:14:05 | INFO | fairseq.trainer | begin training epoch 45
2023-03-11 06:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:14:06 | INFO | train_inner | epoch 045:     13 / 1102 loss=3.257, nll_loss=1.664, ppl=3.17, wps=30472.2, ups=8.44, wpb=3612.6, bsz=144.2, num_updates=48500, lr=0.000203069, gnorm=1.057, loss_scale=32, train_wall=6, gb_free=13.5, wall=3508
2023-03-11 06:14:12 | INFO | train_inner | epoch 045:    113 / 1102 loss=3.139, nll_loss=1.529, ppl=2.89, wps=55082.3, ups=15.39, wpb=3578.2, bsz=154.7, num_updates=48600, lr=0.00020286, gnorm=1.03, loss_scale=32, train_wall=6, gb_free=13.6, wall=3514
2023-03-11 06:14:19 | INFO | train_inner | epoch 045:    213 / 1102 loss=3.169, nll_loss=1.562, ppl=2.95, wps=54222.6, ups=15.24, wpb=3557.4, bsz=152.6, num_updates=48700, lr=0.000202652, gnorm=1.055, loss_scale=32, train_wall=6, gb_free=13.6, wall=3521
2023-03-11 06:14:26 | INFO | train_inner | epoch 045:    313 / 1102 loss=3.201, nll_loss=1.599, ppl=3.03, wps=54714.9, ups=15.35, wpb=3565.3, bsz=142.8, num_updates=48800, lr=0.000202444, gnorm=1.078, loss_scale=32, train_wall=6, gb_free=13.5, wall=3527
2023-03-11 06:14:32 | INFO | train_inner | epoch 045:    413 / 1102 loss=3.194, nll_loss=1.592, ppl=3.02, wps=55594.3, ups=15.47, wpb=3594.4, bsz=154.1, num_updates=48900, lr=0.000202237, gnorm=1.047, loss_scale=32, train_wall=6, gb_free=13.5, wall=3534
2023-03-11 06:14:38 | INFO | train_inner | epoch 045:    513 / 1102 loss=3.186, nll_loss=1.582, ppl=2.99, wps=55629.7, ups=15.34, wpb=3626.1, bsz=155.9, num_updates=49000, lr=0.000202031, gnorm=1.038, loss_scale=32, train_wall=6, gb_free=13.5, wall=3540
2023-03-11 06:14:45 | INFO | train_inner | epoch 045:    613 / 1102 loss=3.23, nll_loss=1.633, ppl=3.1, wps=54683.1, ups=15.11, wpb=3617.9, bsz=147.2, num_updates=49100, lr=0.000201825, gnorm=1.078, loss_scale=32, train_wall=6, gb_free=13.5, wall=3547
2023-03-11 06:14:52 | INFO | train_inner | epoch 045:    713 / 1102 loss=3.211, nll_loss=1.611, ppl=3.05, wps=54510, ups=15.33, wpb=3555.2, bsz=150.6, num_updates=49200, lr=0.000201619, gnorm=1.071, loss_scale=32, train_wall=6, gb_free=13.5, wall=3553
2023-03-11 06:14:58 | INFO | train_inner | epoch 045:    813 / 1102 loss=3.245, nll_loss=1.651, ppl=3.14, wps=54522.1, ups=15.24, wpb=3578.4, bsz=146.3, num_updates=49300, lr=0.000201415, gnorm=1.085, loss_scale=32, train_wall=6, gb_free=13.6, wall=3560
2023-03-11 06:15:05 | INFO | train_inner | epoch 045:    913 / 1102 loss=3.284, nll_loss=1.694, ppl=3.23, wps=55237.8, ups=15.26, wpb=3620.4, bsz=124.5, num_updates=49400, lr=0.000201211, gnorm=1.091, loss_scale=32, train_wall=6, gb_free=13.5, wall=3566
2023-03-11 06:15:11 | INFO | train_inner | epoch 045:   1013 / 1102 loss=3.263, nll_loss=1.671, ppl=3.18, wps=54825.8, ups=15.44, wpb=3549.9, bsz=137.3, num_updates=49500, lr=0.000201008, gnorm=1.101, loss_scale=32, train_wall=6, gb_free=13.6, wall=3573
2023-03-11 06:15:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:15:18 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.984 | nll_loss 2.376 | ppl 5.19 | wps 135494 | wpb 2790.1 | bsz 113.8 | num_updates 49589 | best_loss 3.962
2023-03-11 06:15:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 49589 updates
2023-03-11 06:15:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:15:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:15:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 45 @ 49589 updates, score 3.984) (writing took 3.794833476000349 seconds)
2023-03-11 06:15:22 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2023-03-11 06:15:22 | INFO | train | epoch 045 | loss 3.216 | nll_loss 1.617 | ppl 3.07 | wps 51052.3 | ups 14.25 | wpb 3581.5 | bsz 145.4 | num_updates 49589 | lr 0.000200827 | gnorm 1.069 | loss_scale 32 | train_wall 70 | gb_free 13.5 | wall 3584
2023-03-11 06:15:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-03-11 06:15:22 | INFO | fairseq.trainer | begin training epoch 46
2023-03-11 06:15:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-03-11 06:15:23 | INFO | train_inner | epoch 046:     11 / 1102 loss=3.254, nll_loss=1.66, ppl=3.16, wps=30086.4, ups=8.42, wpb=3572.5, bsz=135, num_updates=49600, lr=0.000200805, gnorm=1.079, loss_scale=32, train_wall=6, gb_free=13.6, wall=3585
2023-03-11 06:15:30 | INFO | train_inner | epoch 046:    111 / 1102 loss=3.16, nll_loss=1.551, ppl=2.93, wps=56169.2, ups=15.43, wpb=3639.8, bsz=146.8, num_updates=49700, lr=0.000200603, gnorm=1.028, loss_scale=32, train_wall=6, gb_free=13.5, wall=3591
2023-03-11 06:15:36 | INFO | train_inner | epoch 046:    211 / 1102 loss=3.18, nll_loss=1.574, ppl=2.98, wps=55180.4, ups=15.44, wpb=3574.3, bsz=137.8, num_updates=49800, lr=0.000200401, gnorm=1.064, loss_scale=32, train_wall=6, gb_free=13.6, wall=3598
2023-03-11 06:15:43 | INFO | train_inner | epoch 046:    311 / 1102 loss=3.169, nll_loss=1.561, ppl=2.95, wps=55087.1, ups=15.48, wpb=3558.3, bsz=139.2, num_updates=49900, lr=0.0002002, gnorm=1.068, loss_scale=32, train_wall=6, gb_free=13.5, wall=3604
2023-03-11 06:15:49 | INFO | train_inner | epoch 046:    411 / 1102 loss=3.206, nll_loss=1.603, ppl=3.04, wps=53798.2, ups=15.32, wpb=3510.8, bsz=137.1, num_updates=50000, lr=0.0002, gnorm=1.107, loss_scale=32, train_wall=6, gb_free=13.5, wall=3611
2023-03-11 06:15:49 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-03-11 06:15:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-03-11 06:15:50 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.001 | nll_loss 2.394 | ppl 5.26 | wps 137539 | wpb 2790.1 | bsz 113.8 | num_updates 50000 | best_loss 3.962
2023-03-11 06:15:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 50000 updates
2023-03-11 06:15:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:15:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ymanne/Intra-Distillation/Machine_Translation/models/de-3-5-false/checkpoint_last.pt
2023-03-11 06:15:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./models/de-3-5-false/checkpoint_last.pt (epoch 46 @ 50000 updates, score 4.001) (writing took 3.8991065089976473 seconds)
2023-03-11 06:15:54 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2023-03-11 06:15:54 | INFO | train | epoch 046 | loss 3.177 | nll_loss 1.57 | ppl 2.97 | wps 45717.1 | ups 12.8 | wpb 3572.7 | bsz 140.3 | num_updates 50000 | lr 0.0002 | gnorm 1.065 | loss_scale 32 | train_wall 26 | gb_free 13.5 | wall 3616
2023-03-11 06:15:54 | INFO | fairseq_cli.train | done training in 3615.9 seconds
