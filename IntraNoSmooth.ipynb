{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Hh-IoFB9ZH-3tFw673dx3ylByB_9xsDS","timestamp":1678514306840}],"authorship_tag":"ABX9TyPp6yUzRDa0G8FkOwGdfR5y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["This file trains the intra-distillation teacher and student models without \n","label smoothing."],"metadata":{"id":"nhN6dTlJp7ED"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"mN3hdD7Yp4zz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import numpy as np\n","device = torch.device('cuda')"],"metadata":{"id":"hIbBDlSKWz-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["K = 3\n","p = 5\n","q = 10\n","alpha = 5\n","N = 4000\n","num_classes = 37\n","batch_size = 4 # Gradients are updated every 60 samples, but we run 4 samples at a time to save memory.\n","iter_batch = 15\n","epochs = 80\n","kd = 1\n","base_lr = 0.00002\n","weight_decay = 0.001\n","dropout_p = 0.1\n","name = 'intranosmooth'"],"metadata":{"id":"n4sbu1ukWoKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1\n","data = torchvision.datasets.OxfordIIITPet('OxfordIIITPet', transform=weights.transforms(), download=True)\n","train, val = torch.utils.data.random_split(data, [3000, 680], generator=torch.Generator().manual_seed(42))\n","train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"05dQ4-i4xjVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dropout(p):\n","    return nn.Sequential(nn.ReLU(), nn.Dropout(p=p))\n","def set_dropout(model, p):\n","    for name, child in model.named_children():\n","        if isinstance(child, nn.ReLU):\n","            model.relu = dropout(p)\n","        set_dropout(child, p)\n","def make_model(model_name='', p=dropout_p):\n","    model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n","    model.fc = torch.nn.Linear(2048, num_classes, bias=False)\n","    set_dropout(model, p)\n","    model = model.to(device)\n","    if model_name != '':\n","        model.load_state_dict(torch.load(f'/content/gdrive/My Drive/{model_name}'))\n","    model.eval();\n","    return model\n","model = make_model()"],"metadata":{"id":"T2BBcrGlO8zX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def X_loss(distributions, target_mask):\n","    dict_size = distributions.shape[-1]\n","\n","    m = torch.sum(distributions, dim=0) / distributions.shape[0]\n","    m = m.float().view(-1, dict_size)[target_mask]\n","\n","    kl_all = 0\n","    for l in distributions:\n","        l = l.float().view(-1, dict_size)[target_mask]\n","        d = (l-m) * (torch.log(l) - torch.log(m))\n","        kl_all += d.sum()\n","    return kl_all / distributions.shape[0]\n","\n","def _get_alpha(alpha, num_update, max_update, p, q):\n","    if num_update >= max_update / p or alpha <= 1:\n","        return alpha\n","    else:\n","        alpha = torch.tensor([alpha])\n","        gamma = torch.log(1/alpha) / torch.log(torch.tensor([p/q])) # log_(p/q)(1/alpha)\n","        new_alpha = ( p**gamma * alpha * num_update ** gamma) / (max_update ** gamma)\n","        return new_alpha.item()\n","\n","def IntraDistillationLoss(N, p, q, alpha):\n","    cross_entropy = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n","    def loss_function(outputs, true, num_update):\n","        K = outputs.shape[0]\n","        num_classes = outputs.shape[-1]\n","        mask = torch.ones((batch_size, num_classes)).type(torch.bool)\n","        probabilities = torch.softmax(outputs, -1)\n","        intra_loss = X_loss(probabilities, mask)\n","        expanded_true = true.repeat(K)\n","        expanded_outputs = outputs.reshape(-1, num_classes)\n","        likelihood_loss = cross_entropy(expanded_outputs, expanded_true)\n","        adaptive_alpha = _get_alpha(alpha, num_update, N, p, q)\n","        return likelihood_loss + adaptive_alpha * intra_loss, likelihood_loss, intra_loss\n","    return loss_function"],"metadata":{"id":"5mmsrebjWoUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the teacher\n","\n","model = make_model()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n","intra_distillation_loss_fn = IntraDistillationLoss(N, p, q, alpha)\n","val_loss_fn = torch.nn.CrossEntropyLoss()\n","num_update = 1\n","curve = []\n","optimizer.zero_grad()\n","\n","for epoch in range(epochs):\n","    train_likelihood = 0\n","    train_intra = 0\n","    val_likelihood = 0\n","\n","    model.train()\n","    for j, (batch, label) in enumerate(train_loader):\n","        outputs = torch.empty(K, batch_size, num_classes)\n","        batch = batch.to(device)\n","        label = label.to(device)\n","        outputs = outputs.to(device)\n","        for i in range(K):\n","            outputs[i] = model(batch)\n","        loss, likelihood_loss, intra_loss = intra_distillation_loss_fn(outputs, label, num_update)\n","        (loss/iter_batch).backward()\n","        train_likelihood += float(likelihood_loss)/iter_batch\n","        train_intra += float(intra_loss)/iter_batch\n","        if (j+1) % iter_batch == 0:\n","            for g in optimizer.param_groups:\n","                if num_update < 50:\n","                    g['lr'] = base_lr * num_update\n","                else:\n","                    g['lr'] = 0.001 / np.sqrt(num_update-49)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            num_update += 1\n","\n","    model.eval()\n","    for batch, label in val_loader:\n","        batch = batch.to(device)\n","        label = label.to(device)\n","        output = model(batch)\n","        loss = val_loss_fn(output, label)\n","        val_likelihood += float(loss)\n","\n","    stat = [train_likelihood/50, train_intra/50, val_likelihood/170]\n","    curve.append(stat)\n","\n","    np.savetxt(f'/content/gdrive/My Drive/{name}_curve.txt', np.array(curve))\n","    torch.save(model.state_dict(), f'/content/gdrive/My Drive/{name}_{epoch}.mdl')"],"metadata":{"id":"YHZhgbE4xnWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the teacher predictions for self-distillation\n","\n","def teacher_loss_fn(outputs, teacher_outputs):\n","    p = torch.softmax(teacher_outputs, dim=-1)\n","    q = torch.softmax(outputs, dim=-1)\n","    return - torch.mean(torch.sum(p * torch.log(q), dim=-1))\n","\n","model = make_model(f'{name}_{epochs}.mdl')\n","next_data = []\n","\n","for img, label in train:\n","    output = model(img.to(device)[None,...]).to('cpu').detach()[0]\n","    next_data.append((img, output, label))\n","next_train_dataloader = torch.utils.data.DataLoader(next_data, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"SIYlyKil4W-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the student\n","\n","model = make_model()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n","intra_distillation_loss_fn = IntraDistillationLoss(N, p, q, alpha)\n","val_loss_fn = torch.nn.CrossEntropyLoss()\n","num_update = 1\n","curve = []\n","optimizer.zero_grad()\n","\n","for epoch in range(epochs):\n","    train_likelihood = 0\n","    train_intra = 0\n","    train_teacher = 0\n","    val_likelihood = 0\n","\n","    model.train()\n","    for j, (img, teacher, label) in enumerate(next_train_dataloader):\n","        outputs = torch.empty(K, batch_size, num_classes)\n","        img = img.to(device)\n","        teacher = teacher.to(device)\n","        label = label.to(device)\n","        outputs = outputs.to(device)\n","        for i in range(K):\n","            outputs[i] = model(img)\n","        intrinsic_loss, likelihood_loss, intra_loss = intra_distillation_loss_fn(outputs, label, num_update)\n","        teacher_loss = teacher_loss_fn(outputs, teacher)\n","        if epoch >= 75:\n","            loss = intrinsic_loss\n","        else:\n","            loss = intrinsic_loss + kd * teacher_loss\n","        (loss/iter_batch).backward()\n","        train_likelihood += float(likelihood_loss)/iter_batch\n","        train_intra += float(intra_loss)/iter_batch\n","        train_teacher += float(teacher_loss)/iter_batch\n","        if (j+1) % iter_batch == 0:\n","            for g in optimizer.param_groups:\n","                if num_update < 50:\n","                    g['lr'] = base_lr * num_update\n","                else:\n","                    g['lr'] = 0.001 / np.sqrt(num_update-49)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            num_update += 1\n","\n","    model.eval()\n","    for batch, label in val_loader:\n","        batch = batch.to(device)\n","        label = label.to(device)\n","        output = model(batch)\n","        loss = val_loss_fn(output, label)\n","        val_likelihood += float(loss)\n","\n","    stat = [train_likelihood/50, train_intra/50, train_teacher/50, val_likelihood/170]\n","    curve.append(stat)\n","\n","    np.savetxt(f'/content/gdrive/My Drive/{name}r1_curve.txt', np.array(curve))\n","    torch.save(model.state_dict(), f'/content/gdrive/My Drive/{name}r1_{epoch}.mdl')"],"metadata":{"id":"Qhbo7SNkxrjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XmvCWTB253c7"},"execution_count":null,"outputs":[]}]}